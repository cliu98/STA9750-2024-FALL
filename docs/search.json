[
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "",
    "text": "The public transit systems found nationwide in the United States play an important role in mobilizing people in their daily lives. This analysis will give an overview of how the transit systems perform relative to another by examining key metrics such as farebox revenues, total number of trips, total vehicle miles traveled and total revenues and expenses by source.\nThis analysis will examine information from 2022 that utilizes data from fare revenue, monthly ridership, and operating expense reports. By evaluating the metrics mentioned earlier, this analysis will identify trends, expose common challenges, and offer insight into transit performances. Various transit performance metrics will be analyzed to gain a general overview of the data. Based on these findings, transit system efficiency will be evaluated on a comparative basis. The working data sets are provided by the Federal Transit Administration. The latter half of the analysis will examine metrics that can define a transit system as efficient. The evaluation of efficiency will depend on the interpretation of what makes something efficient."
  },
  {
    "objectID": "mp01.html#introduction",
    "href": "mp01.html#introduction",
    "title": "Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Introduction",
    "text": "Introduction\nThe public transit systems found nationwide in the United States play an important role in mobilizing people in their daily lives. This analysis will give an overview of how the transit systems perform relative to another by examining key metrics such as farebox revenues, total number of trips, total vehicle miles traveled and total revenues and expenses by source.\nThis analysis will examine information from 2022 that utilizes data from fare revenue, monthly ridership, and operating expense reports. By evaluating the metrics mentioned earlier, this analysis will identify trends, expose common challenges, and offer insight into transit performances. Various transit performance metrics will be analyzed to gain a general overview of the data. Based on these findings, transit system efficiency will be evaluated on a comparative basis. The working data sets are provided by the Federal Transit Administration. The latter half of the analysis will examine metrics that can define a transit system as efficient. The evaluation of efficiency will depend on the interpretation of what makes something efficient."
  },
  {
    "objectID": "mp01.html#analysis",
    "href": "mp01.html#analysis",
    "title": "Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Analysis",
    "text": "Analysis\n\nPreparing, Cleaning & Loading the Dataset\nThe relevant data sets used in the analysis can be found here:\n\n2022 fare revenue\n2022 expenses\nridership\n\nThe first step in the analysis is to ingest the relevant data tables and prepare them for data analysis using R. The following code will clean and join the tables into relevant dataframes used in the analysis. The output will create the following dataframes named: FARES, EXPENSES, FINANCIALS, TRIPS, MILES, and USAGE.\n\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\n# Let's start with Fare Revenue\nlibrary(tidyverse)\nFARES &lt;- readxl::read_xlsx(\"2022_fare_revenue.xlsx\") |&gt;\n  select(\n    -`State/Parent NTD ID`,\n    -`Reporter Type`,\n    -`Reporting Module`,\n    -`TOS`,\n    -`Passenger Paid Fares`,\n    -`Organization Paid Fares`\n  ) |&gt;\n  filter(`Expense Type` == \"Funds Earned During Period\") |&gt;\n  select(-`Expense Type`) |&gt;\n  group_by(\n    `NTD ID`, # Sum over different `TOS` for the same `Mode`\n    `Agency Name`, # These are direct operated and sub-contracted\n    `Mode`\n  ) |&gt; # of the same transit modality\n  # Not a big effect in most munis (significant DO\n  # tends to get rid of sub-contractors), but we'll sum\n  # to unify different passenger experiences\n  summarize(`Total Fares` = sum(`Total Fares`)) |&gt;\n  ungroup()\n\n# Next, expenses\nEXPENSES &lt;- readr::read_csv(\"2022_expenses.csv\") |&gt;\n  select(\n    `NTD ID`,\n    `Agency`,\n    `Total`,\n    `Mode`\n  ) |&gt;\n  mutate(`NTD ID` = as.integer(`NTD ID`)) |&gt;\n  rename(Expenses = Total) |&gt;\n  group_by(`NTD ID`, `Mode`) |&gt;\n  summarize(Expenses = sum(Expenses)) |&gt;\n  ungroup()\n\nFINANCIALS &lt;- inner_join(FARES, EXPENSES, join_by(`NTD ID`, `Mode`))\n\n# Monthly Transit Numbers\nlibrary(tidyverse)\nTRIPS &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet = \"UPT\") |&gt;\n  filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n  select(\n    -`Legacy NTD ID`,\n    -`Reporter Type`,\n    -`Mode/Type of Service Status`,\n    -`UACE CD`,\n    -`TOS`\n  ) |&gt;\n  pivot_longer(-c(`NTD ID`:`3 Mode`),\n    names_to = \"month\",\n    values_to = \"UPT\"\n  ) |&gt;\n  drop_na() |&gt;\n  mutate(month = my(month)) # Parse _m_onth _y_ear date specs\nMILES &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet = \"VRM\") |&gt;\n  filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n  select(\n    -`Legacy NTD ID`,\n    -`Reporter Type`,\n    -`Mode/Type of Service Status`,\n    -`UACE CD`,\n    -`TOS`\n  ) |&gt;\n  pivot_longer(-c(`NTD ID`:`3 Mode`),\n    names_to = \"month\",\n    values_to = \"VRM\"\n  ) |&gt;\n  drop_na() |&gt;\n  group_by(\n    `NTD ID`, `Agency`, `UZA Name`,\n    `Mode`, `3 Mode`, month\n  ) |&gt;\n  summarize(VRM = sum(VRM)) |&gt;\n  ungroup() |&gt;\n  mutate(month = my(month)) # Parse _m_onth _y_ear date specs\n\nUSAGE &lt;- inner_join(TRIPS, MILES) |&gt;\n  mutate(`NTD ID` = as.integer(`NTD ID`))\n\n# End of data ingestion and setup\n\nHere, a summary table of USAGE is created to get an introductory visualization of the table that will be used for analysis.\n\nif (!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\n\n# Initialize a table to begin analysis\nsample_n(USAGE, 1000) |&gt;\n  mutate(month = as.character(month)) |&gt;\n  DT::datatable()\n\n\n\n\n\nTable 1: Quick Overview of USAGE\n\n\nTransforming Data Table\nThe analysis will be initially conducted using the dataframe USAGE. Some of the provided labels are cumbersome to work with in R. It is doable, but we can make our lives easier by renaming them. The first task at hand is to rename the column UZA Name to metro_area. The following code will show how that is done.\n\nUSAGE &lt;- USAGE |&gt;\n  rename(metro_area = `UZA Name`)\n\nEach transportation Mode is represented by a two letter code, for example HR = Heavy Rail. The two letter codes aren’t meaningful to us, as it’s impossible to guess what they are. The first thing I did to clean this portion up was to find all the codes used by the Federal Transit Administration. Running the following will give us the list of codes found in the data set.\n\nlibrary(dplyr)\nunique_modes &lt;- USAGE |&gt;\n  distinct(Mode)\nprint(unique_modes)\n\n# A tibble: 18 × 1\n   Mode \n   &lt;chr&gt;\n 1 DR   \n 2 FB   \n 3 MB   \n 4 SR   \n 5 TB   \n 6 VP   \n 7 CB   \n 8 RB   \n 9 LR   \n10 YR   \n11 MG   \n12 CR   \n13 AR   \n14 TR   \n15 HR   \n16 IP   \n17 PB   \n18 CC   \n\n\nOnce all the codes have been identified, the meanings can be found in the National Transit Database glossary. Using the mutate function, all the codes can be changed into meaningful definitions.\n\nUSAGE &lt;- USAGE |&gt;\n  mutate(Mode = case_when(\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferryboat\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"SR\" ~ \"Streetcar Rail\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"MG\" ~ \"Monorail and Automated Guideway modes\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramways\",\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"\n  ))\n\nNow that the data has been cleaned up to a new extent, let’s create a new summary table from USAGE. To make the outputted table as clean as possible, I’ve opted to get rid of irrelevant columns and change the acronyms to display meaningful words. UPT stands for unlinked passenger trips and VRM stands for vehicle revenue miles.\n\ndatatable(\n  sample_n(USAGE, 1000) |&gt;\n    mutate(month = as.character(month)) |&gt;\n    select(-`NTD ID`, -`3 Mode`) |&gt; # exclude ntd id and 3 mode in visual table\n    rename(\n      `Metro Area` = metro_area, # rename for table output to look cleaner\n      `Unlinked Passenger Trips` = UPT, # rename acronym in visual table\n      `Vehicle Revenue Miles` = VRM # rename acronym in visual table\n    )\n)\n\n\n\n\n\nTable 2: Cleaned Up Version of USAGE\n\n\n\n\n\n\nFor my own sanity, I checked if the table had any NA values I needed to consider.\n\n\n\n\nna_count &lt;- sum(is.na(USAGE))\nprint(na_count)\n\n[1] 0\n\n\nThis code returns 0, which lets me know there are no missing values in the data I’m working with. With this reassurance, I won’t be using the na.rm=TRUE statement in any of my code. However, if the opposite were true instead, I would make sure to use the above statement while utilizing aggregate functions.\n\n\n\n\nInitial Metrics of Interest\nNow that there is a clean table to work with, some questions of interest about the data can be explored. The following questions will explore the use of the following functions filter, group_by, summarize, and arrange.\n\n\n\n\n\n\nThe first set of metrics of interest are:\n\n\n\n\nWhat transit agency had the most total VRM in this sample?\nWhat transit mode had the most total VRM in this sample?\nHow many trips were taken on the NYC Subway (Heavy Rail) in May 2024?\nHow much did NYC subway ridership fall between April 2019 and April 2020?\n\n\n\n\nTo find the transit agency with the most total VRM from the sample, I need to group the data based on the Agency and its respective VRM total. It turns out the MTA New York City Transit has reign over total VRM among the agencies with 10,832,855,350 miles.\n\nmost_vrm_agency &lt;- USAGE |&gt;\n  group_by(Agency) |&gt;\n  summarize(total_vrm = sum(VRM)) |&gt;\n  arrange(desc(total_vrm))\nprint(most_vrm_agency)\n\n# A tibble: 677 × 2\n   Agency                                                              total_vrm\n   &lt;chr&gt;                                                                   &lt;dbl&gt;\n 1 MTA New York City Transit                                             1.08e10\n 2 New Jersey Transit Corporation                                        5.65e 9\n 3 Los Angeles County Metropolitan Transportation Authority              4.35e 9\n 4 Washington Metropolitan Area Transit Authority                        2.82e 9\n 5 Chicago Transit Authority                                             2.81e 9\n 6 Southeastern Pennsylvania Transportation Authority                    2.67e 9\n 7 Massachusetts Bay Transportation Authority                            2.38e 9\n 8 Pace, the Suburban Bus Division of the Regional Transportation Aut…   2.38e 9\n 9 Metropolitan Transit Authority of Harris County, Texas                2.27e 9\n10 Denver Regional Transportation District                               1.99e 9\n# ℹ 667 more rows\n\n\n\n\n\n\n\n\nAlternative code block regarding slice_head:\n\n\n\n\n\nI purposely chose not to include a slice_head function to get a comparative overview of the data. Here, the MTA had an overwhelming total over the other agencies, which was an interesting finding. I stuck with the same philosophy throughout most of this analysis since I was interested in comparing the sheer numbers as well, not just the specific metric I was inquiring about.\n\nmost_vrm_agency1 &lt;- USAGE |&gt;\n  group_by(Agency) |&gt;\n  summarize(total_vrm = sum(VRM)) |&gt;\n  arrange(desc(total_vrm)) |&gt;\n  slice_head(n = 1)\nprint(most_vrm_agency1)\n\n# A tibble: 1 × 2\n  Agency                      total_vrm\n  &lt;chr&gt;                           &lt;dbl&gt;\n1 MTA New York City Transit 10832855350\n\n\n\n\n\nTo find the transit mode with the most total VRM from the sample, I need to group the data based on the Mode and its respective VRM total. By a large margin of 49,444,494,088 miles, the bus(MB) Mode had the most total VRM from the sample.\n\nmost_vrm_mode &lt;- USAGE |&gt;\n  group_by(Mode) |&gt;\n  summarize(total_vrm = sum(VRM)) |&gt;\n  arrange(desc(total_vrm))\nprint(most_vrm_mode)\n\n# A tibble: 18 × 2\n   Mode                                    total_vrm\n   &lt;chr&gt;                                       &lt;dbl&gt;\n 1 Bus                                   49444494088\n 2 Demand Response                       17955073508\n 3 Heavy Rail                            14620362107\n 4 Commuter Rail                          6970644241\n 5 Vanpool                                3015783362\n 6 Light Rail                             2090094714\n 7 Commuter Bus                           1380948975\n 8 Publico                                1021270808\n 9 Trolleybus                              236840288\n10 Bus Rapid Transit                       118425283\n11 Ferryboat                                65589783\n12 Streetcar Rail                           63389725\n13 Monorail and Automated Guideway modes    37879729\n14 Hybrid Rail                              37787608\n15 Alaska Railroad                          13833261\n16 Cable Car                                 7386019\n17 Inclined Plane                             705904\n18 Aerial Tramways                            292860\n\n\nTo find how many trips were taken on the NYC Subway in May 2024, there were multiple criteria to consider here. A filter needs to be used in order to address the transit Mode, month, and Agency. In this case, I made the assumption that the NYC Subway is only operated by the MTA New York City Transit. In May 2024, there were a total of 180,000,000 (1.80e8) trips taken.\n\nnyc_subway_trips &lt;- USAGE |&gt;\n  filter(\n    Agency == \"MTA New York City Transit\",\n    Mode == \"Heavy Rail\",\n    month &gt;= as.Date(\"2024-05-01\") & month &lt;= as.Date(\"2024-05-31\")\n  )\nprint(nyc_subway_trips)\n\n# A tibble: 1 × 8\n  `NTD ID` Agency             metro_area Mode  `3 Mode` month         UPT    VRM\n     &lt;int&gt; &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;    &lt;date&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n1    20008 MTA New York City… New York-… Heav… Rail     2024-05-01 1.80e8 3.00e7\n\n\n\n\n\n\n\n\nInformation Regarding the month Column\n\n\n\nAfter going through this question at hand, I was able to identify that the month is stored as just the first of the month each year. I confirmed that there is only one entry per month for each respective agency, metro area, and mode. Moving forward with any month filters, I don’t have to worry about including the full month as it will always only be in the format of the first of the month in each year. This understanding will be seen in later examples.\n\n\nTo find the ridership difference between April 2019 and April 2020, I need to find the amount of trips taken in each month-year and subtract from one another. Again, I made the assumption that the NYC Subway is only operated by the MTA New York City Transit. In the following code, you can see the difference when filtering the month as referenced in the call-out above! The ridership fell by 211,969,660 trips between April 2019 and April 2020. I interpreted this metric as the difference of trips between each respective month-year, not the total difference in between.\n\nride_fall &lt;- USAGE |&gt;\n filter(Mode == \"Heavy Rail\") |&gt;\n filter(Agency == \"MTA New York City Transit\") |&gt; # this is the agency that runs nyc subway\n filter(month %in% c(as.Date(\"2019-04-01\"), as.Date(\"2020-04-01\"))) |&gt;\n group_by(month) |&gt;\n summarize(total_rides = sum(UPT)) |&gt;\n summarize(difference = total_rides[month == as.Date(\"2020-04-01\")] -\n   total_rides[month == as.Date(\"2019-04-01\")])\nprint(ride_fall)\n\n# A tibble: 1 × 1\n  difference\n       &lt;dbl&gt;\n1 -211969660\n\n\n\n\n\nAdditional Metrics of Interest\nAsides from the metrics explored above, there are a variety of other questions that can be asked from the data. In this section, I will explore three other areas of interest. The data is not limited to the following questions discussed, there are a multitude of statistics that can be uncovered. For the following questions I asked, I am trying to utilize as many R functions as possible.\n\n\n\n\n\n\nAdditional metrics of interest are:\n\n\n\n\nWhat UZA Name / metro_area had the most UPT in January 2022?\nWhat month and year had the most UPT through the bus (MB) in the entire sample?\nWhat is the average amount of trips taken in the New York–Jersey City–Newark, NY–NJ area based on the season from 2018 to 2022?\n\n\n\n\nThe areas of interest require filtering a date, grouping by a variable, and aggregating a variable. In January 2022, the New York City–Jersey City–Newark, NY–NJ area recorded the most trips with 173,719,501 trips. The second-ranked area had only 26,158,306 trips, accounting for just 15% of the total for New York–New Jersey.\n\nlibrary(dplyr)\npopular_area &lt;- USAGE |&gt;\n filter(month %in% c(as.Date(\"2022-01-01\"))) |&gt;\n group_by(metro_area) |&gt;\n summarize(total_trips = sum(UPT)) |&gt;\n arrange(desc(total_trips))\nprint(popular_area)\n\n# A tibble: 295 × 2\n   metro_area                            total_trips\n   &lt;chr&gt;                                       &lt;dbl&gt;\n 1 New York--Jersey City--Newark, NY--NJ   173719501\n 2 Los Angeles--Long Beach--Anaheim, CA     26158306\n 3 Chicago, IL--IN                          16569817\n 4 San Francisco--Oakland, CA               13571654\n 5 Boston, MA--NH                           13220711\n 6 Philadelphia, PA--NJ--DE--MD             12972351\n 7 Washington--Arlington, DC--VA--MD        11229936\n 8 Miami--Fort Lauderdale, FL                8318327\n 9 Seattle--Tacoma, WA                       8250602\n10 San Diego, CA                             4602265\n# ℹ 285 more rows\n\n\nThis metric references the second question from the previous question. Now we’re taking a look at history, seeing exactly when this mode peaked. The data tells us that this happened in October 2018, with 478,806,384 trips. Evidently, the lower ranked months had similar values too. This shows strong consistency for the bus transit mode across the US.\n\nbus_trips &lt;- USAGE |&gt;\n filter(Mode == \"Bus\") |&gt;\n group_by(month) |&gt;\n summarize(total_bus_trips = sum(UPT)) |&gt;\n arrange(desc(total_bus_trips))\nprint(bus_trips)\n\n# A tibble: 271 × 2\n   month      total_bus_trips\n   &lt;date&gt;               &lt;dbl&gt;\n 1 2008-10-01       478806384\n 2 2014-10-01       457089165\n 3 2013-10-01       456214396\n 4 2007-10-01       455193568\n 5 2008-09-01       454077576\n 6 2006-10-01       450496480\n 7 2006-03-01       450386143\n 8 2012-10-01       448572088\n 9 2008-05-01       442961523\n10 2009-10-01       441007281\n# ℹ 261 more rows\n\n\nThe code required for this was a challenge, but it utilized functions already explored earlier and putting them together intricately. The months were assigned to a season within a case_when function within a mutate function. Additionally a filter, group_by, summarize, arrange, and mean function were used as well. From 2018 to 2022, the average amount of trips taken in NY-NJ was:\n\n\n\nSeason\nAverage UPT\n\n\n\n\nFall\n4,960,514\n\n\nWinter\n4,772,907\n\n\nSummer\n4,609,142\n\n\nSpring\n4,508,331\n\n\n\n\nTable 3: Average UPT by Season, NY-NJ, 2018-2022\n\nseasonal_variation &lt;- USAGE |&gt;\n  filter(metro_area == \"New York--Jersey City--Newark, NY--NJ\") |&gt;\n  filter(month &gt;= as.Date(\"2018-01-01\") & month &lt;= as.Date(\"2022-12-01\")) |&gt;\n  mutate(\n    month_num = as.numeric(format(month, \"%m\")), # Extract the month as a number from the date column\n    season = case_when( # Use case_when to categorize into seasons\n      month_num %in% c(12, 1, 2) ~ \"Winter\",\n      month_num %in% c(3, 4, 5) ~ \"Spring\",\n      month_num %in% c(6, 7, 8) ~ \"Summer\",\n      month_num %in% c(9, 10, 11) ~ \"Fall\",\n      TRUE ~ \"Unknown\"\n    )\n  ) |&gt;\n  group_by(season) |&gt;\n  summarize(avg_trips = mean(UPT)) |&gt;\n  arrange(desc(avg_trips))\nprint(seasonal_variation)\n\n\nThis concludes the first half of the analysis. A variety of transit metric data was unearthed. A better understanding of the R functions were explored through data analysis. Now that preliminary data has been identified, we can move forward to the next half of the analysis. The fare data available to use is from 2022. In order to do a deeper analysis, the USAGE table will need to be converted to a 2022 version in order to join the fare data information together. Once we have a combined table, we can uncover what farebox recovery looked like in 2022.\n\n\nFarebox Recovery\nThe first task at hand is to extract only the 2022 information from USAGE. The parameters of interest kept are NTD ID, Agency, metro_area, Mode, UPT, VRM. Normally, filtering just the year and selecting the parameters would be straightforward. However, the UPT and VRM need to be aggregated for the new joined table. Additionally, the mutate function is used to convert NTD ID to a double type in order to match the same type as the NT ID in the FINANCIALS table we will be joining to later. The new table is called USAGE_2022_ANNUAL. For the farebox recovery analysis, the sample will focus solely on major transit systems, which is defined as those with 400,000 UPT per annum. For this definition of major transit systems, the total UPT per Agency was considered (the mode is not taken into consideration, just the Agency as a whole itself).\n\n# Calculate UPT per agency per year to only consider agencies with UPT of 400,000 or more per year\nagencies_400k_upt &lt;- USAGE |&gt;\n  mutate(Year = year(month)) |&gt; # Extract year from month\n  group_by(`NTD ID`, Agency, Year) |&gt; # Group by agency and year\n  summarize(Total_UPT = sum(UPT, na.rm = TRUE), .groups = 'drop') |&gt; # Summarize total UPT per Agency, aggregating the UPT across all modes\n  filter(Total_UPT &gt;= 400000) |&gt; # Keep agencies with total UPT &gt;= 400,000 annum\n  ungroup() |&gt; # Ungroup to prepare for next operation\n  distinct(Agency) # Get distinct agencies\n\n# Filter the 2022 data for only those agencies\nUSAGE_2022_ANNUAL &lt;- USAGE |&gt;\n  filter(year(month) == 2022) |&gt; # Only data from 2022\n  filter(Agency %in% agencies_400k_upt$Agency) |&gt; # Filter agencies that meet avg UPT condition\n  group_by(`NTD ID`, Agency, metro_area, Mode) |&gt; # Group by relevant columns\n  summarize(\n    UPT = sum(UPT), # Sum UPT for 2022\n    VRM = sum(VRM) # Sum VRM for 2022\n  ) |&gt;\n  ungroup() |&gt;\n  mutate(`NTD ID` = as.double(`NTD ID`)) # Convert NTD ID to double for joining\n\nprint(USAGE_2022_ANNUAL) # Output the filtered table\n\n# A tibble: 1,023 × 6\n   `NTD ID` Agency                                metro_area Mode     UPT    VRM\n      &lt;dbl&gt; &lt;chr&gt;                                 &lt;chr&gt;      &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1        1 King County                           Seattle--… Bus   5.40e7 6.16e7\n 2        1 King County                           Seattle--… Dema… 6.63e5 1.29e7\n 3        1 King County                           Seattle--… Ferr… 4.00e5 5.12e4\n 4        1 King County                           Seattle--… Stre… 1.12e6 1.80e5\n 5        1 King County                           Seattle--… Trol… 9.58e6 2.64e6\n 6        1 King County                           Seattle--… Vanp… 7.03e5 4.41e6\n 7        2 Spokane Transit Authority             Spokane, … Bus   6.60e6 6.49e6\n 8        2 Spokane Transit Authority             Spokane, … Dema… 3.10e5 4.04e6\n 9        2 Spokane Transit Authority             Spokane, … Vanp… 9.06e4 9.06e5\n10        3 Pierce County Transportation Benefit… Seattle--… Bus   4.95e6 4.23e6\n# ℹ 1,013 more rows\n\n\n\n\n\n\n\n\nAdditional transformation is required:\n\n\n\nBefore we can join USAGE_2022_ANNUAL onto FINANCIALS, we need to revisit the mode conversion we did earlier in the data cleaning. The FINANCIALS table follows the same format with the mode being a code as seen earlier.\n\nFINANCIALS &lt;- FINANCIALS |&gt;\n  mutate(Mode = case_when(\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferryboat\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"SR\" ~ \"Streetcar Rail\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"MG\" ~ \"Monorail and Automated Guideway modes\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramways\",\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"\n  ))\n\n\n\nFinally, we can join the USAGE_2022_ANNUAL and FINANCIALS tables together. USAGE_AND_FINANCIALS will be used to conduct the final analysis on farebox recovery in 2022. An innjer_join is used since some values were dropped when filtering out for only major transit systems. In order to join the data properly, an inner_join matches the values that are only present in the USAGE_2022_ANNUAL table.\n\nUSAGE_AND_FINANCIALS &lt;- inner_join(\n  USAGE_2022_ANNUAL,\n  FINANCIALS,\n  join_by(`NTD ID`, Mode)\n  )\n\ndatatable( \n  sample_n(USAGE_AND_FINANCIALS, 1000) |&gt;\n    select(-`Agency Name`) |&gt; # exclude extra agency name column from financials table\n    rename(\n      `Metro Area` = metro_area, # rename for table output to look cleaner\n      `Unlinked Passenger Trips` = UPT, # rename acronym in visual table\n      `Vehicle Revenue Miles` = VRM # rename acronym in visual table\n          )\n)\n\n\n\n\n\nTable 4: Visual of USAGE_AND_FINANCIALS\n\n\n\n\n\n\nFarebox recovery metrics to be analyzed:\n\n\n\n\nWhich transit system (agency and mode) had the most UPT in 2022?\nWhich transit system (agency and mode) had the highest farebox recovery, defined as the highest ratio of Total Fares to Expenses?\nWhich transit system (agency and mode) has the lowest expenses per UPT?\nWhich transit system (agency and mode) has the highest total fares per UPT?\nWhich transit system (agency and mode) has the lowest expenses per VRM?\nWhich transit system (agency and mode) has the highest total fares per VRM?\n\n\n\n\nIn 2022, the MTA New York City Transit had the most UPT via the heavy rail (subway). There was a total of 1,793,073,801 trips taken. This result is not suprising given the sheer population size of the NYC tri-state area, as well as the vast amount of public transportation accessibility throughout the city.\n\nmost_UPT_2022 &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(most_UPT = UPT, .groups = 'drop') |&gt;\n  arrange(desc(most_UPT)) \nprint(most_UPT_2022)\n\n# A tibble: 1,016 × 3\n   Agency                                                   Mode        most_UPT\n   &lt;chr&gt;                                                    &lt;chr&gt;          &lt;dbl&gt;\n 1 MTA New York City Transit                                Heavy Rail    1.79e9\n 2 MTA New York City Transit                                Bus           4.59e8\n 3 Los Angeles County Metropolitan Transportation Authority Bus           1.94e8\n 4 Chicago Transit Authority                                Bus           1.40e8\n 5 New Jersey Transit Corporation                           Bus           1.13e8\n 6 Chicago Transit Authority                                Heavy Rail    1.04e8\n 7 MTA Bus Company                                          Bus           1.00e8\n 8 Washington Metropolitan Area Transit Authority           Heavy Rail    9.84e7\n 9 Southeastern Pennsylvania Transportation Authority       Bus           9.66e7\n10 Washington Metropolitan Area Transit Authority           Bus           8.99e7\n# ℹ 1,006 more rows\n\n\nIn 2022, the County of Miami-Dade via vanpool had the highest farebox recovery with a ratio of 1.67. I found this result intriguing as vanpool would not have been my first assumption for this metric. Living in Brooklyn, the concept of vanpooling is unfamiliar to me. However, this did give me insight to how the rest of the country can greatly differ depending on the area of interest.\n\nhighest_farebox &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(total_fares = `Total Fares`, total_expenses = Expenses, .groups = 'drop') |&gt;\n  mutate(recovery = total_fares / total_expenses) |&gt;\n  arrange(desc(recovery))\nprint(highest_farebox)\n\n# A tibble: 1,016 × 5\n   Agency                              Mode  total_fares total_expenses recovery\n   &lt;chr&gt;                               &lt;chr&gt;       &lt;dbl&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n 1 County of Miami-Dade                Vanp…     1987879        1191874     1.67\n 2 Yuma County Intergovernmental Publ… Vanp…      411216         279585     1.47\n 3 Port Imperial Ferry Corporation     Ferr…    33443241       23417248     1.43\n 4 Hyannis Harbor Tours, Inc.          Ferr…    25972659       18383764     1.41\n 5 Trans-Bridge Lines, Inc.            Comm…    11325199        8495611     1.33\n 6 Chattanooga Area Regional Transpor… Incl…     3005198        2290714     1.31\n 7 Municipality of Anchorage           Vanp…     1400709        1105911     1.27\n 8 Regional Transportation Commission… Vanp…     3561776        2876745     1.24\n 9 Fort Worth Transportation Authority Vanp…     1410877        1141477     1.24\n10 Hampton Jitney, Inc.                Comm…    21539188       17957368     1.20\n# ℹ 1,006 more rows\n\n\nIn 2022, North Carolina State University via bus had the lowest expenses per trip with a ratio of 1.18. More insight into the expenses of this transportation mode could provide better context to the performance. A university funded transportation system could potentially have lower operation costs compared to a metropolitan transit system due to less logistical hurdles to overcome.\n\nlow_expense_UPT &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(total_expenses = Expenses, total_UPT = UPT, .groups = 'drop') |&gt;\n  mutate(lowestUPT = total_expenses/total_UPT) |&gt;\n  arrange(lowestUPT)\nprint(low_expense_UPT)\n\n# A tibble: 1,016 × 5\n   Agency                               Mode  total_expenses total_UPT lowestUPT\n   &lt;chr&gt;                                &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 North Carolina State University      Bus          2727412   2313091      1.18\n 2 Anaheim Transportation Network       Bus          9751600   7635011      1.28\n 3 Valley Metro Rail, Inc.              Stre…         542700    364150      1.49\n 4 University of Iowa                   Bus          3751241   2437750      1.54\n 5 Chatham Area Transit Authority       Ferr…         935249    582988      1.60\n 6 Texas State University               Bus          4825081   2348943      2.05\n 7 South Florida Regional Transportati… Bus           731643    322155      2.27\n 8 University of Georgia                Bus          6267845   2714941      2.31\n 9 Hillsborough Area Regional Transit … Stre…        2780595   1137177      2.45\n10 University of Michigan Parking and … Bus         11990864   4754836      2.52\n# ℹ 1,006 more rows\n\n\nIn 2022, the Altoona Metro Transit via demand rail had the highest total fares per trip with a ratio of 660. Note that the ratio is absurdly large, there are indications as to why this number is so high. The output shows that there were only a total of 26 unlinked passenger trips for this mode of transportation. The lack of total trips is something to consider when answering this question. I would postulate setting a minimum number of unlinked passenger trips when asking this question if I wanted to explore the data more in depth.\n\nhigh_fare_UPT &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(total_fares = `Total Fares`, total_UPT = UPT, .groups = 'drop') |&gt;\n  mutate(fare_UPT_ratio = total_fares/total_UPT) |&gt;\n  arrange(desc(fare_UPT_ratio))\nprint(high_fare_UPT)\n\n# A tibble: 1,016 × 5\n   Agency                             Mode  total_fares total_UPT fare_UPT_ratio\n   &lt;chr&gt;                              &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;\n 1 Altoona Metro Transit              Dema…       17163        26          660. \n 2 Central Pennsylvania Transportati… Dema…    14084946    280455           50.2\n 3 Hampton Jitney, Inc.               Comm…    21539188    521577           41.3\n 4 County of Placer                   Comm…       40847      1054           38.8\n 5 Lane Transit District              Dema…    10724805    314974           34.0\n 6 Pennsylvania Department of Transp… Comm…    14580664    452034           32.3\n 7 Hyannis Harbor Tours, Inc.         Ferr…    25972659    878728           29.6\n 8 Trans-Bridge Lines, Inc.           Comm…    11325199    403646           28.1\n 9 SeaStreak, LLC                     Ferr…    16584600    750392           22.1\n10 Cambria County Transit Authority   Dema…      520554     25831           20.2\n# ℹ 1,006 more rows\n\n\nIn 2022, the VIA Metropolitan Transit via vanpool had the lowest expenses per vehicle revenue mile with a ratio of 0.37. Once again, the transportation mode of vanpool has been a key finding in the metrics observed. Somethings that could be further explored are the characteristics of the areas vanpools are popular in.\n\nlow_expense_VRM &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(total_expenses = Expenses, total_VRM = VRM, .groups = 'drop') |&gt;\n  mutate(lowestVRM = total_expenses/total_VRM) |&gt;\n  arrange(lowestVRM)\nprint(low_expense_VRM)\n\n# A tibble: 1,016 × 5\n   Agency                               Mode  total_expenses total_VRM lowestVRM\n   &lt;chr&gt;                                &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 VIA Metropolitan Transit             Vanp…        1298365   3505579     0.370\n 2 County of Miami-Dade                 Vanp…        1191874   3091052     0.386\n 3 County of Volusia                    Vanp…          87487    222484     0.393\n 4 Corpus Christi Regional Transportat… Vanp…         433951   1006399     0.431\n 5 Metropolitan Transportation Commiss… Vanp…        5491767  12341055     0.445\n 6 Central Midlands Regional Transport… Vanp…         195326    438557     0.445\n 7 Fort Worth Transportation Authority  Vanp…        1141477   2372285     0.481\n 8 San Joaquin Council                  Vanp…        4629125   9297516     0.498\n 9 Salem Area Mass Transit District     Vanp…         238952    468018     0.511\n10 San Diego Association of Governments Vanp…        5264624   9740828     0.540\n# ℹ 1,006 more rows\n\n\nIn 2022, the Chicago Water Taxi (Wendella) via Ferry had the highest total fares per vehicle revenue mile with a ratio of 237. Like the fourth farebox recovery metric, this data point also seems skewed because of the lack of vehicle revenue miles involved. I would suggest setting a minimum amount of vehicle revenue miles as well to gauge a better understanding of which transit system boasts the highest total fare per vehicle revenue mile.\n\nhigh_fare_VRM &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(total_fares = `Total Fares`, total_VRM = VRM, .groups = 'drop') |&gt;\n  mutate(fare_VRM_ratio = total_fares/total_VRM) |&gt;\n  arrange(desc(fare_VRM_ratio))\nprint(high_fare_VRM)\n\n# A tibble: 1,016 × 5\n   Agency                             Mode  total_fares total_VRM fare_VRM_ratio\n   &lt;chr&gt;                              &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;\n 1 Chicago Water Taxi (Wendella)      Ferr…      142473       600          237. \n 2 Altoona Metro Transit              Dema…       17163        75          229. \n 3 Jacksonville Transportation Autho… Ferr…     1432549      9084          158. \n 4 Chattanooga Area Regional Transpo… Incl…     3005198     20128          149. \n 5 Hyannis Harbor Tours, Inc.         Ferr…    25972659    188694          138. \n 6 SeaStreak, LLC                     Ferr…    16584600    143935          115. \n 7 Cape May Lewes Ferry               Ferr…     6663334     71640           93.0\n 8 Woods Hole, Martha's Vineyard and… Ferr…    33424462    364574           91.7\n 9 Washington State Ferries           Ferr…    57644277    738094           78.1\n10 County of Pierce                   Ferr…     2979914     44548           66.9\n# ℹ 1,006 more rows"
  },
  {
    "objectID": "mp01.html#conclusion",
    "href": "mp01.html#conclusion",
    "title": "Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Conclusion",
    "text": "Conclusion\nThis analysis was ultimately inspired by how farebox recovery rates vary by transit system across the nation. The definition of efficiency I would like to use is the transit system with the highest farebox recovery rate. From this sample, the County of Miami-Dade vanpool transit system is the most efficient. The fares made on the transit system trips exceed the operation cost per trip. Not only does the return cover the operation costs, it also exceeds it slightly. If profit is not the goal for the transit system, the surplus revenue can be reinvested into infrastructure to ensure smooth operations. Continuously enhancing the rider experience can help sustain the system over the long term, with the aim of increasing ride share participation over time.\nThere are other data points that can be analyzed and incorporated into this analysis for further exploration. Some areas of interest I would explore are comparing trips taken to the total population the area serves. This can give a better idea about what percentage of the population is utilizing the public transportation system. Trends about public transportation usage based on the population available can highlight how popular public transit is depending on an area. Another area of interest I would like to explore is the carbon emission reduction provided by transit systems. A transit system’s financial stability could be easily offset by environmental impact depending on the mode of transportation. There are multitudes of other data points that can be extrapolated to explore how efficient a transit system is within the scope of defining what efficiency is."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome To My STA9750 Portfolio",
    "section": "",
    "text": "Mini Project #01: Fiscal Characteristics of Major US Public Transit Systems\nMini Project #02: The Business of Show Business\nMini Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Welcome To My STA9750 Portfolio",
    "section": "About Me",
    "text": "About Me\n\nChris Liu\nI am currently attending Baruch College in the Zicklin School of Business with an anticpated graudation of May 2026. I am pursuing a MS in Statistics with a focus in data science. I hope to pursue a career within statistical machine learning after graduation. If you’re interested in getting connected, here is my LinkedIn profile.\nI graduated with a B.S. in Mechanical Engineering from Boston University in 2021. Some of my previous work done in my undergraduate courses can be found in my mechanical engineering project portfolio."
  },
  {
    "objectID": "index.html#portfolio-projects",
    "href": "index.html#portfolio-projects",
    "title": "Welcome To My STA9750 Portfolio",
    "section": "",
    "text": "Mini Project #01: Fiscal Characteristics of Major US Public Transit Systems\nMini Project #02: The Business of Show Business\nMini Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?"
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Mini-Project #02: The Business of Show Business",
    "section": "",
    "text": "Hollywood executives are tasked with coming up with new movie ideas that will convince an audience to see their movie. Traditionally, executives secure life rights to produce biopics, obtain licensing agreements to adapt existing forms of media onto the big screen, and work with owners of promising intellectual property that can be adapted. However, Hollywood movies are struggling to recapture the box office successes as seen in the past. For one reason or another, movie goers have not been flocking to theaters to see the next big film. The goal of this project is to develop data-driven insights for new movie ideas.\nIn order to derive insights, we will be diving into data from the Internet Movie Database (IMDb). The dataset used in this project analysis comes from the IMDb non-commercial release data tables. We will explore key characteristics of successful movies in history, identify successful actors and filmmakers and examine downfalls in Hollywood history."
  },
  {
    "objectID": "mp02.html#introduction",
    "href": "mp02.html#introduction",
    "title": "Mini-Project #02: The Business of Show Business",
    "section": "",
    "text": "Hollywood executives are tasked with coming up with new movie ideas that will convince an audience to see their movie. Traditionally, executives secure life rights to produce biopics, obtain licensing agreements to adapt existing forms of media onto the big screen, and work with owners of promising intellectual property that can be adapted. However, Hollywood movies are struggling to recapture the box office successes as seen in the past. For one reason or another, movie goers have not been flocking to theaters to see the next big film. The goal of this project is to develop data-driven insights for new movie ideas.\nIn order to derive insights, we will be diving into data from the Internet Movie Database (IMDb). The dataset used in this project analysis comes from the IMDb non-commercial release data tables. We will explore key characteristics of successful movies in history, identify successful actors and filmmakers and examine downfalls in Hollywood history."
  },
  {
    "objectID": "mp02.html#analysis",
    "href": "mp02.html#analysis",
    "title": "Mini-Project #02: The Business of Show Business",
    "section": "Analysis",
    "text": "Analysis\n\nPreparing, Cleaning & Loading the Dataset\nThe following packages will be used for this analysis: dplyr, tidyr, stringr, DT, ggplot2 and plotly. If these packages have not been installed in the system, they can be with the following code:\nif (!require(\"dplyr\")) install.packages(\"dplyr\")\nif (!require(\"tidyr\")) install.packages(\"tidyr\")\nif (!require(\"stringr\")) install.packages(\"stringr\")\nif (!require(\"DT\")) install.packages(\"DT\")\nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\nif (!require(\"gganimate\")) install.packages(\"gganimate\")\nif (!require(\"gifski\")) install.packages(\"gifski\")\nif (!require(\"plotly\")) install.packages(\"plotly\")\nif (!require(\"treemap\")) install.packages(\"treemap\")\nThe dataset used in the analysis contains large files. It will take some time for the data to be downloaded and extracted. The following code will download the files and create six relevant dataframes that will be referenced throughout the analysis.\n\n\nCode\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(DT)\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(gifski)\nlibrary(plotly)\nlibrary(treemap)\n\nget_imdb_file &lt;- function(fname) {\n  BASE_URL &lt;- \"https://datasets.imdbws.com/\"\n  fname_ext &lt;- paste0(fname, \".tsv.gz\")\n  if (!file.exists(fname_ext)) {\n    FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n    download.file(FILE_URL,\n      destfile = fname_ext\n    )\n  }\n  as.data.frame(readr::read_tsv(fname_ext, lazy = FALSE))\n}\n\n\nSort data into tables:\n\n\nCode\nNAME_BASICS      &lt;- get_imdb_file(\"name.basics\")\n\n\n\n\nCode\nTITLE_BASICS     &lt;- get_imdb_file(\"title.basics\")\n\n\n\n\nCode\nTITLE_EPISODES   &lt;- get_imdb_file(\"title.episode\")\n\n\n\n\nCode\nTITLE_RATINGS    &lt;- get_imdb_file(\"title.ratings\")\n\n\n\n\nCode\nTITLE_CREW       &lt;- get_imdb_file(\"title.crew\")\n\n\n\n\nCode\nTITLE_PRINCIPALS &lt;- get_imdb_file(\"title.principals\")\n\n\n\nData Sub-Sampling\nNote that the data we have extracted so far is large enough that we want to further downsize so we have a dataset we can analyze smoothly moving forward. The first thing we will do is modify the NAME_BASICS table to focus on people with at least two “known for” credits. The following code will help downsize the data present in NAME_BASICS.\n\n\nCode\nNAME_BASICS &lt;- NAME_BASICS |&gt; \n    filter(str_count(knownForTitles, \",\") &gt; 1)\n\n\nIMDb includes data for all sorts of media from major studios to independent producers. This includes obscure media that won’t be relevant to us for our analysis. The following code will visualize the distribution of ratings among the titles found on IMDb.\n\n\nCode\nTITLE_RATINGS |&gt;\n  ggplot(aes(x = numVotes)) +\n  geom_histogram(bins = 30) +\n  xlab(\"Number of IMDB Ratings\") +\n  ylab(\"Number of Titles\") +\n  ggtitle(\"Majority of IMDB Titles Have Less than 100 Ratings\") +\n  theme_bw() +\n  scale_x_log10(label = scales::comma) +\n  scale_y_continuous(label = scales::comma)\n\n\n\n\n\n\n\n\n\nThe majority of the titles found in IMDb have less than 100 ratings. We will go ahead and drop any title with less than 100 ratings so our computers are able to run the analysis fluidly. We can apply this drop with the following code:\n\n\nCode\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n    filter(numVotes &gt;= 100)\n\n\nNow that we have downsized the amount of titles we will be analyzing, we can apply the same filtering to our other tables. This can be done by joining the TITLE_RATINGS table with the other TITLE_ tables with a semi_join.\n\n\nCode\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n  semi_join(\n    TITLE_RATINGS,\n    join_by(tconst == tconst)\n  )\n\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n  semi_join(\n    TITLE_RATINGS,\n    join_by(tconst == tconst)\n  )\n\nTITLE_EPISODES_1 &lt;- TITLE_EPISODES |&gt;\n  semi_join(\n    TITLE_RATINGS,\n    join_by(tconst == tconst)\n  )\nTITLE_EPISODES_2 &lt;- TITLE_EPISODES |&gt;\n  semi_join(\n    TITLE_RATINGS,\n    join_by(parentTconst == tconst)\n  )\n\nTITLE_EPISODES &lt;- bind_rows(\n  TITLE_EPISODES_1,\n  TITLE_EPISODES_2\n) |&gt;\n  distinct()\n\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt;\n  semi_join(TITLE_RATINGS, join_by(tconst == tconst))\n\n\nrm(TITLE_EPISODES_1)\nrm(TITLE_EPISODES_2)\n\n\nThe dataset is now more manageable to work with. We can now begin the analysis portion of this project.\n\n\n\nExploring The Data\n\nTidying Data Types\nUsing the glimpse function, we can examine each table to see the type/mode for each column. At first glance, the majority of the columns appear to be a character (string) vector. Some of these columns should be numerical values instead, but due to missing values R converted the missing numerical data into characters instead. We will need to fix this issue by using the mutate function along with as.numeric() and as.logical() on the desired columns. The following code will update the desired columns to the correct format.\n\n\nCode\n# Clean the NAMES_BASICS table, replace missing string values to numeric NA values\nNAME_BASICS &lt;- NAME_BASICS |&gt;\n  mutate(\n    birthYear = as.numeric(birthYear),\n    deathYear = as.numeric(deathYear)\n  )\n\n# TITLE_BASICS has 4 column types to correct, isAdult, startYear, endYear and runtimeMinutes\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n  mutate(\n    isAdult = as.logical(isAdult),\n    startYear = as.numeric(startYear),\n    endYear = as.numeric(endYear),\n    runtimeMinutes = as.numeric(runtimeMinutes)\n  )\n\n# TITLE_EPISODES has 2 column types to correct, seasonNumber and episodeNumber\nTITLE_EPISODES &lt;- TITLE_EPISODES |&gt;\n  mutate(\n    seasonNumber = as.numeric(seasonNumber),\n    episodeNumber = as.numeric(episodeNumber)\n  )\n\n# TITLE_RATINGS has no columns to correct\n\n# TITLE_CREW has the correct column types but I want to convert the \\\\N values to NA instead\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n  mutate(\n    directors = na_if(directors, \"\\\\N\"),\n    writers = na_if(writers, \"\\\\N\")\n  )\n\n# TITLE_PRINCIPALS has the correct column types but I want to convert the \\\\N values to NA instead\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt;\n  mutate(\n    job = na_if(job, \"\\\\N\"),\n    characters = na_if(characters, \"\\\\N\")\n  )\n\n\n\n\n\n\n\n\nCaution\n\n\n\n\n\nThere are a few columns that contain multiple pieces of data in one cell. For example in the NAME_BASICS table, the primaryProfession and knownForTitles columns combine multiple values into one cell. We can use the separate_longer_delim function to break these values into multiple rows. To keep the analysis simple, we will use this function later when answering specific questions instead of breaking up the data beforehand.\n\n\n\n\n\n\nUncovering Insights From the IMDb Data\nThe first step in the analysis is to get a better understanding of the data we are working with. A series of questions are provided to get a grasp of what information we are able to derive from the provided data. To make things easier, creating a schema map of the tables helps us understand the relationships between each table. We will need to use multiple tables to derive insights to our questions.\n\n\n\nSchema of Tables Used\n\n\n\n\n\n\n\n\nThe first set of metrics of interest are:\n\n\n\n\nHow many movies are in our data set? How many TV series? How many TV episodes?\nWho is the oldest living person in our data set?\nThere is one TV Episode in this data set with a perfect 10/10 rating and 200,000 IMDb ratings. What is it? What series does it belong to?\nWhat four projects is the actor Mark Hamill most known for?\nWhat TV series, with more than 12 episodes, has the highest average rating?\nThe TV series Happy Days (1974-1984) gives us the common idiom “jump the shark”. The phrase comes from a controversial fifth season episode (aired in 1977) in which a lead character literally jumped over a shark on water skis. Idiomatically, it is used to refer to the moment when a once-great show becomes ridiculous and rapidly looses quality.\n\nIs it true that episodes from later seasons of Happy Days have lower average ratings than the early seasons?\n\n\n\nThe first thing I want to do is get a breakdown of data in titleType to understand how the media is categorized by IMDb. The simplest way to do so is by running the following code:\n\n\nunique(TITLE_BASICS$titleType)\n\n [1] \"short\"        \"movie\"        \"tvSeries\"     \"tvShort\"      \"tvMovie\"     \n [6] \"tvEpisode\"    \"tvMiniSeries\" \"video\"        \"tvSpecial\"    \"videoGame\"   \n\n\nThe output lets us know exactly what I want to filter for to answer this question. Now that I know what parameters to filter on, the following code will count the amount of media that falls into each category.\n\n\nCode\ncount_types &lt;- TITLE_BASICS |&gt;\n  filter(titleType %in% c(\"movie\", \"tvSeries\", \"tvEpisode\")) |&gt; # for this question I will only use movie, tvSeries, and tvEpisode\n  group_by(titleType) |&gt;\n  summarise(count = n())\n\ndatatable(setNames(count_types, c(\"Type\", \"Total\")),\n  options = list(pageLength = 10, autoWidth = TRUE),\n  caption = \"Table 1: Number of movies, TV series, and TV episodes\"\n)\n\n\n\n\n\n\n\n\nThere are some things to consider when working with the provided data. In the NAME_BASICS table we are provided the columns birthYear and deathYear. The issue at hand to consider is that the value of NA in the deathYear column can either indicate the person is either still alive or the record is incomplete. To tidy this up, as of October 2024 the oldest person alive in the world was born in 1908. I will use this as a benchmark for the birth year when filtering for this person. Another thing to keep in mind is that the data does not provide a month or date, so we cannot distinguish who is older for people that share the same birth year. The following is a list of the 10 oldest living people in the data, however we cannot distinguish any further from the available data.\n\n\n\nCode\noldest_living_person &lt;- NAME_BASICS |&gt;\n  filter(\n    is.na(deathYear), # NA in deathYear indicates person is living\n    birthYear &gt;= 1908\n  ) |&gt; # data is incomplete from deathYear, only considering people born after 1908 as there are incomplete entries (the oldest verified living person in the world as of Oct 2024 was born in 1908)\n  arrange(birthYear) |&gt; # order from oldest to youngest\n  slice_head(n = 10) |&gt;\n  select(primaryName)\n# note that only the birth year is available, no months or dates so this may not be as accurate as I'd like\n\ndatatable(setNames(oldest_living_person, c(\"Name\")),\n  options = list(pageLength = 10, autoWidth = TRUE),\n  caption = \"Table 2: Oldest Living Person in IMDb\"\n)\n\n\n\n\n\n\n\n\nIMDb has their rating system set up so that both a TV episode and TV series has its own separate ratings. For this metric, we want to focus strictly on the TV episode that has a perfect 10/10 rating with 200,000+ ratings. The first step is to identify that TV episode first. Once that has been identified, we can join that specific TV episode with the TITLE_EPISODES and TITLE_BASICS tables to get the series name. We find that the answer is none other than Breaking Bad - Ozymandias.\n\n\n\nCode\n# Find the episode with a perfect rating and over 200,000 ratings\nperfect_episode &lt;- TITLE_BASICS |&gt;\n  left_join(\n    TITLE_RATINGS,\n    join_by(tconst == tconst)\n  ) |&gt;\n  filter(\n    titleType == \"tvEpisode\",\n    averageRating == 10,\n    numVotes &gt;= 200000\n  )\n\n# Join back to TITLE_EPISODES and TITLE_BASICS to get the series name\nperfect_episode_series &lt;- perfect_episode |&gt;\n  left_join(\n    TITLE_EPISODES,\n    join_by(tconst == tconst)\n  ) |&gt;\n  left_join(\n    TITLE_BASICS,\n    join_by(parentTconst == tconst)\n  ) |&gt;\n  rename(\n    episode_title = primaryTitle.x, # rename column to episode title\n    series_title = primaryTitle.y\n  ) |&gt; # rename column to series title\n  select(\n    series_title,\n    episode_title\n  )\nprint(perfect_episode_series)\n\n\n  series_title episode_title\n1 Breaking Bad    Ozymandias\n\n\n\n\nWithin the NAME_BASICS table, each person has an associated list of titles they are known for found in the column knownTitles. This is a case where we want to use the separate_longer_delim function. We can get the titles easily by filtering specifically for Mark Hamill, but the results are stored as the identifier tconst. The last thing we need to do is join our results to the TITLE_BASICS table to get the actual name of the media Mark is known for.\n\n\n\nCode\n# Find the four projects Mark Hamill is known for first\nmark_hamill &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Mark Hamill\") |&gt;\n  separate_longer_delim(knownForTitles, \",\") |&gt;\n  select(knownForTitles)\n# this gives us the IDs, so we need to make further joins to get the names of the projects\n\n# Join results to TITLE_BASICS to get the name of the projects\nmark_hamill_projects &lt;- mark_hamill |&gt;\n  left_join(\n    TITLE_BASICS,\n    join_by(knownForTitles == tconst)\n  ) |&gt;\n  select(primaryTitle)\n\ndatatable(setNames(mark_hamill_projects, c(\"Project Title\")),\n  options = list(pageLength = 10, autoWidth = TRUE),\n  caption = \"Table 3: The 4 Projects Mark Hamill Is Most Known For\"\n)\n\n\n\n\n\n\n\n\nThere are two trains of thoughts I have when approaching this question. A TV series and TV episode both have their own ratings. The first approach is to strictly focus on the average ratings on the TV series itself. The second approach is to find the average ratings among the episodes within a TV series. I will demonstrate how these two distinct approaches produce different answers. Before doing either analysis, I want to create a table that only includes series that have more than 12 episodes.\n\n\n\nCode\ntv_series_12 &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"tvSeries\") |&gt; # Only want TV series\n  left_join(\n    TITLE_EPISODES,\n    join_by(tconst == parentTconst)\n  ) |&gt; # Join with TITLE_EPISODES to count number of episodes\n  group_by(tconst, primaryTitle) |&gt; # Group by series to count episodes\n  summarise(total_episodes = n()) |&gt; # Count total number of episodes\n  filter(total_episodes &gt; 12)\n\n\nNow that we have a list of TV series with more than 12 episodes, we can continue our analysis. The first analysis will look at the TV series ratings itself:\n\n\nCode\nhighest_rated_tv_series &lt;- tv_series_12 |&gt;\n  left_join(\n    TITLE_RATINGS,\n    join_by(tconst == tconst)\n  ) |&gt; # Join with TITLE_RATINGS to get average ratings\n  ungroup() |&gt;\n  arrange(desc(averageRating)) |&gt;\n  select(tconst, primaryTitle, averageRating) |&gt;\n  slice_head(n = 10)\n\ndatatable(setNames(highest_rated_tv_series, c(\"ID\", \"Title\", \"Average Rating\")),\n  options = list(pageLength = 10, autoWidth = TRUE),\n  caption = \"Table 4: Highest Rated TV Series (Series Ratings)\"\n)\n\n\n\n\n\n\nThe second approach is averaging the episode ratings for the series:\n\n\nCode\n# find the series and average rating by aggregating the episode ratings\nseries_ratings_byEpisode &lt;- TITLE_EPISODES |&gt;\n  semi_join(\n    tv_series_12,\n    join_by(parentTconst == tconst)\n  ) |&gt;\n  left_join(\n    TITLE_RATINGS,\n    join_by(tconst == tconst)\n  ) |&gt;\n  group_by(parentTconst) |&gt;\n  summarise(avg_ratings = mean(averageRating)) |&gt;\n  arrange(desc(avg_ratings))\n# join onto the TITLE_BASICS table to get the name of the series\nseries_ratings_byEpisode &lt;- series_ratings_byEpisode |&gt;\n  left_join(\n    TITLE_BASICS,\n    join_by(parentTconst == tconst)\n  ) |&gt;\n  ungroup() |&gt;\n  select(parentTconst, primaryTitle, avg_ratings) |&gt;\n  slice_head(n = 10)\n\ndatatable(setNames(series_ratings_byEpisode, c(\"ID\", \"Title\", \"Average Rating\")),\n  options = list(pageLength = 10, autoWidth = TRUE),\n  caption = \"Table 5: Highest Rated TV Series (Episode Ratings)\"\n) |&gt;\n  formatRound(columns = \"Average Rating\", digits = 2)\n\n\n\n\n\n\nWe can see right away that the results from both approaches are very different from one another. Depending on the context, either answer is suitable. This all depends on how we define the average rating of a series. Some things to consider are for the second approach, it is possible for some episodes to have no ratings at all which can create some skewing of the mean rating. The TV series rating itself can also be different from the average ratings of the episodes.\n\n\nHappy Days(1974 - 1984) has a total of 11 seasons. Since we can’t split it up evenly, I will define the early seasons as seasons 1-6 and the later seasons as season 7-11. To make sure I am looking at the correct series, the first thing to do is find the identifier for Happy Days(1974 - 1984).\n\n\n\nCode\n# Get tconst for the series first so we can find the episode ratings after\nhappy_days_id &lt;- TITLE_BASICS |&gt;\n  filter(\n    primaryTitle == \"Happy Days\",\n    titleType == \"tvSeries\",\n    startYear == 1974,\n    endYear == 1984\n  ) |&gt;\n  select(tconst)\n\n\nNow that tconst has been identified, we can find the average ratings of the early and later seasons.\n\n\nCode\nearly_happy_days_episode_ratings &lt;- TITLE_EPISODES |&gt;\n  semi_join(\n    happy_days_id,\n    join_by(parentTconst == tconst)\n  ) |&gt;\n  left_join(\n    TITLE_RATINGS,\n    join_by(tconst == tconst)\n  ) |&gt;\n  drop_na() |&gt; # get rid of any ratings that are NA\n  filter(seasonNumber &lt; 7) |&gt; # only care about seasons 1-6\n  summarize(avg_rating = mean(averageRating)) |&gt;\n  mutate(season = \"Early Seasons (1-6)\")\n\nlater_happy_days_episode_ratings &lt;- TITLE_EPISODES |&gt;\n  semi_join(\n    happy_days_id,\n    join_by(parentTconst == tconst)\n  ) |&gt;\n  left_join(\n    TITLE_RATINGS,\n    join_by(tconst == tconst)\n  ) |&gt;\n  drop_na() |&gt; # get rid of any ratings that are NA\n  filter(seasonNumber &gt;= 7) |&gt; # only care about seasons 7-11\n  summarize(avg_rating = mean(averageRating)) |&gt;\n  mutate(season = \"Later Seasons (7-11)\")\n\ncombined_ratings &lt;- bind_rows(early_happy_days_episode_ratings, later_happy_days_episode_ratings)\n\nggplot(combined_ratings, aes(x = season, y = avg_rating, fill = season)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(\n    title = \"Average Ratings of Happy Days\",\n    x = \"Seasons\",\n    y = \"Average Rating\"\n  ) +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"Early Seasons (1-6)\" = \"blue\", \"Later Seasons (7-11)\" = \"red\")) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nBased on the data provided, the later seasons did indeed have a lower average rating compared to the earlier seasons."
  },
  {
    "objectID": "mp02.html#quantifying-success",
    "href": "mp02.html#quantifying-success",
    "title": "Mini-Project #02: The Business of Show Business",
    "section": "Quantifying Success",
    "text": "Quantifying Success\nRecall that the goal of the analysis is to develop a new data driven model for coming up with new movie ideas. We want to quantify a movie’s success based on the data we have available to work with. Two metrics we can incorporate into a success factor is the IMDb ratings and number of voters. The IMDb ratings acts as an indicator of quality while the number of voters is an indicator of public popularity.\nI will employ a success score for each movie based on IMDb ratings and number of voters. There are some things to consider beforehand, for example a movie can have a nearly perfect rating but have a small number of voters. We need to account for such cases when scoring the movies. To account for the effects of having a small number of voters, I will be employing a Bayesian average inspired model for my success scoring system. I will not delve into the mathematical theory behind this, but in general terms the Bayesian average will help account for title ratings with a lower number of voters. I encourage you to do your own research on the topic if it interests you.\n\nCustom Success Metric\nBased on the Bayesian average, I will define a new metric called weighted_rating for each movie title as a new rating measurement. The metric will be defined as:\n\\[ weighted\\_rating = [(averageRating * numVotes) / (numVotes + avg\\_num\\_voters)] \\] \\[ + [avg\\_num\\_voters/(numVotes + avg\\_num\\_voters)]*avg\\_movie\\_rating \\]\nAs it stands, the variables avg_num_voters and avg_movie_rating are undefined in our data, but will be defined momentarily. The variable names are representative of what they define, we will be sampling the average number of voters and the average movie rating across all movies in our data. Having these metrics will give us a baseline to work with when quantifying the success of a movie.\nHere we define the average number of voters:\n\n\nCode\navg_num_voters &lt;- TITLE_RATINGS |&gt;\n  left_join(\n    TITLE_BASICS,\n    join_by(tconst == tconst)\n  ) |&gt;\n  filter(titleType == \"movie\") |&gt;\n  summarize(avg_votes = mean(numVotes))\nprint(avg_num_voters)\n\n\n  avg_votes\n1  8691.113\n\n\nBased on the sample, the average number of voters is 8691.414 but for simplicity lets round this to 8700.\nNext, we define the average movie rating:\n\n\nCode\navg_movie_rating &lt;- TITLE_RATINGS |&gt;\n  left_join(\n    TITLE_BASICS,\n    join_by(tconst == tconst)\n  ) |&gt;\n  filter(titleType == \"movie\") |&gt;\n  summarize(avg_rating = mean(averageRating))\nprint(avg_movie_rating)\n\n\n  avg_rating\n1   5.923033\n\n\nThe average movie rating is about 5.9 from our sample.\nWith our baselines variables established, I will now create a new table that consists of movie titles and their weighted rating. Note that this is not the final criteria used to define success. The weighted score is just a new rating number that accounts for the number of people that left a rating on the movie.\n\n\nCode\nmovie_ratings &lt;- TITLE_RATINGS |&gt;\n  left_join(\n    TITLE_BASICS,\n    join_by(tconst == tconst)\n  ) |&gt;\n  filter(titleType == \"movie\") |&gt;\n  mutate(weighted_rating = ((averageRating * numVotes) / (numVotes + 8700)) + ((8700 * 5.9) / (numVotes + 8700))) |&gt;\n  select(tconst, primaryTitle, weighted_rating) |&gt;\n  arrange(desc(weighted_rating))\n\nlimited_movie_ratings &lt;- movie_ratings[1:1000, ] # limit to the first 1000 rows intending to save memory usage\n\ndatatable(setNames(limited_movie_ratings, c(\"ID\", \"Title\", \"Weighted Rating\")),\n  options = list(pageLength = 10, autoWidth = TRUE),\n  caption = \"Table 6: Movie Titles With Weighted Rating\"\n) |&gt;\n  formatRound(columns = \"Weighted Rating\", digits = 2)\n\n\n\n\n\n\nThe next step I will take is creating a new popularity factor to help define success. First, I am taking a look at the maximum & lowest number of voters a movie has.\n\n\nCode\nmax(\n  TITLE_RATINGS |&gt;\n    left_join(TITLE_BASICS, join_by(tconst == tconst)) |&gt;\n    filter(titleType == \"movie\") |&gt;\n    pull(numVotes)\n) # returns 2946100\n\n\n[1] 2952726\n\n\nCode\nmin(\n  TITLE_RATINGS |&gt;\n    left_join(TITLE_BASICS, join_by(tconst == tconst)) |&gt;\n    filter(titleType == \"movie\") |&gt;\n    pull(numVotes)\n) # returns 100\n\n\n[1] 100\n\n\nSince there is such a large discrepancy between the maximum and minimum, a logarithmic scale should be used to scale the number of votes down to a reasonable factor.\n\n\nCode\npopularity_scaling &lt;- TITLE_RATINGS |&gt;\n  left_join(\n    TITLE_BASICS,\n    join_by(tconst == tconst)\n  ) |&gt;\n  filter(titleType == \"movie\") |&gt;\n  mutate(scaled_numVotes = log(numVotes + 1)) |&gt;\n  select(tconst, primaryTitle, scaled_numVotes) |&gt;\n  arrange(desc(scaled_numVotes))\n\nlimited_popularity_scaling &lt;- popularity_scaling[1:1000, ] # limit to the first 1000 rows intending to save memory usage\n\ndatatable(setNames(limited_popularity_scaling, c(\"ID\", \"Title\", \"Scaled Number of Votes\")),\n  options = list(pageLength = 10, autoWidth = TRUE),\n  caption = \"Table 7: Movie Titles With Scaled Votes\"\n) |&gt;\n  formatRound(columns = \"Scaled Number of Votes\", digits = 2)\n\n\n\n\n\n\nTying it all together now, I will define a success score by multiplying the weighted rating and the scaled popularity factor. I’ve defined the new table as TITLE_RATINGS_MOVIES, and will use my new success_score metric for further analysis.\n\n\nCode\nTITLE_RATINGS_MOVIES &lt;- TITLE_RATINGS |&gt;\n  left_join(\n    movie_ratings,\n    join_by(tconst == tconst)\n  ) |&gt;\n  left_join(\n    popularity_scaling,\n    join_by(tconst == tconst)\n  ) |&gt;\n  left_join(\n    TITLE_BASICS,\n    join_by(tconst == tconst)\n  ) |&gt;\n  mutate(success_score = weighted_rating * scaled_numVotes) |&gt;\n  select(tconst, primaryTitle.x, success_score, averageRating, numVotes, startYear) |&gt;\n  arrange(desc(success_score))\n\n\n\n\n\n\n\n\nTo check that the new success metric functions as anticpated, validation will be confirmed with the following:\n\n\n\n\nChoose the top 5-10 movies on your metric and confirm that they were indeed box office successes.\nChoose 3-5 movies with large numbers of IMDb votes that score poorly on your success metric and confirm that they are indeed of low quality.\nChoose a prestige actor or director and confirm that they have many projects with high scores on your success metric.\nPerform at least one other form of ‘spot check’ validation.\nCome up with a numerical threshold for a project to be a ‘success’; that is, determine a value such that movies above are all “solid” or better.\n\n\n\n\nTaking a look at the top 10 movies using my success score, all the movies are box office successes with the exception of The Shawshank Redemption (however this would go onto be a financial success through other mediums). Box office financial information can be found here.\n\n\n\nCode\ntop10_TITLE_RATINGS_MOVIES &lt;- TITLE_RATINGS_MOVIES[1:10, ] # limiting output\n\ntreemap(\n  top10_TITLE_RATINGS_MOVIES,\n  index = \"primaryTitle.x\",\n  vSize = \"success_score\",\n  vColor = \"primaryTitle.x\",\n  type = \"index\",\n  title = \"Top 10 Movies by Success Score\",\n  fontsize.labels = 10,\n  fontsize.title = 14,\n  draw.legend = FALSE\n)\n\n\n\n\n\n\n\n\n\n\n\nTaking a look at the generated list, we can pick out movies with lower success scores and a high number of voters. The 10 lowest scoring movies had at least 20,000 or more voters. Taking a look at the IMDb average rating to measure quality, none of these films were above a 2.0 rating.\n\n\n\nCode\nbad_movies &lt;- TITLE_RATINGS_MOVIES |&gt;\n  arrange(success_score, desc(numVotes))\n\nten_bad_movies &lt;- bad_movies[1:10, ]\n\ndatatable(setNames(ten_bad_movies, c(\"ID\", \"Title\", \"Success Score\", \"Average IMDb Rating\", \"Number of Voters\", \"Release  Year\")),\n  options = list(pageLength = 10, autoWidth = TRUE),\n  caption = \"Table 8: Bad Movies\"\n) |&gt;\n  formatRound(columns = \"Success Score\", digits = 2)\n\n\n\n\n\n\n\n\nFor this validation method, I will be taking a look at Christopher Nolan’s work. Nolan’s works do score high under my defined success metric.\n\n\n\nCode\nNAME_BASICS |&gt; filter(primaryName == \"Christopher Nolan\") # nm0634240, there are more than 1 Christopher Nolan in the database, so I'm making sure I'm choosing the correct one\n\n\n     nconst       primaryName birthYear deathYear        primaryProfession\n1 nm0634240 Christopher Nolan      1970        NA writer,producer,director\n2 nm3059799 Christopher Nolan        NA        NA                      \\\\N\n3 nm9782801 Christopher Nolan        NA        NA        camera_department\n                           knownForTitles\n1 tt6723592,tt0816692,tt1375666,tt0482571\n2 tt1238854,tt0385423,tt0824052,tt1397480\n3         tt16711020,tt10365464,tt5247284\n\n\nCode\nnolan_projects &lt;- NAME_BASICS |&gt;\n  filter(nconst == \"nm0634240\") |&gt;\n  left_join(\n    TITLE_PRINCIPALS,\n    join_by(nconst == nconst)\n  ) |&gt;\n  left_join(\n    TITLE_RATINGS_MOVIES,\n    join_by(tconst == tconst)\n  ) |&gt;\n  select(tconst, primaryTitle.x, success_score) |&gt;\n  drop_na() |&gt;\n  distinct()\n\ndatatable(setNames(nolan_projects, c(\"ID\", \"Title\", \"Success Score\")),\n  options = list(pageLength = 11, autoWidth = TRUE),\n  caption = \"Table 9: Christopher Nolan Projects\"\n) |&gt;\n  formatRound(columns = \"Success Score\", digits = 2)\n\n\n\n\n\n\n\n\nI will check the success scores of the last 5 Oscar winners: Oppenheimer, Everything Everywhere All at Once, CODA, Nomadland & Parasite. All the oscar winners mentioned performed well under my success score.\n\n\n\nCode\noscar_winners &lt;- TITLE_RATINGS_MOVIES |&gt;\n  filter(\n    (primaryTitle.x == \"Oppenheimer\" & startYear == 2023) |\n      (primaryTitle.x == \"Everything Everywhere All at Once\" & startYear == 2022) |\n      (primaryTitle.x == \"CODA\" & startYear == 2021) |\n      (primaryTitle.x == \"Nomadland\" & startYear == 2020) |\n      (primaryTitle.x == \"Parasite\" & startYear == 2019)\n  )\n\ndatatable(setNames(oscar_winners, c(\"ID\", \"Title\", \"Success Score\", \"Average IMDb Rating\", \"Number of Voters\", \"Release Year\")),\n  options = list(pageLength = 10, autoWidth = TRUE),\n  caption = \"Table 10: Last 5 Oscar Winners\"\n) |&gt;\n  formatRound(columns = \"Success Score\", digits = 2)\n\n\n\n\n\n\n\n\nTo come up with a baseline for a “solid” movie, I want to take a look at the distribution of the scores in my rating system. I can use this by looking at the quantile distribution of the success_score. The 75% quantile shows us that 75% of the films fall under 43.6 points, setting this as a benchmark for success. Any movie with a success_score that is greater than or equal 43.6 is a “solid” movie.\n\n\n\nCode\nTITLE_RATINGS_MOVIES |&gt;\n  pull(success_score) |&gt;\n  quantile(na.rm = TRUE)\n\n\n       0%       25%       50%       75%      100% \n 16.98349  31.11881  36.10112  43.59991 138.40482"
  },
  {
    "objectID": "mp02.html#examining-success-by-genre-and-decade",
    "href": "mp02.html#examining-success-by-genre-and-decade",
    "title": "Mini-Project #02: The Business of Show Business",
    "section": "Examining Success by Genre and Decade",
    "text": "Examining Success by Genre and Decade\nNow that a “successful” movie is quantifiable, it is time to uncover trends found over time. Deriving insight from history can help determine what type of movies have been the most successful and point us in a direction when coming up with new movie ideas. The following questions can help determine what type of movie genre should be pursued for a Hollywood success.\n\n\n\n\n\n\nTrends in Success Over Time\n\n\n\n\nWhat was the genre with the most “successes” in each decade?\nWhat genre consistently has the most “successes”? What genre used to reliably produced “successes” and has fallen out of favor?\nWhat genre has produced the most “successes” since 2010? Does it have the highest success rate or does it only have a large number of successes because there are many productions in that genre?\nWhat genre has become more popular in recent years?\n\n\n\n\nIn order to identify the genre with the most “successes”, a new column will need to be created representing the decade the movie was released in. Once a decade has been assigned to each movie, we can further manipulate the data by grouping the genres together by decade to count the total number of movies in each section. Based on my own defined success score, the movie genre drama had the most successes in each decade represented by the data. Keeping this trend in mind, we will dive deeper into the success of dramas shortly.\n\n\n\nCode\nmovie_genre_success &lt;- TITLE_RATINGS_MOVIES |&gt;\n  left_join(\n    TITLE_BASICS,\n    join_by(tconst == tconst)\n  ) |&gt;\n  filter(!is.na(genres) & !is.na(startYear.x)) |&gt;\n  separate_longer_delim(genres, \",\") |&gt;\n  mutate(decade = floor(startYear.x / 10) * 10) |&gt; # create a decade column\n  select(tconst, primaryTitle.x, success_score, startYear.x, decade, genres)\n\ndecade_success &lt;- movie_genre_success |&gt;\n  filter(success_score &gt;= 43.6) |&gt;\n  group_by(decade, genres) |&gt;\n  summarize(\n    total_movies = n(), # Count the total number of successful movies\n    .groups = \"drop\"\n  ) |&gt;\n  group_by(decade) |&gt;\n  slice_max(total_movies, n = 1) |&gt;\n  ungroup()\n\nggplot(decade_success, aes(x = factor(decade), y = total_movies, fill = genres)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") + # Grouped bars\n  labs(\n    title = \"Top Genre per Decade\",\n    x = \"Decade\",\n    y = \"Number of Successful Movies\",\n    fill = \"Genres\"\n  ) +\n  theme_minimal() + # Clean minimal theme\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nOnce again, dramas dominate the genre field over time. Romance movies showed success in the past but have fallen out of favor in recent times. The following plot shows the top 10 genres by decade, the plot is interactive allowing users to manipulate the legend to hide any outputs. While the top genres are easily distinguishable, there are some overlapping data points that can be viewed better by hiding selective genre. Dramas have held onto the top spot throughout the time series, the 1980s was the only decade where comedies came close to taking the number 1 spot.\n\n\n\nCode\n# look at success of genre type by decade again, but this time including 10 genres to gauge how it changes\ndecade_success_top10 &lt;- movie_genre_success |&gt;\n  filter(success_score &gt;= 43.6) |&gt;\n  group_by(decade, genres) |&gt;\n  summarize(\n    total_movies = n(), # Count the total number of successful movies\n    .groups = \"drop\"\n  ) |&gt;\n  group_by(decade) |&gt;\n  slice_max(total_movies, n = 10, with_ties = FALSE) |&gt;\n  mutate(rank = dense_rank(desc(total_movies))) |&gt;\n  ungroup() |&gt;\n  arrange(decade, rank) # Sort by decade and rank\n\n# visually plot\ninteractive_decade_success &lt;- ggplot(decade_success_top10, aes(x = decade, y = total_movies, color = genres, group = genres)) +\n  geom_line(linewidth = 1) + # Plot the lines\n  geom_point(size = 3) + # Add points at each decade\n  labs(\n    title = \"Top 10 Genres per Decade\",\n    x = \"Decade\",\n    y = \"Number of Successful Movies\",\n    color = \"Genres\",\n  ) +\n  scale_x_continuous(breaks = seq(min(decade_success_top10$decade), max(decade_success_top10$decade), by = 10)) + # Set breaks every 10 years\n  theme_minimal() + # Use a clean theme\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"bottom\"\n  )\n# Convert ggplot to interactive plot using plotly\nggplotly(interactive_decade_success)\n\n\n\n\n\n\n\n\nThere is an ongoing trend here, the genre with most successes since 2010 is once again dramas. Taking a look at how many movies in each genre was produced since 2010 can help give context as well. There is definitely an impact by the sheer number of drama movies produced that help it have the highest success rate.\n\n\n\nCode\n# count number of successful projects since 2010 by genre\nsuccesses_since_2010 &lt;- movie_genre_success |&gt;\n  filter(\n    success_score &gt;= 43.6,\n    decade &gt;= 2010\n  ) |&gt;\n  group_by(genres) |&gt;\n  summarize(total_movies = n(), .groups = \"drop\") |&gt;\n  arrange(desc(total_movies))\n\n# count how many movies were made per genre since 2010\nmovie_genres_produced &lt;- movie_genre_success |&gt;\n  filter(decade &gt;= 2010) |&gt;\n  group_by(genres) |&gt;\n  summarize(total_movies_produced = n(), .groups = \"drop\") |&gt;\n  arrange(desc(total_movies_produced))\n\n# combine data\ngenre_distribution_2010 &lt;- left_join(successes_since_2010, movie_genres_produced, by = \"genres\") |&gt;\n  mutate(`Unsuccessful Movies` = total_movies_produced - total_movies) |&gt;\n  arrange(desc(total_movies_produced)) |&gt;\n  slice_head(n = 10)\n\n# pivot the data for long format\ngenre_distribution_2010 &lt;- genre_distribution_2010 |&gt;\n  pivot_longer(\n    cols = c(`Unsuccessful Movies`, total_movies),\n    names_to = \"Type\",\n    values_to = \"Count\"\n  ) |&gt;\n  mutate(Type = ifelse(Type == \"total_movies\", \"Successful Movies\", Type))\n\n# create a dumbbell plot\nggplot(genre_distribution_2010, aes(x = Count, y = reorder(genres, Count), group = Type)) +\n  geom_segment(aes(xend = 0, yend = reorder(genres, Count)), color = \"grey\", size = 1) + # Draw lines to x = 0\n  geom_point(aes(color = Type), size = 3) + # Points for each type\n  labs(\n    title = \"Distribution of Movie Success by Genre Since 2010\",\n    x = \"Number of Movies\",\n    y = \"Genre\"\n  ) +\n  scale_color_manual(values = c(\"Successful Movies\" = \"blue\", \"Unsuccessful Movies\" = \"red\")) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nIf we refer back to the Top 10 Genres per Decade plot, we see that the action genre separated itself from the other genres after the 2000s and has claimed to be the 3rd most popular genre since. Below we can see how the popularity changes over time for each genre to get a better individual understanding.\n\n\n\nCode\nanimate_decade_genre &lt;- ggplot(decade_success_top10, aes(x = decade, y = total_movies, color = genres, group = genres)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 1) +\n  labs(\n    title = \"Top 10 Genres per Decade by Number of Successful Movies\",\n    x = \"Decade\",\n    y = \"Number of Successful Movies\"\n  ) +\n  scale_x_continuous(breaks = seq(min(decade_success_top10$decade), max(decade_success_top10$decade), by = 10)) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 90, hjust = 1),\n    legend.position = \"none\"\n  ) +\n  facet_wrap(~genres) +\n  transition_reveal(decade) # animate the line growth over time (decade)\n\nanimate(animate_decade_genre, renderer = gifski_renderer(file = paste0(save_directory, \"/decade_success_animation.gif\")))\n\n\n\n\n\n\n\n\n\nBased on the insights we’ve uncovered, the genre I would like to pursue for my movie project is drama. According to IMDb, “the drama genre is a broad category that features stories portraying human experiences, emotions, conflicts, and relationships in a realistic and emotionally impactful way”. Taking a closer look at the most successful movies from my own metric in the table below, we see that dramas occupy 9/10 on the list. Due note that the majority of the titles have multiple genres. Drama coexists with other genres on this list, letting us know that success can be created by blending other genres together with drama. While I do want the main focus to be drama, I definitely will want to blend other elements in to give a more refined story.\n\n\nCode\ntop_10 &lt;- TITLE_RATINGS_MOVIES |&gt;\n  left_join(\n    TITLE_BASICS,\n    join_by(tconst == tconst)\n  ) |&gt;\n  select(primaryTitle.x, genres) |&gt;\n  slice_head(n = 10)\n\ndatatable(setNames(top_10, c(\"Title\", \"Genre(s)\")),\n  options = list(pageLength = 10, autoWidth = TRUE),\n  caption = \"Table 11: Top 10 Successful Movies\"\n)"
  },
  {
    "objectID": "mp02.html#successful-personnel-in-the-genre",
    "href": "mp02.html#successful-personnel-in-the-genre",
    "title": "Mini-Project #02: The Business of Show Business",
    "section": "Successful Personnel in the Genre",
    "text": "Successful Personnel in the Genre\nSelecting personnel is an important factor in producing a successful movie. For my movie, I would like to have Christopher Nolan as the director. The first actor I would like to pair is someone who has had success in the past with Nolan. I’ve chosen Cillian Murphy to join my project. The pair has demonstrated they work well together and have successful movies. The graph below shows the four projects they have worked on together. The dotted red line represents the solid success score as discussed earlier. The success scoring system has all of the titles exceeding the baseline, indicating that this director-actor duo is a good choice based on the provided data.\n\n\nCode\nmurphy_nolan_projects &lt;- TITLE_PRINCIPALS |&gt;\n  filter(nconst %in% c(\"nm0634240\", \"nm0614165\")) |&gt;\n  distinct(tconst, nconst) |&gt; # ensure only distinct person-title pairs\n  group_by(tconst) |&gt;\n  summarize(duo_works = n()) |&gt; # counts distinct personnel\n  filter(duo_works == 2) # only keep titles where both are present\n\nmurphy_nolan_projects &lt;- murphy_nolan_projects |&gt;\n  left_join(\n    TITLE_RATINGS_MOVIES,\n    join_by(tconst == tconst)\n  ) |&gt;\n  select(tconst, primaryTitle.x, success_score, startYear)\n\nggplot(murphy_nolan_projects, aes(x = reorder(primaryTitle.x, success_score), y = success_score)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") + # Bar chart with success scores\n  geom_hline(yintercept = 43.6, linetype = \"dashed\", color = \"red\", linewidth = 1) + # horizontal line at y = 43.6 represents solid score\n  labs(\n    title = \"Success Scores of Murphy and Nolan Projects\",\n    x = \"Project Title\",\n    y = \"Success Score\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe next actor I have in mind is an upcoming actor that could potentially become a Hollywood star in the future. I’ve chosen Paul Mescal to join the film, he has experience working in the drama genre and has recently started becoming more popular. At the time of this report, Mescal is set to star in Gladiator 2 which may end up being his big break cementing his name in Hollywood. The table below shows Mescal’s most known works. You can see that dramas are in all four titles, as well as some overlapping genres as well. Mescal’s familiarity with the genre will be an asset for success.\n\n\nCode\nmescal &lt;- NAME_BASICS |&gt;\n  filter(nconst == \"nm8958770\") |&gt;\n  separate_longer_delim(knownForTitles, \",\") |&gt;\n  select(knownForTitles)\n\nmescal &lt;- mescal |&gt;\n  left_join(\n    TITLE_BASICS,\n    join_by(knownForTitles == tconst)\n  ) |&gt;\n  select(primaryTitle, genres)\n\ndatatable(setNames(mescal, c(\"Project Title\", \"Genre(s)\")),\n  options = list(pageLength = 10, autoWidth = TRUE),\n  caption = \"Table 12: Paul Mescal Known Works\"\n)\n\n\n\n\n\n\n\nNostalgia and Remakes\nThe movie I want to remake is Metropolis from 1927. The movie has an average IMDb rating of 8.3 with over 188,000 voters. The movie has drama and sci-fi listed as its genres. It is confirmed there has been no remake of the film since its initial release. I would have Cillian Murphy play Joh Fredersen and Paul Mescal play his son, Freder Fredersen.\n\n\nCode\nmetropolis &lt;- TITLE_BASICS |&gt;\n  filter(primaryTitle == \"Metropolis\",\n         titleType == \"movie\")\n# if we print this original output, we will get two results for Metropolis. However upon further inspection, the Metropolis form 2001 has nothing in common with the movie I am looking to remake. We can move on by specifying the year.\n\nmetropolis &lt;- TITLE_BASICS |&gt;\n  filter(\n    primaryTitle == \"Metropolis\",\n    titleType == \"movie\",\n    startYear == 1927\n  ) |&gt;\n  left_join(\n    TITLE_RATINGS,\n    join_by(tconst == tconst)\n  ) |&gt;\n  select(primaryTitle, startYear, genres, averageRating, numVotes)\n\ndatatable(setNames(metropolis, c(\"Project Title\", \"Release Year\", \"Genre(s)\", \"Average IMDb Rating\", \"Number of Votes\")),\n  caption = \"Table 13: Metropolis\"\n)\n\n\n\n\n\n\nSince the movie is from 1927, the likelihood of any of the key personnel involved in the original film being around is unlikely. We can still double check this using the data provided by IMDb. According to the Metropolis IMDb page the key people are Fritz Lang (director & writer), Thea von Harbou (writer), Brigitte Helm (star), Alfred Abel (star) and Gustav Fröhlich (star). The table below confirms that the key personnel are no longer alive.\n\n\nCode\nmetropolis_personnel &lt;- TITLE_PRINCIPALS |&gt;\n  filter(tconst == \"tt0017136\") |&gt;\n  left_join(NAME_BASICS, join_by(nconst == nconst)) |&gt;\n  select(primaryName, birthYear, deathYear) |&gt;\n  distinct() |&gt;\n  filter(primaryName == \"Fritz Lang\" | primaryName == \"Thea von Harbou\" | primaryName == \"Brigitte Helm\" | primaryName == \"Alfred Abel\" | primaryName == \"Gustav Fröhlich\")\n\ndatatable(setNames(metropolis_personnel, c(\"Name\", \"Birth Year\", \"Death Year\")),\n  caption = \"Table 14: Metropolis Key Personnel\"\n)"
  },
  {
    "objectID": "mp02.html#studio-pitch",
    "href": "mp02.html#studio-pitch",
    "title": "Mini-Project #02: The Business of Show Business",
    "section": "Studio Pitch",
    "text": "Studio Pitch\nDramas have been the bestselling genre in the movie industry in the last century. Between the 1980s to the 2010s, the number of successful drama movies has increased by 394%. Part of the success comes from intertwining the drama genre with other genres to create a more profound story. Among the top ten successful movies, 90% are dramas. Taking a deeper look, 70% of the top ten successful movies blend drama with another genre type. Dramas captivate an audience using emotions and relatability. Tying in another genre adds another layer of complexity that enhances the viewing experience.\nAll of Christopher Nolan’s directorial works have been well received. 58% of the movies Nolan directed included drama as a genre. Cillian Murphy has been a frequent collaborator of Nolan. All their films together have been hits, their most recent work in Oppenheimer won an Oscar award. Recently, Paul Mescal has garnered attention for his work in the drama genre but has yet to reach stardom. Imagine bringing the three individuals together for a drama project. From Christopher Nolan, the visionary mind behind Inception; and from Cillian Murphy, beloved star of Oppenheimer; and from Paul Mescal, television icon of TV dramas, comes the timeless tail of Metropolis, a story of social inequality, dystopian society, and the effects of industrialization coming soon to a theater near you. Metropolis is primed for a remake, with the star power behind the project a box office hit is guaranteed.\n\n\n\nPromotional Metropolis Remake Poster, generated by ChatGPT"
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "",
    "text": "The US electoral college system allocates votes for presidential candidates during the election year. The fine details have changed over the years in history but some basic principles remain the same. Each state receives R+2 electoral votes (ECVs), where R is the number of representatives a state has in the US House of Representatives. States are free to allocate the votes as they like, and the presidential candidate that wins is whoever wins the majority of the electoral college votes.\nThe Constitution has no ruling about how the states need to allocate the ECVs. Each individual state is responsible for their own system implementation. Throughout history, states have chosen to use the following schemes for electoral vote allocation:\n\nState-Wide Winner-Take-All\nDistrict-Wide Winner-Take-All + State-Wide “At Large” Votes\nState-Wide Proportional\nNational Proportional\n\nEach of these allocation strategies will be discussed in-depth later.\nThe objective of this project is to identify how the outcome of the US presidential elections would have changed under different ECVs allocation rules. This will be done by exploring historical congressional election data that is available online."
  },
  {
    "objectID": "mp03.html#introduction",
    "href": "mp03.html#introduction",
    "title": "Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "",
    "text": "The US electoral college system allocates votes for presidential candidates during the election year. The fine details have changed over the years in history but some basic principles remain the same. Each state receives R+2 electoral votes (ECVs), where R is the number of representatives a state has in the US House of Representatives. States are free to allocate the votes as they like, and the presidential candidate that wins is whoever wins the majority of the electoral college votes.\nThe Constitution has no ruling about how the states need to allocate the ECVs. Each individual state is responsible for their own system implementation. Throughout history, states have chosen to use the following schemes for electoral vote allocation:\n\nState-Wide Winner-Take-All\nDistrict-Wide Winner-Take-All + State-Wide “At Large” Votes\nState-Wide Proportional\nNational Proportional\n\nEach of these allocation strategies will be discussed in-depth later.\nThe objective of this project is to identify how the outcome of the US presidential elections would have changed under different ECVs allocation rules. This will be done by exploring historical congressional election data that is available online."
  },
  {
    "objectID": "mp03.html#downloading-importing-the-dataset",
    "href": "mp03.html#downloading-importing-the-dataset",
    "title": "Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Downloading & Importing the Dataset",
    "text": "Downloading & Importing the Dataset\nThe following packages will be used for this analysis: readr, sf, dplyr, tidyr, tidyverse, ggplot2, DT and gganimate. If these packages have not been installed in the system, they can be with the following code:\nif (!require(\"readr\")) install.packages(\"readr\")\nif (!require(\"sf\")) install.packages(\"sf\")\nif (!require(\"dplyr\")) install.packages(\"dplyr\")\nif (!require(\"tidyr\")) install.packages(\"tidyr\")\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif (!require(\"DT\")) install.packages(\"DT\")\nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\nif (!require(\"gganimate\")) install.packages(\"gganimate\")\nLoad the required libraries:\n\n\nCode\nlibrary(readr)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(tidyverse)\nlibrary(DT)\nlibrary(ggplot2)\nlibrary(gganimate)\n\n\nThe first two data files that will need to be downloaded are provided by MIT Election Data Science Lab.1 The data sets we want in particular are the 1976–2022 U.S. House Elections and the 1976-2020 US Presidential Elections. These can be downloaded from the provided links in your browser.\nThe following code will read the CSV files and store them into dataframes to analyze.\n\n\nCode\n# Data 1: US House Election Votes from 1976 to 2022\nhouse_rep_vote_count &lt;- read_csv(\"1976-2022-house.csv\")\npresidential_vote_count &lt;- read_csv(\"1976-2020-president.csv\")\n\n\nThe next set of data needed are the congressional districts shapefiles from 1976 to 2012. The files are provided by Jeffrey B. Lewis, Brandon DeVine, and Lincoln Pritcher with Kenneth C. Martis which can be found here. The following code will automatically download all the necessary files needed.\n\n\nCode\n# Task 1: import congressional district data from cdmaps from 1976 to 2012\nget_cdmaps_file &lt;- function(fname) {\n  BASE_URL &lt;- \"https://cdmaps.polisci.ucla.edu/shp/\"\n  fname_ext &lt;- paste0(fname, \".zip\")\n  if (!file.exists(fname_ext)) {\n    FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n    download.file(FILE_URL,\n                  destfile = fname_ext\n    )\n  }\n}\noptions(timeout = 180) # keep the downloads from potentially timing out\n\nget_cdmaps_file(\"districts112\") # January 5, 2011 to January 3, 2013\nget_cdmaps_file(\"districts111\") # January 6, 2009 to December 22, 2010\nget_cdmaps_file(\"districts110\") # January 4, 2007 to January 3, 2009\nget_cdmaps_file(\"districts109\") # January 4, 2005 to December 9, 2006\nget_cdmaps_file(\"districts108\") # January 7, 2003 to December 8, 2004\nget_cdmaps_file(\"districts107\") # January 3, 2001 to November 22, 2002\nget_cdmaps_file(\"districts106\") # January 6, 1999 to December 15, 2000\nget_cdmaps_file(\"districts105\") # January 7, 1997 to December 19, 1998\nget_cdmaps_file(\"districts104\") # January 4, 1995 to October 4, 1996\nget_cdmaps_file(\"districts103\") # January 5, 1993 to December 1, 1994 \nget_cdmaps_file(\"districts102\") # January 3, 1991 to October 9, 1992\nget_cdmaps_file(\"districts101\") # January 3, 1989 to October 28, 1990\nget_cdmaps_file(\"districts100\") # January 6, 1987 to October 22, 1988\nget_cdmaps_file(\"districts099\")  # January 3, 1985 to October 18, 1986\nget_cdmaps_file(\"districts098\")  # January 3, 1983 to October 12, 1984\nget_cdmaps_file(\"districts097\")  # January 5, 1981 to December 23, 1982\nget_cdmaps_file(\"districts096\")  # January 15, 1979 to December 16, 1980\nget_cdmaps_file(\"districts095\")  # January 4, 1977 to October 15, 1978\nget_cdmaps_file(\"districts094\")  # January 14, 1975 to October 1, 1976\n\n\nThe last set of data needed are the congressional districts shapefiles from 2014 to the present. The files will be taken from the US Census Bureau. The following code will download all necessary files as well.\n\n\nCode\n# Task 2: import data from census\n# download shape files for 113th congress\nif (!file.exists(\"districts113.zip\")) {\n  download.file(\"https://www2.census.gov/geo/tiger/TIGER2013/CD/tl_2013_us_cd113.zip\",\n    destfile = \"districts113.zip\"\n  )\n}\n\n# download shape files for 114th congress\nif (!file.exists(\"districts114.zip\")) {\n  download.file(\"https://www2.census.gov/geo/tiger/TIGER2014/CD/tl_2014_us_cd114.zip\",\n    destfile = \"districts114.zip\"\n  )\n}\n\n# download shape files for 115th congress\nif (!file.exists(\"districts115.zip\")) {\n  download.file(\"https://www2.census.gov/geo/tiger/TIGER2016/CD/tl_2016_us_cd115.zip\",\n    destfile = \"districts115.zip\"\n  )\n}\n\n# download shape files for 116th congress\nif (!file.exists(\"districts116.zip\")) {\n  download.file(\"https://www2.census.gov/geo/tiger/TIGER2018/CD/tl_2018_us_cd116.zip\",\n    destfile = \"districts116.zip\"\n  )\n}"
  },
  {
    "objectID": "mp03.html#analysis",
    "href": "mp03.html#analysis",
    "title": "Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Analysis",
    "text": "Analysis\n\nExploration of Vote Count Data\nNow that all the necessary data has been imported and ready to use, we can begin exploring the information we have available to us. The following questions can be answered by exploring the data.\nWhich states have gained and lost the most seats in the US House of Representatives between 1976 and 2022?\n\n\nCode\n# find the number of seats each state has in 1976 and 2022\nhouse_seats_1976_2022 &lt;- house_rep_vote_count |&gt;\n  filter(year %in% c(1976, 2022)) |&gt;\n  group_by(year, state) |&gt;\n  summarise(total_seats = n_distinct(district)) |&gt; # count number of seats by number of electoral districts\n  select(year, state, total_seats)\n\n# pivot table to find difference easily: state, 1976 seats, 2022 seats\nhouse_seats_1976_2022_wide &lt;- house_seats_1976_2022 |&gt;\n  pivot_wider(names_from = year, values_from = total_seats, names_prefix = \"total_seats_\") |&gt;\n  mutate(difference = total_seats_2022 - total_seats_1976)\n\n# find the change in seats from 2022 to 1976\nseat_changes &lt;- house_seats_1976_2022_wide |&gt;\n  select(state, difference)\n\n# visual representation\nseat_changes_filtered &lt;- seat_changes |&gt; # excluding zero for visual aesthetic \n  filter(difference != 0)\n\nggplot(seat_changes_filtered, aes(x = reorder(state, difference), y = difference, fill = difference &gt; 0)) +\n  geom_bar(stat = \"identity\", show.legend = FALSE) +\n  scale_fill_manual(values = c(\"salmon2\", \"cornflowerblue\")) +  # Blue for increases, red for decreases\n  coord_flip() +  # Flip coordinates for horizontal bars\n  labs(title = \"House Seats Gained/Lost by State (1976-2022)\",\n       x = \"State\",\n       y = \"Change in Seats\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nEvidently, Texas gained the most number of seats in the house of representatives between 1976-2022. On the other hand, New York lost the most since in the same time frame. This indicates that Texas had the largest proportional change in population since 1976. The overall population of the US has shifted away from New York and into other states over the years. The tables below show the exact number of seats changed over the time period.\n\n\nCode\n# take a closer look at the exact number of seats gained\ngained_seats &lt;- seat_changes |&gt;\n  arrange(desc(difference)) |&gt;\n  filter(difference &gt; 0)\n\ndatatable(setNames(gained_seats, c(\"State\", \"Seats Gained\")),\n          options = list(pageLength = 10, autoWidth = TRUE),\n          caption = \"Table 1: House Seats Gained (1976-2022)\"\n)\n\n\n\n\n\n\n\n\nCode\n# take a closer look at the exact number of seats lost\nlost_seats &lt;- seat_changes |&gt;\n  arrange(difference) |&gt;\n  filter(difference &lt; 0)\n\ndatatable(setNames(lost_seats, c(\"State\", \"Seats Lost\")),\n          options = list(pageLength = 10, autoWidth = TRUE),\n          caption = \"Table 2: House Seats Lost (1976-2022)\"\n)\n\n\n\n\n\n\n\nAre there any elections in our data where the election would have had a different outcome if the “fusion” system was not used and candidates only received the votes their received from their “major party line” (Democrat or Republican) and not their total number of votes across all lines?\nA “fusion” voting system is when one candidate can appear on multiple “lines” on the ballot and their vote counts are totaled.\n\n\nCode\n# find the historical winner based on the total votes received on tickets (which includes the fusion system)\nhouse_election_winner &lt;- house_rep_vote_count |&gt;\n  group_by(year, state, district, candidate) |&gt;\n  summarize(total_votes = sum(candidatevotes), .groups = 'drop') |&gt; # aggregate votes across different parties on ticket for fusion ticket candidates\n  group_by(year, state, district) |&gt;\n  slice_max(order_by = total_votes, n = 1, with_ties = FALSE) |&gt; # find the winner based on who has the most total votes\n  rename(historical_winner = candidate) # rename for conventional understanding\n  \n# find winner without fusion system\nprimary_party_winner &lt;- house_rep_vote_count |&gt;\n  group_by(year, state, district, candidate, party) |&gt;\n  summarize(primary_party_votes = sum(candidatevotes), .groups = \"drop\") |&gt; \n  group_by(year, state, district) |&gt;\n  slice_max(order_by = primary_party_votes, n = 1, with_ties = FALSE) |&gt;\n  rename(single_party_winner = candidate) |&gt; # rename for conventional understanding\n  select(-party) # deselecting since I'm only interested in the candidate name and votes\n\n# find any elections where the historical winner is not the same as the single major party winner\npotential_election_changes &lt;- house_election_winner |&gt;\n  left_join(primary_party_winner, by = c(\"year\", \"state\", \"district\")) |&gt;\n  mutate(different_outcome = historical_winner != single_party_winner) |&gt; # create a logic column checks if the outcomes were the same or not\n  filter(different_outcome == TRUE) |&gt; # filter where the historical winner is not the same as the single party vote winner\n  select(-different_outcome)\n\ndatatable(setNames(potential_election_changes, c(\"Year\", \"State\", \"District\", \"Historical Winner\", \"Votes (with fusion)\", \"Single Party Winner\", \"Single Party Votes\")),\n          options = list(pageLength = 10, autoWidth = TRUE),\n          caption = \"Table 3: Potential Differences in House Elections Due to Fusion Voting\"\n)  \n\n\n\n\n\n\n\n \n\nThere are 24 district elections in history where the outcome could have been different if the fusion voting system was not in place. The majority of these occurrences take place in New York.\n\nDo presidential candidates tend to run ahead of or run behind congressional candidates in the same state? That is, does a Democratic candidate for president tend to get more votes in a given state than all Democratic congressional candidates in the same state?\nDoes this trend differ over time? Does it differ across states or across parties? Are any presidents particularly more or less popular than their co-partisans?\n\n\nCode\n# aggregate votes by year, state, and party for house candidates\ncongressional_party_votes &lt;- house_rep_vote_count |&gt;\n  group_by(year, state, party) |&gt;\n  summarize(total_congressional_votes = sum(candidatevotes, na.rm = TRUE), .groups = \"drop\")\n\n# aggregate votes by year, state, and party for presidential candidates\npresidential_party_votes &lt;- presidential_vote_count |&gt;\n  group_by(year, state, party_detailed) |&gt;\n  summarize(total_presidential_votes = sum(candidatevotes, na.rm = TRUE), .groups = \"drop\") |&gt;\n  rename(party = party_detailed) # renaming for joining convention\n\n# find difference between presidential votes and congressional\nvote_disparity &lt;- presidential_party_votes |&gt;\n  inner_join(congressional_party_votes, by = c(\"year\", \"state\", \"party\")) |&gt;\n  mutate(vote_difference = total_presidential_votes - total_congressional_votes) |&gt;\n  select(-total_presidential_votes, -total_congressional_votes)\n\n# focus on Democrat and Republican parties for trends since these are the two major parties\n\n# group by year and party summing across the United States\nvote_disparity_year &lt;- vote_disparity |&gt;\n  filter(party %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  group_by(year, party) |&gt;\n  summarize(total_vote_difference = sum(vote_difference))\n\nggplot(vote_disparity_year,\n       aes(x = year,\n           y = total_vote_difference,\n           color = party)) +\n  geom_point() +\n  geom_line() +\n  scale_color_manual(values = c(\"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\")) +\n  labs(title = \"Presidential Votes Disparity\",\n       subtitle = \"Difference between presidential votes and house rep votes aggregated over all states.\",\n       x = \"Year\",\n       y = \"Vote Difference\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nOver time, the Democratic presidential nominee has gained more votes in the US than their house constituents. The Republican presidential nominee has slowly lost popularity and received similar votes to their house constituents over time.\nThe 1984 and 1988 presidential elections were the only elections where the Republican candidate received at least 10 million more votes than their co-partisans running for the house of representatives. The candidates at the time were Ronald Reagan and George H.W. Bush respectively, both of which ended up winning the presidential election.\n\n\nCode\n# group by state and party\nvote_disparity_state &lt;- vote_disparity |&gt;\n  filter(party %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  group_by(state, party) |&gt;\n  summarize(average_vote_difference = round(mean(vote_difference), digits = 0))\n\ndatatable(setNames(vote_disparity_state, c(\"State\", \"Party\", \"Average Vote Difference\")),\n  options = list(pageLength = 10, autoWidth = TRUE),\n  caption = \"Table 4: Average Vote Difference Presidential Candidate Receives vs House Reps\"\n)\n\n\n\n\n\n\n\n \n\nThe table above shows the average vote difference that the presidential candidate receives as compared to their house representative counterparts. The average varies from state to state, exploring the entire table can provide insight into how citizens vote in each state. An interesting insight is that Massachusetts has historically leaned blue in the presidential race, but on average the presidential candidate receives less votes than the constituents combined. Florida tends to be a battleground state in presidential election years, this is seen as both parties draw a significant amount of more votes in for the presidential candidate than the house representatives.\n\n\nChoropleth Maps & Shapefiles\nEarlier we downloaded zip files for the US congressional districts. In order to access the shapefiles, we can automate a zip file extraction with some code.\n\n\nCode\n# Task 4: Automate Zip File Extraction\n# the following code writes a function that can read any zip file and extract the shapefile within\nread_shp_from_zip &lt;- function(zip_file) {\n  temp_dir &lt;- tempdir() # Create a temporary directory\n  zip_contents &lt;- unzip(zip_file, exdir = temp_dir) # Unzip the contents and\n  shp_file &lt;- zip_contents[grepl(\"\\\\.shp$\", zip_contents)] # filter for .shp files\n  sf_object &lt;- read_sf(shp_file) # Read the .shp file into an sf object\n  return(sf_object) # Return the sf object\n}\n\n\nTo get a better idea of what can be done with the shapefiles, a chloropleth visualization of the electoral college results for the 2000 presidential election will be created.\n\n\nCode\nzip_file &lt;- \"districts106.zip\" # district 106 corresponds to when the 2000 elections took place\nshapefile_us &lt;- read_shp_from_zip(zip_file)\n\n# Task 5: Choropleth Visualization of 2000 Electoral College Results\n\n# find election results in each state\nwinner_2000_election &lt;- presidential_vote_count |&gt;\n  filter(year == 2000) |&gt; # 2000 election\n  group_by(state, party_simplified) |&gt; # I want to find the total votes by state and party\n  summarize(total_votes = sum(candidatevotes), .groups = \"drop\") |&gt; # sum the votes across all districts\n  group_by(state) |&gt; # group by state to find the top candidate in each state\n  slice_max(total_votes, n = 1) |&gt;\n  ungroup() |&gt;\n  select(state, party_simplified) |&gt; # don't care for the amount of votes, just the winner\n  rename(winning_party = party_simplified) # rename for convention, will use to fill the choropleth\n\n# join the shape file to election results\nshapefile_us_2000 &lt;- shapefile_us |&gt;\n  mutate(STATENAME = toupper(trimws(STATENAME))) |&gt; # need to match the characters from both tables to join correctly\n  left_join(\n    winner_2000_election,\n    join_by(STATENAME == state)\n  )\n\n\n# create Choropleth of contiguous US first\ncontiguous_us &lt;- ggplot(shapefile_us_2000,\n  aes(\n    geometry = geometry,\n    fill = winning_party\n  ),\n  color = \"black\"\n) +\n  geom_sf() +\n  scale_fill_manual(values = c(\"DEMOCRAT\" = \"skyblue3\", \"REPUBLICAN\" = \"firebrick1\")) +\n  theme_minimal() +\n  labs(\n    title = \"Presidential Election State Results 2000\",\n    subtitle = \"Choropleth Map of U.S. Districts\",\n    fill = \"Winning Party\"\n  ) +\n  theme(legend.position = \"bottom\") +\n  coord_sf(xlim = c(-130, -60), ylim = c(20, 50), expand = FALSE)\n\n# Alaska inset\nalaska_sf &lt;- shapefile_us_2000[shapefile_us_2000$STATENAME == \"ALASKA\", ] # pull Alaska sf info to plot individually\ninset_alaska &lt;- ggplot(alaska_sf,\n  aes(\n    geometry = geometry,\n    fill = winning_party\n  ),\n  color = \"black\"\n) +\n  geom_sf() +\n  scale_fill_manual(values = c(\"DEMOCRAT\" = \"skyblue3\", \"REPUBLICAN\" = \"firebrick1\")) +\n  theme_void() +\n  theme(legend.position = \"none\") + # will display legend on the contiguous plot\n  coord_sf(xlim = c(-180, -140), ylim = c(50, 72), expand = FALSE)\n\n# Hawaii inset\nhawaii_sf &lt;- shapefile_us_2000[shapefile_us_2000$STATENAME == \"HAWAII\", ] # pull Hawaii sf info to plot individually\ninset_hawaii &lt;- ggplot(hawaii_sf,\n  aes(\n    geometry = geometry,\n    fill = winning_party\n  ),\n  color = \"black\"\n) +\n  geom_sf() +\n  scale_fill_manual(values = c(\"DEMOCRAT\" = \"skyblue3\", \"REPUBLICAN\" = \"firebrick1\")) +\n  theme_void() +\n  theme(legend.position = \"none\") + # will display legend on the contiguous plot\n  coord_sf(xlim = c(-161, -154), ylim = c(18, 23), expand = FALSE)\n\ncombined_map &lt;- contiguous_us +\n  annotation_custom(ggplotGrob(inset_alaska),\n    xmin = -120, xmax = -130, # Adjust position for Alaska\n    ymin = 15, ymax = 40\n  ) +\n  annotation_custom(ggplotGrob(inset_hawaii),\n    xmin = -115, xmax = -100, # Adjust position for Hawaii\n    ymin = 20, ymax = 30\n  ) # Adjust these values to fit\n\n# Print the combined map\nprint(combined_map)\n\n\n\n\n\n\n\n\n\n\nTaking it a step further, we can see how the election results changed over the years in each states with an animated plot.\n\n\nCode\n# Task 6: Advanced Choropleth Visualization of Electoral College Results\n# Modify your previous code to make an animated version showing election results over time.\n\n# write a function that gets the presidential election winners in each desired election year\n\nelection_years &lt;- c(1976, 1980, 1984, 1988, 1992, 1996, 2000, 2004, 2008, 2012, 2016, 2020) # election years in the data\n\n# majority of the code is the same as above used to find the winner in 2000, but this time including the year\nget_winner_election_year &lt;- function(input_year) {\n  presidential_vote_count |&gt;\n    filter(year == input_year) |&gt; # Filter for the specific year\n    group_by(state, party_simplified) |&gt;\n    summarize(total_votes = sum(candidatevotes), .groups = \"drop\") |&gt;\n    group_by(state) |&gt;\n    slice_max(total_votes, n = 1) |&gt;\n    ungroup() |&gt;\n    select(state, party_simplified) |&gt;\n    rename(winning_party = party_simplified) |&gt;\n    mutate(year = input_year) # add year to the table\n}\n\n# bind the results into one table for time frame animation\nelection_winner_by_year &lt;- bind_rows(lapply(election_years, get_winner_election_year)) # lapply() will take the arguments in election_years and input it into the get_winner_election_year function\n\n# unzip shape file for states\nshapefile_states &lt;- read_shp_from_zip(\"tl_2018_us_state.zip\")\n\n# join the US states shapefile to the list of election winners\n\nshapefile_states &lt;- shapefile_states|&gt;\n  mutate(NAME = toupper(trimws(NAME))) |&gt; # need to match the characters from both tables to join correctly\n  left_join(election_winner_by_year,\n    join_by(NAME == state),\n    relationship = \"many-to-many\"\n  ) |&gt;\n  filter(!is.na(year))\n\n# animated plot\nanimate_election &lt;- ggplot(shapefile_states,\n  aes(\n    geometry = geometry,\n    fill = winning_party\n  ),\n  color = \"black\"\n) +\n  geom_sf() +\n  scale_fill_manual(values = c(\"DEMOCRAT\" = \"skyblue3\", \"REPUBLICAN\" = \"firebrick1\")) +\n  theme_minimal() +\n  labs(\n    title = \"Presidential Election State Results {closest_state}\",\n    subtitle = \"Choropleth Map of U.S. States\",\n    fill = \"Winning Party\"\n  ) +\n  theme(legend.position = \"bottom\") +\n  transition_states(year, transition_length = 0, state_length = 1) +\n  coord_sf(xlim = c(-175, -60), expand = FALSE)\n\nanimate(animate_election, renderer = gifski_renderer(file = paste0(save_directory, \"/election_results_animation.gif\"))) # save as a GIF\n\n\n\n\n\n\n\n\n\n\n\n\nComparing the Effects of ECV Allocation Rules\nTaking a look at the ECVs allocation schemes mentioned earlier, we can explore historical data to compare the derive the winning presidential candidate under each scheme and compare it to the actual historical winner.\nThe first task at hand is to identify the number of electoral votes each state has in each election year as it has changed over time.\n\n\nCode\n# Task 7: Evaluating Fairness of ECV Allocation Schemes\n# find number of electoral votes each state has each year\nelectoral_votes_by_state &lt;- house_rep_vote_count |&gt;\n  group_by(year, state) |&gt;\n  summarize(total_reps = n_distinct(district)) |&gt;\n  mutate(electoral_votes = total_reps + 2) |&gt; # R+2 votes\n  select(year, state, electoral_votes)\n\n# add DC votes\nelectoral_votes_by_state &lt;- electoral_votes_by_state |&gt;\n  ungroup() |&gt;\n  add_row(year = 1976, state = \"DISTRICT OF COLUMBIA\", electoral_votes = 3) |&gt;\n  add_row(year = 1980, state = \"DISTRICT OF COLUMBIA\", electoral_votes = 3) |&gt;\n  add_row(year = 1984, state = \"DISTRICT OF COLUMBIA\", electoral_votes = 3) |&gt;\n  add_row(year = 1988, state = \"DISTRICT OF COLUMBIA\", electoral_votes = 3) |&gt;\n  add_row(year = 1992, state = \"DISTRICT OF COLUMBIA\", electoral_votes = 3) |&gt;\n  add_row(year = 1996, state = \"DISTRICT OF COLUMBIA\", electoral_votes = 3) |&gt;\n  add_row(year = 2000, state = \"DISTRICT OF COLUMBIA\", electoral_votes = 3) |&gt;\n  add_row(year = 2004, state = \"DISTRICT OF COLUMBIA\", electoral_votes = 3) |&gt;\n  add_row(year = 2008, state = \"DISTRICT OF COLUMBIA\", electoral_votes = 3) |&gt;\n  add_row(year = 2012, state = \"DISTRICT OF COLUMBIA\", electoral_votes = 3) |&gt;\n  add_row(year = 2016, state = \"DISTRICT OF COLUMBIA\", electoral_votes = 3) |&gt;\n  add_row(year = 2020, state = \"DISTRICT OF COLUMBIA\", electoral_votes = 3) |&gt;\n  distinct() # avoid any duplicate entries in case\n\n\n\nState-Wide Winner-Take-All\nWhichever candidate receives the most votes in the state wins all the electoral votes.\n\n\nCode\n# find the candidate with the most votes each year in each state\nstate_wide_winner_take_all &lt;- presidential_vote_count |&gt;\n  group_by(year, state, candidate) |&gt;\n  summarize(total_votes = sum(candidatevotes), .groups = \"drop\") |&gt;\n  group_by(year, state) |&gt;\n  slice_max(order_by = total_votes, n = 1, with_ties = FALSE) |&gt; # find the winner of each state based on who has the most total votes\n  rename(winner = candidate) # rename for conventional understanding\n\n# join the state, winner, and number of electoral votes & sum which candidate that gets the most electoral votes\nstate_wide_winner_take_all &lt;- state_wide_winner_take_all |&gt;\n  left_join(electoral_votes_by_state,\n    by = c(\"year\", \"state\")\n  ) |&gt;\n  group_by(year, winner) |&gt;\n  summarize(total_electoral_votes = sum(electoral_votes)) |&gt; # sum electoral votes across all states by candidate each year\n  slice_max(order_by = total_electoral_votes, n = 1, with_ties = FALSE)\n\ndatatable(setNames(state_wide_winner_take_all, c(\"Year\", \"Winning Candidate\", \"Electoral Votes\")),\n          options = list(pageLength = 12, autoWidth = TRUE),\n          caption = \"Table 5: State-Wide Winner-Take-All: Presidential Winning Candidate\"\n)\n\n\n\n\n\n\n\n\nDistrict-Wide Winner-Take-All + State-Wide “At Large” Votes\nEach district electoral vote is assigned to the district winner, and whichever candidate wins the popular vote in the state wins the remaining 2 at large votes.\nAssume the winning party of the house district is the same for the president.\n\n\nCode\n# find number of districts each party won to represent electoral votes won in each state\ndistrict_winner &lt;- house_rep_vote_count |&gt;\n  group_by(year, state, district) |&gt;\n  slice_max(order_by = candidatevotes, n = 1, with_ties = FALSE) |&gt;\n  select(year, state, district, party) |&gt;\n  group_by(year, state, party) |&gt;\n  summarize(districts_won = n()) # number of electoral votes received by each party\n\n# find popular vote winner in the state\nat_large_winner &lt;- house_rep_vote_count |&gt;\n  group_by(year, state) |&gt;\n  slice_max(order_by = candidatevotes, n = 1, with_ties = FALSE) |&gt;\n  select(year, state, party) |&gt;\n  add_column(at_large_votes = 2) # designating the vote count\n\n# join tables together to find total electoral votes the presidential party receives in each state\ndistrict_wide_winner_take_all &lt;- district_winner |&gt;\n  left_join(at_large_winner,\n    by = c(\"year\", \"state\", \"party\")\n  ) |&gt;\n  mutate(across(where(is.numeric), ~ ifelse(is.na(.), 0, .))) |&gt; # set NA to 0 for the rows that had no resulting joins\n  mutate(total_electoral_votes = districts_won + at_large_votes) |&gt;\n  select(-districts_won, -at_large_votes) |&gt;\n  rename(party_simplified = party) |&gt; # rename for easier joining convention\n  left_join(presidential_vote_count,\n    by = c(\"year\", \"state\", \"party_simplified\")\n  ) |&gt; # join to presidential candidate\n  select(year, state, total_electoral_votes, candidate) |&gt;\n  group_by(year, candidate) |&gt;\n  summarize(electoral_votes = sum(total_electoral_votes)) |&gt;\n  slice_max(order_by = electoral_votes, n = 1, with_ties = FALSE) |&gt;\n  drop_na() # get rid of the non-presidential election years\n\ndatatable(setNames(district_wide_winner_take_all, c(\"Year\", \"Winning Candidate\", \"Electoral Votes\")),\n          options = list(pageLength = 12, autoWidth = TRUE),\n          caption = \"Table 6: District-Wide Winner-Take-All: Presidential Winning Candidate\"\n)\n\n\n\n\n\n\n\n\nState-Wide Proportional\nElectoral votes are distributed based on the percentage of the popular vote the candidate receives in the state.\n\n\nCode\n# find the percentage of the votes received in each state\nstate_wide_proportional &lt;- presidential_vote_count |&gt;\n  select(year, state, candidate, candidatevotes, totalvotes) |&gt;\n  mutate(percentage_state_votes = (candidatevotes / totalvotes)) |&gt;\n  select(-candidatevotes, -totalvotes)\n\n# find the number of electoral votes received by each candidate\nstate_wide_proportional &lt;- state_wide_proportional |&gt;\n  left_join(electoral_votes_by_state,\n    by = c(\"year\", \"state\")\n  ) |&gt;\n  mutate(votes_received = round(percentage_state_votes * electoral_votes, digits = 0)) |&gt;\n  select(-percentage_state_votes, -electoral_votes)\n\n# sum total votes and find presidential winner\nstate_wide_proportional &lt;- state_wide_proportional |&gt;\n  group_by(year, candidate) |&gt;\n  summarize(total_electoral_votes = sum(votes_received)) |&gt;\n  slice_max(order_by = total_electoral_votes, n = 1, with_ties = FALSE) |&gt;\n  rename(winner = candidate)\n\ndatatable(setNames(state_wide_proportional, c(\"Year\", \"Winning Candidate\", \"Electoral Votes\")),\n          options = list(pageLength = 12, autoWidth = TRUE),\n          caption = \"Table 7: State-Wide Proportional: Presidential Winning Candidate\"\n)\n\n\n\n\n\n\n\n\nNational Proportional\nTake a look at the overall popular vote across the US, the candidate receives the number of electoral votes based on the percentage of the popular vote they received.\n\n\nCode\n# find total number of electoral votes available\nelectoral_votes_available &lt;- electoral_votes_by_state |&gt;\n  group_by(year) |&gt;\n  summarize(electoral_college_votes = sum(electoral_votes))\n\n# find percentage of popular vote each candidate received\nnational_proportional &lt;- presidential_vote_count |&gt;\n  select(year, state, candidate, candidatevotes) |&gt;\n  group_by(year, candidate) |&gt;\n  summarize(total_electoral_votes = sum(candidatevotes)) |&gt;\n  group_by(year) |&gt;\n  mutate(population_vote_count = sum(total_electoral_votes)) |&gt; # find total number of votes cast in election year\n  ungroup() |&gt;\n  mutate(percentage_population_vote = (total_electoral_votes / population_vote_count)) |&gt;\n  select(-total_electoral_votes, -population_vote_count) |&gt;\n  # find the proportion of the electoral votes received based on the popular vote percentage\n  left_join(\n    electoral_votes_available,\n    join_by(year == year)\n  ) |&gt;\n  mutate(electoral_votes_received = round(percentage_population_vote * electoral_college_votes, digits = 0)) |&gt;\n  select(-percentage_population_vote, -electoral_college_votes) |&gt;\n  group_by(year) |&gt;\n  slice_max(order_by = electoral_votes_received, n = 1, with_ties = FALSE) |&gt;\n  rename(winner = candidate)\n\ndatatable(setNames(national_proportional, c(\"Year\", \"Winning Candidate\", \"Electoral Votes\")),\n          options = list(pageLength = 12, autoWidth = TRUE),\n          caption = \"Table 8: National Proportional: Presidential Winning Candidate\"\n)\n\n\n\n\n\n\n\n \n\nTo do so, you should first determine which allocation scheme you consider “fairest”. You should then see which schemes give different results, if they ever do. To make your fact check more compelling, select one election where the ECV scheme had the largest impact–if one exists–and explain how the results would have been different under a different ECV scheme.\nEach electoral college vote allocation scheme has its own pros and drawbacks. I believe that the national proportional allocation scheme is the “fairest” strategy out of all schemes. It is representative of the entire nation’s voting populations interests. The president is the governing entity of the entire nation, the national proportional scheme represents the entire nation’s interests as a whole. The population of the United States is not distributed evenly across all the states, thus this scheme is fair since it accounts for everyone’s vote regardless of where they reside.\nThe state-wide proportional scheme is the next “fairest” allocation system that can be implemented. It is similar to the national-proportional in nature but it takes a step down and looks at the interests of the voters by state instead. Since different opinions can exist throughout a state, this allocates a fairer distribution of votes. The only drawback is that the representation is not on a national level, but on a state by state level. There can be a slight skew in favoritism since each state has a different amount of electoral votes available.\nIn my opinion, the state-wide winner-take-all and district-wide winner-take-all are the least “fair” allocation systems. They do not offer a true representation of the nation’s interests, but focus on a subset of opinions that may not be commonly held by the entire nation. The state-wide winner-take-all system suffers from representing the entire sentiment of the voters in a state. A small majority win can tip the balance to one party, whilst ignoring the remaining voter’s opinions in the state. The district-wide winner-take-all system examines the opinions of voter’s on a district level. Although this may seem fair since each district has a voice, the distribution can be skewed since the districts don’t represent the population size located within. Political tactics like gerrymandering can greatly effect the voices of voters and not reflect the overall voter preferences.\nThe 2000 presidential election is an example of where the electoral college vote scheme had the greatest impact on the results. The national proportional scheme solely determined that Al Gore would have won the election. Every other scheme resulted in George W. Bush winning the election, including the historical one."
  },
  {
    "objectID": "mp03.html#footnotes",
    "href": "mp03.html#footnotes",
    "title": "Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMIT Election Data + Science Lab. (n.d.). MIT Election Lab. MIT Election Data + Science Lab. https://electionlab.mit.edu/↩︎"
  }
]