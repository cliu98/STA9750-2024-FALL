[
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "",
    "text": "The public transit systems found nationwide in the United States play an important role in mobilizing people in their daily lives. This analysis will give an overview of how the transit systems perform relative to another by examining key metrics such as farebox revenues, total number of trips, total vehicle miles traveled and total revenues and expenses by source.\nThis analysis will examine information from 2022 that utilizes data from fare revenue, monthly ridership, and operating expense reports. By evaluating the metrics mentioned earlier, this analysis will identify trends, expose common challenges, and offer insight into transit performances. Various transit performance metrics will be analyzed to gain a general overview of the data. Based on these findings, transit system efficiency will be evaluated on a comparative basis. The working data sets are provided by the Federal Transit Administration. The latter half of the analysis will examine metrics that can define a transit system as efficient. The evaluation of efficiency will depend on the interpretation of what makes something efficient."
  },
  {
    "objectID": "mp01.html#introduction",
    "href": "mp01.html#introduction",
    "title": "Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Introduction",
    "text": "Introduction\nThe public transit systems found nationwide in the United States play an important role in mobilizing people in their daily lives. This analysis will give an overview of how the transit systems perform relative to another by examining key metrics such as farebox revenues, total number of trips, total vehicle miles traveled and total revenues and expenses by source.\nThis analysis will examine information from 2022 that utilizes data from fare revenue, monthly ridership, and operating expense reports. By evaluating the metrics mentioned earlier, this analysis will identify trends, expose common challenges, and offer insight into transit performances. Various transit performance metrics will be analyzed to gain a general overview of the data. Based on these findings, transit system efficiency will be evaluated on a comparative basis. The working data sets are provided by the Federal Transit Administration. The latter half of the analysis will examine metrics that can define a transit system as efficient. The evaluation of efficiency will depend on the interpretation of what makes something efficient."
  },
  {
    "objectID": "mp01.html#analysis",
    "href": "mp01.html#analysis",
    "title": "Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Analysis",
    "text": "Analysis\n\nPreparing, Cleaning & Loading the Dataset\nThe relevant data sets used in the analysis can be found here:\n\n2022 fare revenue\n2022 expenses\nridership\n\nThe first step in the analysis is to ingest the relevant data tables and prepare them for data analysis using R. The following code will clean and join the tables into relevant dataframes used in the analysis. The output will create the following dataframes named: FARES, EXPENSES, FINANCIALS, TRIPS, MILES, and USAGE.\n\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\n# Let's start with Fare Revenue\nlibrary(tidyverse)\nFARES &lt;- readxl::read_xlsx(\"2022_fare_revenue.xlsx\") |&gt;\n  select(\n    -`State/Parent NTD ID`,\n    -`Reporter Type`,\n    -`Reporting Module`,\n    -`TOS`,\n    -`Passenger Paid Fares`,\n    -`Organization Paid Fares`\n  ) |&gt;\n  filter(`Expense Type` == \"Funds Earned During Period\") |&gt;\n  select(-`Expense Type`) |&gt;\n  group_by(\n    `NTD ID`, # Sum over different `TOS` for the same `Mode`\n    `Agency Name`, # These are direct operated and sub-contracted\n    `Mode`\n  ) |&gt; # of the same transit modality\n  # Not a big effect in most munis (significant DO\n  # tends to get rid of sub-contractors), but we'll sum\n  # to unify different passenger experiences\n  summarize(`Total Fares` = sum(`Total Fares`)) |&gt;\n  ungroup()\n\n# Next, expenses\nEXPENSES &lt;- readr::read_csv(\"2022_expenses.csv\") |&gt;\n  select(\n    `NTD ID`,\n    `Agency`,\n    `Total`,\n    `Mode`\n  ) |&gt;\n  mutate(`NTD ID` = as.integer(`NTD ID`)) |&gt;\n  rename(Expenses = Total) |&gt;\n  group_by(`NTD ID`, `Mode`) |&gt;\n  summarize(Expenses = sum(Expenses)) |&gt;\n  ungroup()\n\nFINANCIALS &lt;- inner_join(FARES, EXPENSES, join_by(`NTD ID`, `Mode`))\n\n# Monthly Transit Numbers\nlibrary(tidyverse)\nTRIPS &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet = \"UPT\") |&gt;\n  filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n  select(\n    -`Legacy NTD ID`,\n    -`Reporter Type`,\n    -`Mode/Type of Service Status`,\n    -`UACE CD`,\n    -`TOS`\n  ) |&gt;\n  pivot_longer(-c(`NTD ID`:`3 Mode`),\n    names_to = \"month\",\n    values_to = \"UPT\"\n  ) |&gt;\n  drop_na() |&gt;\n  mutate(month = my(month)) # Parse _m_onth _y_ear date specs\nMILES &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet = \"VRM\") |&gt;\n  filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n  select(\n    -`Legacy NTD ID`,\n    -`Reporter Type`,\n    -`Mode/Type of Service Status`,\n    -`UACE CD`,\n    -`TOS`\n  ) |&gt;\n  pivot_longer(-c(`NTD ID`:`3 Mode`),\n    names_to = \"month\",\n    values_to = \"VRM\"\n  ) |&gt;\n  drop_na() |&gt;\n  group_by(\n    `NTD ID`, `Agency`, `UZA Name`,\n    `Mode`, `3 Mode`, month\n  ) |&gt;\n  summarize(VRM = sum(VRM)) |&gt;\n  ungroup() |&gt;\n  mutate(month = my(month)) # Parse _m_onth _y_ear date specs\n\nUSAGE &lt;- inner_join(TRIPS, MILES) |&gt;\n  mutate(`NTD ID` = as.integer(`NTD ID`))\n\n# End of data ingestion and setup\n\nHere, a summary table of USAGE is created to get an introductory visualization of the table that will be used for analysis.\n\nif (!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\n\n# Initialize a table to begin analysis\nsample_n(USAGE, 1000) |&gt;\n  mutate(month = as.character(month)) |&gt;\n  DT::datatable()\n\n\n\n\n\nTable 1: Quick Overview of USAGE\n\n\nTransforming Data Table\nThe analysis will be initially conducted using the dataframe USAGE. Some of the provided labels are cumbersome to work with in R. It is doable, but we can make our lives easier by renaming them. The first task at hand is to rename the column UZA Name to metro_area. The following code will show how that is done.\n\nUSAGE &lt;- USAGE |&gt;\n  rename(metro_area = `UZA Name`)\n\nEach transportation Mode is represented by a two letter code, for example HR = Heavy Rail. The two letter codes aren’t meaningful to us, as it’s impossible to guess what they are. The first thing I did to clean this portion up was to find all the codes used by the Federal Transit Administration. Running the following will give us the list of codes found in the data set.\n\nlibrary(dplyr)\nunique_modes &lt;- USAGE |&gt;\n  distinct(Mode)\nprint(unique_modes)\n\n# A tibble: 18 × 1\n   Mode \n   &lt;chr&gt;\n 1 DR   \n 2 FB   \n 3 MB   \n 4 SR   \n 5 TB   \n 6 VP   \n 7 CB   \n 8 RB   \n 9 LR   \n10 YR   \n11 MG   \n12 CR   \n13 AR   \n14 TR   \n15 HR   \n16 IP   \n17 PB   \n18 CC   \n\n\nOnce all the codes have been identified, the meanings can be found in the National Transit Database glossary. Using the mutate function, all the codes can be changed into meaningful definitions.\n\nUSAGE &lt;- USAGE |&gt;\n  mutate(Mode = case_when(\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferryboat\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"SR\" ~ \"Streetcar Rail\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"MG\" ~ \"Monorail and Automated Guideway modes\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramways\",\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"\n  ))\n\nNow that the data has been cleaned up to a new extent, let’s create a new summary table from USAGE. To make the outputted table as clean as possible, I’ve opted to get rid of irrelevant columns and change the acronyms to display meaningful words. UPT stands for unlinked passenger trips and VRM stands for vehicle revenue miles.\n\ndatatable(\n  sample_n(USAGE, 1000) |&gt;\n    mutate(month = as.character(month)) |&gt;\n    select(-`NTD ID`, -`3 Mode`) |&gt; # exclude ntd id and 3 mode in visual table\n    rename(\n      `Metro Area` = metro_area, # rename for table output to look cleaner\n      `Unlinked Passenger Trips` = UPT, # rename acronym in visual table\n      `Vehicle Revenue Miles` = VRM # rename acronym in visual table\n    )\n)\n\n\n\n\n\nTable 2: Cleaned Up Version of USAGE\n\n\n\n\n\n\nFor my own sanity, I checked if the table had any NA values I needed to consider.\n\n\n\n\nna_count &lt;- sum(is.na(USAGE))\nprint(na_count)\n\n[1] 0\n\n\nThis code returns 0, which lets me know there are no missing values in the data I’m working with. With this reassurance, I won’t be using the na.rm=TRUE statement in any of my code. However, if the opposite were true instead, I would make sure to use the above statement while utilizing aggregate functions.\n\n\n\n\nInitial Metrics of Interest\nNow that there is a clean table to work with, some questions of interest about the data can be explored. The following questions will explore the use of the following functions filter, group_by, summarize, and arrange.\n\n\n\n\n\n\nThe first set of metrics of interest are:\n\n\n\n\nWhat transit agency had the most total VRM in this sample?\nWhat transit mode had the most total VRM in this sample?\nHow many trips were taken on the NYC Subway (Heavy Rail) in May 2024?\nHow much did NYC subway ridership fall between April 2019 and April 2020?\n\n\n\n\nTo find the transit agency with the most total VRM from the sample, I need to group the data based on the Agency and its respective VRM total. It turns out the MTA New York City Transit has reign over total VRM among the agencies with 10,832,855,350 miles.\n\nmost_vrm_agency &lt;- USAGE |&gt;\n  group_by(Agency) |&gt;\n  summarize(total_vrm = sum(VRM)) |&gt;\n  arrange(desc(total_vrm))\nprint(most_vrm_agency)\n\n# A tibble: 677 × 2\n   Agency                                                              total_vrm\n   &lt;chr&gt;                                                                   &lt;dbl&gt;\n 1 MTA New York City Transit                                             1.08e10\n 2 New Jersey Transit Corporation                                        5.65e 9\n 3 Los Angeles County Metropolitan Transportation Authority              4.35e 9\n 4 Washington Metropolitan Area Transit Authority                        2.82e 9\n 5 Chicago Transit Authority                                             2.81e 9\n 6 Southeastern Pennsylvania Transportation Authority                    2.67e 9\n 7 Massachusetts Bay Transportation Authority                            2.38e 9\n 8 Pace, the Suburban Bus Division of the Regional Transportation Aut…   2.38e 9\n 9 Metropolitan Transit Authority of Harris County, Texas                2.27e 9\n10 Denver Regional Transportation District                               1.99e 9\n# ℹ 667 more rows\n\n\n\n\n\n\n\n\nAlternative code block regarding slice_head:\n\n\n\n\n\nI purposely chose not to include a slice_head function to get a comparative overview of the data. Here, the MTA had an overwhelming total over the other agencies, which was an interesting finding. I stuck with the same philosophy throughout most of this analysis since I was interested in comparing the sheer numbers as well, not just the specific metric I was inquiring about.\n\nmost_vrm_agency1 &lt;- USAGE |&gt;\n  group_by(Agency) |&gt;\n  summarize(total_vrm = sum(VRM)) |&gt;\n  arrange(desc(total_vrm)) |&gt;\n  slice_head(n = 1)\nprint(most_vrm_agency1)\n\n# A tibble: 1 × 2\n  Agency                      total_vrm\n  &lt;chr&gt;                           &lt;dbl&gt;\n1 MTA New York City Transit 10832855350\n\n\n\n\n\nTo find the transit mode with the most total VRM from the sample, I need to group the data based on the Mode and its respective VRM total. By a large margin of 49,444,494,088 miles, the bus(MB) Mode had the most total VRM from the sample.\n\nmost_vrm_mode &lt;- USAGE |&gt;\n  group_by(Mode) |&gt;\n  summarize(total_vrm = sum(VRM)) |&gt;\n  arrange(desc(total_vrm))\nprint(most_vrm_mode)\n\n# A tibble: 18 × 2\n   Mode                                    total_vrm\n   &lt;chr&gt;                                       &lt;dbl&gt;\n 1 Bus                                   49444494088\n 2 Demand Response                       17955073508\n 3 Heavy Rail                            14620362107\n 4 Commuter Rail                          6970644241\n 5 Vanpool                                3015783362\n 6 Light Rail                             2090094714\n 7 Commuter Bus                           1380948975\n 8 Publico                                1021270808\n 9 Trolleybus                              236840288\n10 Bus Rapid Transit                       118425283\n11 Ferryboat                                65589783\n12 Streetcar Rail                           63389725\n13 Monorail and Automated Guideway modes    37879729\n14 Hybrid Rail                              37787608\n15 Alaska Railroad                          13833261\n16 Cable Car                                 7386019\n17 Inclined Plane                             705904\n18 Aerial Tramways                            292860\n\n\nTo find how many trips were taken on the NYC Subway in May 2024, there were multiple criteria to consider here. A filter needs to be used in order to address the transit Mode, month, and Agency. In this case, I made the assumption that the NYC Subway is only operated by the MTA New York City Transit. In May 2024, there were a total of 180,000,000 (1.80e8) trips taken.\n\nnyc_subway_trips &lt;- USAGE |&gt;\n  filter(\n    Agency == \"MTA New York City Transit\",\n    Mode == \"Heavy Rail\",\n    month &gt;= as.Date(\"2024-05-01\") & month &lt;= as.Date(\"2024-05-31\")\n  )\nprint(nyc_subway_trips)\n\n# A tibble: 1 × 8\n  `NTD ID` Agency             metro_area Mode  `3 Mode` month         UPT    VRM\n     &lt;int&gt; &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;    &lt;date&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n1    20008 MTA New York City… New York-… Heav… Rail     2024-05-01 1.80e8 3.00e7\n\n\n\n\n\n\n\n\nInformation Regarding the month Column\n\n\n\nAfter going through this question at hand, I was able to identify that the month is stored as just the first of the month each year. I confirmed that there is only one entry per month for each respective agency, metro area, and mode. Moving forward with any month filters, I don’t have to worry about including the full month as it will always only be in the format of the first of the month in each year. This understanding will be seen in later examples.\n\n\nTo find the ridership difference between April 2019 and April 2020, I need to find the amount of trips taken in each month-year and subtract from one another. Again, I made the assumption that the NYC Subway is only operated by the MTA New York City Transit. In the following code, you can see the difference when filtering the month as referenced in the call-out above! The ridership fell by 211,969,660 trips between April 2019 and April 2020. I interpreted this metric as the difference of trips between each respective month-year, not the total difference in between.\n\nride_fall &lt;- USAGE |&gt;\n filter(Mode == \"Heavy Rail\") |&gt;\n filter(Agency == \"MTA New York City Transit\") |&gt; # this is the agency that runs nyc subway\n filter(month %in% c(as.Date(\"2019-04-01\"), as.Date(\"2020-04-01\"))) |&gt;\n group_by(month) |&gt;\n summarize(total_rides = sum(UPT)) |&gt;\n summarize(difference = total_rides[month == as.Date(\"2020-04-01\")] -\n   total_rides[month == as.Date(\"2019-04-01\")])\nprint(ride_fall)\n\n# A tibble: 1 × 1\n  difference\n       &lt;dbl&gt;\n1 -211969660\n\n\n\n\n\nAdditional Metrics of Interest\nAsides from the metrics explored above, there are a variety of other questions that can be asked from the data. In this section, I will explore three other areas of interest. The data is not limited to the following questions discussed, there are a multitude of statistics that can be uncovered. For the following questions I asked, I am trying to utilize as many R functions as possible.\n\n\n\n\n\n\nAdditional metrics of interest are:\n\n\n\n\nWhat UZA Name / metro_area had the most UPT in January 2022?\nWhat month and year had the most UPT through the bus (MB) in the entire sample?\nWhat is the average amount of trips taken in the New York–Jersey City–Newark, NY–NJ area based on the season from 2018 to 2022?\n\n\n\n\nThe areas of interest require filtering a date, grouping by a variable, and aggregating a variable. In January 2022, the New York City–Jersey City–Newark, NY–NJ area recorded the most trips with 173,719,501 trips. The second-ranked area had only 26,158,306 trips, accounting for just 15% of the total for New York–New Jersey.\n\nlibrary(dplyr)\npopular_area &lt;- USAGE |&gt;\n filter(month %in% c(as.Date(\"2022-01-01\"))) |&gt;\n group_by(metro_area) |&gt;\n summarize(total_trips = sum(UPT)) |&gt;\n arrange(desc(total_trips))\nprint(popular_area)\n\n# A tibble: 295 × 2\n   metro_area                            total_trips\n   &lt;chr&gt;                                       &lt;dbl&gt;\n 1 New York--Jersey City--Newark, NY--NJ   173719501\n 2 Los Angeles--Long Beach--Anaheim, CA     26158306\n 3 Chicago, IL--IN                          16569817\n 4 San Francisco--Oakland, CA               13571654\n 5 Boston, MA--NH                           13220711\n 6 Philadelphia, PA--NJ--DE--MD             12972351\n 7 Washington--Arlington, DC--VA--MD        11229936\n 8 Miami--Fort Lauderdale, FL                8318327\n 9 Seattle--Tacoma, WA                       8250602\n10 San Diego, CA                             4602265\n# ℹ 285 more rows\n\n\nThis metric references the second question from the previous question. Now we’re taking a look at history, seeing exactly when this mode peaked. The data tells us that this happened in October 2018, with 478,806,384 trips. Evidently, the lower ranked months had similar values too. This shows strong consistency for the bus transit mode across the US.\n\nbus_trips &lt;- USAGE |&gt;\n filter(Mode == \"Bus\") |&gt;\n group_by(month) |&gt;\n summarize(total_bus_trips = sum(UPT)) |&gt;\n arrange(desc(total_bus_trips))\nprint(bus_trips)\n\n# A tibble: 271 × 2\n   month      total_bus_trips\n   &lt;date&gt;               &lt;dbl&gt;\n 1 2008-10-01       478806384\n 2 2014-10-01       457089165\n 3 2013-10-01       456214396\n 4 2007-10-01       455193568\n 5 2008-09-01       454077576\n 6 2006-10-01       450496480\n 7 2006-03-01       450386143\n 8 2012-10-01       448572088\n 9 2008-05-01       442961523\n10 2009-10-01       441007281\n# ℹ 261 more rows\n\n\nThe code required for this was a challenge, but it utilized functions already explored earlier and putting them together intricately. The months were assigned to a season within a case_when function within a mutate function. Additionally a filter, group_by, summarize, arrange, and mean function were used as well. From 2018 to 2022, the average amount of trips taken in NY-NJ was:\n\n\n\nSeason\nAverage UPT\n\n\n\n\nFall\n4,960,514\n\n\nWinter\n4,772,907\n\n\nSummer\n4,609,142\n\n\nSpring\n4,508,331\n\n\n\n\nTable 3: Average UPT by Season, NY-NJ, 2018-2022\n\nseasonal_variation &lt;- USAGE |&gt;\n  filter(metro_area == \"New York--Jersey City--Newark, NY--NJ\") |&gt;\n  filter(month &gt;= as.Date(\"2018-01-01\") & month &lt;= as.Date(\"2022-12-01\")) |&gt;\n  mutate(\n    month_num = as.numeric(format(month, \"%m\")), # Extract the month as a number from the date column\n    season = case_when( # Use case_when to categorize into seasons\n      month_num %in% c(12, 1, 2) ~ \"Winter\",\n      month_num %in% c(3, 4, 5) ~ \"Spring\",\n      month_num %in% c(6, 7, 8) ~ \"Summer\",\n      month_num %in% c(9, 10, 11) ~ \"Fall\",\n      TRUE ~ \"Unknown\"\n    )\n  ) |&gt;\n  group_by(season) |&gt;\n  summarize(avg_trips = mean(UPT)) |&gt;\n  arrange(desc(avg_trips))\nprint(seasonal_variation)\n\n\nThis concludes the first half of the analysis. A variety of transit metric data was unearthed. A better understanding of the R functions were explored through data analysis. Now that preliminary data has been identified, we can move forward to the next half of the analysis. The fare data available to use is from 2022. In order to do a deeper analysis, the USAGE table will need to be converted to a 2022 version in order to join the fare data information together. Once we have a combined table, we can uncover what farebox recovery looked like in 2022.\n\n\nFarebox Recovery\nThe first task at hand is to extract only the 2022 information from USAGE. The parameters of interest kept are NTD ID, Agency, metro_area, Mode, UPT, VRM. Normally, filtering just the year and selecting the parameters would be straightforward. However, the UPT and VRM need to be aggregated for the new joined table. Additionally, the mutate function is used to convert NTD ID to a double type in order to match the same type as the NT ID in the FINANCIALS table we will be joining to later. The new table is called USAGE_2022_ANNUAL. For the farebox recovery analysis, the sample will focus solely on major transit systems, which is defined as those with 400,000 UPT per annum. For this definition of major transit systems, the total UPT per Agency was considered (the mode is not taken into consideration, just the Agency as a whole itself).\n\n# Calculate UPT per agency per year to only consider agencies with UPT of 400,000 or more per year\nagencies_400k_upt &lt;- USAGE |&gt;\n  mutate(Year = year(month)) |&gt; # Extract year from month\n  group_by(`NTD ID`, Agency, Year) |&gt; # Group by agency and year\n  summarize(Total_UPT = sum(UPT, na.rm = TRUE), .groups = 'drop') |&gt; # Summarize total UPT per Agency, aggregating the UPT across all modes\n  filter(Total_UPT &gt;= 400000) |&gt; # Keep agencies with total UPT &gt;= 400,000 annum\n  ungroup() |&gt; # Ungroup to prepare for next operation\n  distinct(Agency) # Get distinct agencies\n\n# Filter the 2022 data for only those agencies\nUSAGE_2022_ANNUAL &lt;- USAGE |&gt;\n  filter(year(month) == 2022) |&gt; # Only data from 2022\n  filter(Agency %in% agencies_400k_upt$Agency) |&gt; # Filter agencies that meet avg UPT condition\n  group_by(`NTD ID`, Agency, metro_area, Mode) |&gt; # Group by relevant columns\n  summarize(\n    UPT = sum(UPT), # Sum UPT for 2022\n    VRM = sum(VRM) # Sum VRM for 2022\n  ) |&gt;\n  ungroup() |&gt;\n  mutate(`NTD ID` = as.double(`NTD ID`)) # Convert NTD ID to double for joining\n\nprint(USAGE_2022_ANNUAL) # Output the filtered table\n\n# A tibble: 1,023 × 6\n   `NTD ID` Agency                                metro_area Mode     UPT    VRM\n      &lt;dbl&gt; &lt;chr&gt;                                 &lt;chr&gt;      &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1        1 King County                           Seattle--… Bus   5.40e7 6.16e7\n 2        1 King County                           Seattle--… Dema… 6.63e5 1.29e7\n 3        1 King County                           Seattle--… Ferr… 4.00e5 5.12e4\n 4        1 King County                           Seattle--… Stre… 1.12e6 1.80e5\n 5        1 King County                           Seattle--… Trol… 9.58e6 2.64e6\n 6        1 King County                           Seattle--… Vanp… 7.03e5 4.41e6\n 7        2 Spokane Transit Authority             Spokane, … Bus   6.60e6 6.49e6\n 8        2 Spokane Transit Authority             Spokane, … Dema… 3.10e5 4.04e6\n 9        2 Spokane Transit Authority             Spokane, … Vanp… 9.06e4 9.06e5\n10        3 Pierce County Transportation Benefit… Seattle--… Bus   4.95e6 4.23e6\n# ℹ 1,013 more rows\n\n\n\n\n\n\n\n\nAdditional transformation is required:\n\n\n\nBefore we can join USAGE_2022_ANNUAL onto FINANCIALS, we need to revisit the mode conversion we did earlier in the data cleaning. The FINANCIALS table follows the same format with the mode being a code as seen earlier.\n\nFINANCIALS &lt;- FINANCIALS |&gt;\n  mutate(Mode = case_when(\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferryboat\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"SR\" ~ \"Streetcar Rail\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"MG\" ~ \"Monorail and Automated Guideway modes\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramways\",\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"\n  ))\n\n\n\nFinally, we can join the USAGE_2022_ANNUAL and FINANCIALS tables together. USAGE_AND_FINANCIALS will be used to conduct the final analysis on farebox recovery in 2022. An innjer_join is used since some values were dropped when filtering out for only major transit systems. In order to join the data properly, an inner_join matches the values that are only present in the USAGE_2022_ANNUAL table.\n\nUSAGE_AND_FINANCIALS &lt;- inner_join(\n  USAGE_2022_ANNUAL,\n  FINANCIALS,\n  join_by(`NTD ID`, Mode)\n  )\n\ndatatable( \n  sample_n(USAGE_AND_FINANCIALS, 1000) |&gt;\n    select(-`Agency Name`) |&gt; # exclude extra agency name column from financials table\n    rename(\n      `Metro Area` = metro_area, # rename for table output to look cleaner\n      `Unlinked Passenger Trips` = UPT, # rename acronym in visual table\n      `Vehicle Revenue Miles` = VRM # rename acronym in visual table\n          )\n)\n\n\n\n\n\nTable 4: Visual of USAGE_AND_FINANCIALS\n\n\n\n\n\n\nFarebox recovery metrics to be analyzed:\n\n\n\n\nWhich transit system (agency and mode) had the most UPT in 2022?\nWhich transit system (agency and mode) had the highest farebox recovery, defined as the highest ratio of Total Fares to Expenses?\nWhich transit system (agency and mode) has the lowest expenses per UPT?\nWhich transit system (agency and mode) has the highest total fares per UPT?\nWhich transit system (agency and mode) has the lowest expenses per VRM?\nWhich transit system (agency and mode) has the highest total fares per VRM?\n\n\n\n\nIn 2022, the MTA New York City Transit had the most UPT via the heavy rail (subway). There was a total of 1,793,073,801 trips taken. This result is not suprising given the sheer population size of the NYC tri-state area, as well as the vast amount of public transportation accessibility throughout the city.\n\nmost_UPT_2022 &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(most_UPT = UPT, .groups = 'drop') |&gt;\n  arrange(desc(most_UPT)) \nprint(most_UPT_2022)\n\n# A tibble: 1,016 × 3\n   Agency                                                   Mode        most_UPT\n   &lt;chr&gt;                                                    &lt;chr&gt;          &lt;dbl&gt;\n 1 MTA New York City Transit                                Heavy Rail    1.79e9\n 2 MTA New York City Transit                                Bus           4.59e8\n 3 Los Angeles County Metropolitan Transportation Authority Bus           1.94e8\n 4 Chicago Transit Authority                                Bus           1.40e8\n 5 New Jersey Transit Corporation                           Bus           1.13e8\n 6 Chicago Transit Authority                                Heavy Rail    1.04e8\n 7 MTA Bus Company                                          Bus           1.00e8\n 8 Washington Metropolitan Area Transit Authority           Heavy Rail    9.84e7\n 9 Southeastern Pennsylvania Transportation Authority       Bus           9.66e7\n10 Washington Metropolitan Area Transit Authority           Bus           8.99e7\n# ℹ 1,006 more rows\n\n\nIn 2022, the County of Miami-Dade via vanpool had the highest farebox recovery with a ratio of 1.67. I found this result intriguing as vanpool would not have been my first assumption for this metric. Living in Brooklyn, the concept of vanpooling is unfamiliar to me. However, this did give me insight to how the rest of the country can greatly differ depending on the area of interest.\n\nhighest_farebox &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(total_fares = `Total Fares`, total_expenses = Expenses, .groups = 'drop') |&gt;\n  mutate(recovery = total_fares / total_expenses) |&gt;\n  arrange(desc(recovery))\nprint(highest_farebox)\n\n# A tibble: 1,016 × 5\n   Agency                              Mode  total_fares total_expenses recovery\n   &lt;chr&gt;                               &lt;chr&gt;       &lt;dbl&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n 1 County of Miami-Dade                Vanp…     1987879        1191874     1.67\n 2 Yuma County Intergovernmental Publ… Vanp…      411216         279585     1.47\n 3 Port Imperial Ferry Corporation     Ferr…    33443241       23417248     1.43\n 4 Hyannis Harbor Tours, Inc.          Ferr…    25972659       18383764     1.41\n 5 Trans-Bridge Lines, Inc.            Comm…    11325199        8495611     1.33\n 6 Chattanooga Area Regional Transpor… Incl…     3005198        2290714     1.31\n 7 Municipality of Anchorage           Vanp…     1400709        1105911     1.27\n 8 Regional Transportation Commission… Vanp…     3561776        2876745     1.24\n 9 Fort Worth Transportation Authority Vanp…     1410877        1141477     1.24\n10 Hampton Jitney, Inc.                Comm…    21539188       17957368     1.20\n# ℹ 1,006 more rows\n\n\nIn 2022, North Carolina State University via bus had the lowest expenses per trip with a ratio of 1.18. More insight into the expenses of this transportation mode could provide better context to the performance. A university funded transportation system could potentially have lower operation costs compared to a metropolitan transit system due to less logistical hurdles to overcome.\n\nlow_expense_UPT &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(total_expenses = Expenses, total_UPT = UPT, .groups = 'drop') |&gt;\n  mutate(lowestUPT = total_expenses/total_UPT) |&gt;\n  arrange(lowestUPT)\nprint(low_expense_UPT)\n\n# A tibble: 1,016 × 5\n   Agency                               Mode  total_expenses total_UPT lowestUPT\n   &lt;chr&gt;                                &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 North Carolina State University      Bus          2727412   2313091      1.18\n 2 Anaheim Transportation Network       Bus          9751600   7635011      1.28\n 3 Valley Metro Rail, Inc.              Stre…         542700    364150      1.49\n 4 University of Iowa                   Bus          3751241   2437750      1.54\n 5 Chatham Area Transit Authority       Ferr…         935249    582988      1.60\n 6 Texas State University               Bus          4825081   2348943      2.05\n 7 South Florida Regional Transportati… Bus           731643    322155      2.27\n 8 University of Georgia                Bus          6267845   2714941      2.31\n 9 Hillsborough Area Regional Transit … Stre…        2780595   1137177      2.45\n10 University of Michigan Parking and … Bus         11990864   4754836      2.52\n# ℹ 1,006 more rows\n\n\nIn 2022, the Altoona Metro Transit via demand rail had the highest total fares per trip with a ratio of 660. Note that the ratio is absurdly large, there are indications as to why this number is so high. The output shows that there were only a total of 26 unlinked passenger trips for this mode of transportation. The lack of total trips is something to consider when answering this question. I would postulate setting a minimum number of unlinked passenger trips when asking this question if I wanted to explore the data more in depth.\n\nhigh_fare_UPT &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(total_fares = `Total Fares`, total_UPT = UPT, .groups = 'drop') |&gt;\n  mutate(fare_UPT_ratio = total_fares/total_UPT) |&gt;\n  arrange(desc(fare_UPT_ratio))\nprint(high_fare_UPT)\n\n# A tibble: 1,016 × 5\n   Agency                             Mode  total_fares total_UPT fare_UPT_ratio\n   &lt;chr&gt;                              &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;\n 1 Altoona Metro Transit              Dema…       17163        26          660. \n 2 Central Pennsylvania Transportati… Dema…    14084946    280455           50.2\n 3 Hampton Jitney, Inc.               Comm…    21539188    521577           41.3\n 4 County of Placer                   Comm…       40847      1054           38.8\n 5 Lane Transit District              Dema…    10724805    314974           34.0\n 6 Pennsylvania Department of Transp… Comm…    14580664    452034           32.3\n 7 Hyannis Harbor Tours, Inc.         Ferr…    25972659    878728           29.6\n 8 Trans-Bridge Lines, Inc.           Comm…    11325199    403646           28.1\n 9 SeaStreak, LLC                     Ferr…    16584600    750392           22.1\n10 Cambria County Transit Authority   Dema…      520554     25831           20.2\n# ℹ 1,006 more rows\n\n\nIn 2022, the VIA Metropolitan Transit via vanpool had the lowest expenses per vehicle revenue mile with a ratio of 0.37. Once again, the transportation mode of vanpool has been a key finding in the metrics observed. Somethings that could be further explored are the characteristics of the areas vanpools are popular in.\n\nlow_expense_VRM &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(total_expenses = Expenses, total_VRM = VRM, .groups = 'drop') |&gt;\n  mutate(lowestVRM = total_expenses/total_VRM) |&gt;\n  arrange(lowestVRM)\nprint(low_expense_VRM)\n\n# A tibble: 1,016 × 5\n   Agency                               Mode  total_expenses total_VRM lowestVRM\n   &lt;chr&gt;                                &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 VIA Metropolitan Transit             Vanp…        1298365   3505579     0.370\n 2 County of Miami-Dade                 Vanp…        1191874   3091052     0.386\n 3 County of Volusia                    Vanp…          87487    222484     0.393\n 4 Corpus Christi Regional Transportat… Vanp…         433951   1006399     0.431\n 5 Metropolitan Transportation Commiss… Vanp…        5491767  12341055     0.445\n 6 Central Midlands Regional Transport… Vanp…         195326    438557     0.445\n 7 Fort Worth Transportation Authority  Vanp…        1141477   2372285     0.481\n 8 San Joaquin Council                  Vanp…        4629125   9297516     0.498\n 9 Salem Area Mass Transit District     Vanp…         238952    468018     0.511\n10 San Diego Association of Governments Vanp…        5264624   9740828     0.540\n# ℹ 1,006 more rows\n\n\nIn 2022, the Chicago Water Taxi (Wendella) via Ferry had the highest total fares per vehicle revenue mile with a ratio of 237. Like the fourth farebox recovery metric, this data point also seems skewed because of the lack of vehicle revenue miles involved. I would suggest setting a minimum amount of vehicle revenue miles as well to gauge a better understanding of which transit system boasts the highest total fare per vehicle revenue mile.\n\nhigh_fare_VRM &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency, Mode) |&gt;\n  summarize(total_fares = `Total Fares`, total_VRM = VRM, .groups = 'drop') |&gt;\n  mutate(fare_VRM_ratio = total_fares/total_VRM) |&gt;\n  arrange(desc(fare_VRM_ratio))\nprint(high_fare_VRM)\n\n# A tibble: 1,016 × 5\n   Agency                             Mode  total_fares total_VRM fare_VRM_ratio\n   &lt;chr&gt;                              &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;\n 1 Chicago Water Taxi (Wendella)      Ferr…      142473       600          237. \n 2 Altoona Metro Transit              Dema…       17163        75          229. \n 3 Jacksonville Transportation Autho… Ferr…     1432549      9084          158. \n 4 Chattanooga Area Regional Transpo… Incl…     3005198     20128          149. \n 5 Hyannis Harbor Tours, Inc.         Ferr…    25972659    188694          138. \n 6 SeaStreak, LLC                     Ferr…    16584600    143935          115. \n 7 Cape May Lewes Ferry               Ferr…     6663334     71640           93.0\n 8 Woods Hole, Martha's Vineyard and… Ferr…    33424462    364574           91.7\n 9 Washington State Ferries           Ferr…    57644277    738094           78.1\n10 County of Pierce                   Ferr…     2979914     44548           66.9\n# ℹ 1,006 more rows"
  },
  {
    "objectID": "mp01.html#conclusion",
    "href": "mp01.html#conclusion",
    "title": "Mini-Project #01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Conclusion",
    "text": "Conclusion\nThis analysis was ultimately inspired by how farebox recovery rates vary by transit system across the nation. The definition of efficiency I would like to use is the transit system with the highest farebox recovery rate. From this sample, the County of Miami-Dade vanpool transit system is the most efficient. The fares made on the transit system trips exceed the operation cost per trip. Not only does the return cover the operation costs, it also exceeds it slightly. If profit is not the goal for the transit system, the surplus revenue can be reinvested into infrastructure to ensure smooth operations. Continuously enhancing the rider experience can help sustain the system over the long term, with the aim of increasing ride share participation over time.\nThere are other data points that can be analyzed and incorporated into this analysis for further exploration. Some areas of interest I would explore are comparing trips taken to the total population the area serves. This can give a better idea about what percentage of the population is utilizing the public transportation system. Trends about public transportation usage based on the population available can highlight how popular public transit is depending on an area. Another area of interest I would like to explore is the carbon emission reduction provided by transit systems. A transit system’s financial stability could be easily offset by environmental impact depending on the mode of transportation. There are multitudes of other data points that can be extrapolated to explore how efficient a transit system is within the scope of defining what efficiency is."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome To My STA9750 Portfolio",
    "section": "",
    "text": "Mini Project #01: Fiscal Characteristics of Major US Public Transit Systems\nMini Project #02: The Business of Show Business\nMini Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?\nMini Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Welcome To My STA9750 Portfolio",
    "section": "About Me",
    "text": "About Me\n\nChris Liu\nI am currently attending Baruch College in the Zicklin School of Business with an anticipated graduation of May 2026. I am pursuing a MS in Statistics with a focus in data science. I hope to pursue a career within statistical machine learning after graduation. If you’re interested in getting connected, here is my LinkedIn profile.\nI graduated with a B.S. in Mechanical Engineering from Boston University in 2021. Some of my previous work done in my undergraduate courses can be found in my mechanical engineering project portfolio."
  },
  {
    "objectID": "index.html#portfolio-projects",
    "href": "index.html#portfolio-projects",
    "title": "Welcome To My STA9750 Portfolio",
    "section": "",
    "text": "Mini Project #01: Fiscal Characteristics of Major US Public Transit Systems\nMini Project #02: The Business of Show Business\nMini Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?\nMini Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans"
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Mini-Project #02: The Business of Show Business",
    "section": "",
    "text": "Hollywood executives are tasked with coming up with new movie ideas that will convince an audience to see their movie. Traditionally, executives secure life rights to produce biopics, obtain licensing agreements to adapt existing forms of media onto the big screen, and work with owners of promising intellectual property that can be adapted. However, Hollywood movies are struggling to recapture the box office successes as seen in the past. For one reason or another, movie goers have not been flocking to theaters to see the next big film. The goal of this project is to develop data-driven insights for new movie ideas.\nIn order to derive insights, we will be diving into data from the Internet Movie Database (IMDb). The dataset used in this project analysis comes from the IMDb non-commercial release data tables. We will explore key characteristics of successful movies in history, identify successful actors and filmmakers and examine downfalls in Hollywood history."
  },
  {
    "objectID": "mp02.html#introduction",
    "href": "mp02.html#introduction",
    "title": "Mini-Project #02: The Business of Show Business",
    "section": "",
    "text": "Hollywood executives are tasked with coming up with new movie ideas that will convince an audience to see their movie. Traditionally, executives secure life rights to produce biopics, obtain licensing agreements to adapt existing forms of media onto the big screen, and work with owners of promising intellectual property that can be adapted. However, Hollywood movies are struggling to recapture the box office successes as seen in the past. For one reason or another, movie goers have not been flocking to theaters to see the next big film. The goal of this project is to develop data-driven insights for new movie ideas.\nIn order to derive insights, we will be diving into data from the Internet Movie Database (IMDb). The dataset used in this project analysis comes from the IMDb non-commercial release data tables. We will explore key characteristics of successful movies in history, identify successful actors and filmmakers and examine downfalls in Hollywood history."
  },
  {
    "objectID": "mp02.html#analysis",
    "href": "mp02.html#analysis",
    "title": "Mini-Project #02: The Business of Show Business",
    "section": "Analysis",
    "text": "Analysis\n\nPreparing, Cleaning & Loading the Dataset\nThe following packages will be used for this analysis: dplyr, tidyr, stringr, DT, ggplot2 and plotly. If these packages have not been installed in the system, they can be with the following code:\nif (!require(\"dplyr\")) install.packages(\"dplyr\")\nif (!require(\"tidyr\")) install.packages(\"tidyr\")\nif (!require(\"stringr\")) install.packages(\"stringr\")\nif (!require(\"DT\")) install.packages(\"DT\")\nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\nif (!require(\"gganimate\")) install.packages(\"gganimate\")\nif (!require(\"gifski\")) install.packages(\"gifski\")\nif (!require(\"plotly\")) install.packages(\"plotly\")\nif (!require(\"treemap\")) install.packages(\"treemap\")\nThe dataset used in the analysis contains large files. It will take some time for the data to be downloaded and extracted. The following code will download the files and create six relevant dataframes that will be referenced throughout the analysis.\n\n\nCode\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(DT)\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(gifski)\nlibrary(plotly)\nlibrary(treemap)\n\nget_imdb_file &lt;- function(fname) {\n  BASE_URL &lt;- \"https://datasets.imdbws.com/\"\n  fname_ext &lt;- paste0(fname, \".tsv.gz\")\n  if (!file.exists(fname_ext)) {\n    FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n    download.file(FILE_URL,\n      destfile = fname_ext\n    )\n  }\n  as.data.frame(readr::read_tsv(fname_ext, lazy = FALSE))\n}\n\n\nSort data into tables:\n\n\nCode\nNAME_BASICS      &lt;- get_imdb_file(\"name.basics\")\n\n\n\n\nCode\nTITLE_BASICS     &lt;- get_imdb_file(\"title.basics\")\n\n\n\n\nCode\nTITLE_EPISODES   &lt;- get_imdb_file(\"title.episode\")\n\n\n\n\nCode\nTITLE_RATINGS    &lt;- get_imdb_file(\"title.ratings\")\n\n\n\n\nCode\nTITLE_CREW       &lt;- get_imdb_file(\"title.crew\")\n\n\n\n\nCode\nTITLE_PRINCIPALS &lt;- get_imdb_file(\"title.principals\")\n\n\n\nData Sub-Sampling\nNote that the data we have extracted so far is large enough that we want to further downsize so we have a dataset we can analyze smoothly moving forward. The first thing we will do is modify the NAME_BASICS table to focus on people with at least two “known for” credits. The following code will help downsize the data present in NAME_BASICS.\n\n\nCode\nNAME_BASICS &lt;- NAME_BASICS |&gt; \n    filter(str_count(knownForTitles, \",\") &gt; 1)\n\n\nIMDb includes data for all sorts of media from major studios to independent producers. This includes obscure media that won’t be relevant to us for our analysis. The following code will visualize the distribution of ratings among the titles found on IMDb.\n\n\nCode\nTITLE_RATINGS |&gt;\n  ggplot(aes(x = numVotes)) +\n  geom_histogram(bins = 30) +\n  xlab(\"Number of IMDB Ratings\") +\n  ylab(\"Number of Titles\") +\n  ggtitle(\"Majority of IMDB Titles Have Less than 100 Ratings\") +\n  theme_bw() +\n  scale_x_log10(label = scales::comma) +\n  scale_y_continuous(label = scales::comma)\n\n\n\n\n\n\n\n\n\nThe majority of the titles found in IMDb have less than 100 ratings. We will go ahead and drop any title with less than 100 ratings so our computers are able to run the analysis fluidly. We can apply this drop with the following code:\n\n\nCode\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n    filter(numVotes &gt;= 100)\n\n\nNow that we have downsized the amount of titles we will be analyzing, we can apply the same filtering to our other tables. This can be done by joining the TITLE_RATINGS table with the other TITLE_ tables with a semi_join.\n\n\nCode\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n  semi_join(\n    TITLE_RATINGS,\n    join_by(tconst == tconst)\n  )\n\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n  semi_join(\n    TITLE_RATINGS,\n    join_by(tconst == tconst)\n  )\n\nTITLE_EPISODES_1 &lt;- TITLE_EPISODES |&gt;\n  semi_join(\n    TITLE_RATINGS,\n    join_by(tconst == tconst)\n  )\nTITLE_EPISODES_2 &lt;- TITLE_EPISODES |&gt;\n  semi_join(\n    TITLE_RATINGS,\n    join_by(parentTconst == tconst)\n  )\n\nTITLE_EPISODES &lt;- bind_rows(\n  TITLE_EPISODES_1,\n  TITLE_EPISODES_2\n) |&gt;\n  distinct()\n\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt;\n  semi_join(TITLE_RATINGS, join_by(tconst == tconst))\n\n\nrm(TITLE_EPISODES_1)\nrm(TITLE_EPISODES_2)\n\n\nThe dataset is now more manageable to work with. We can now begin the analysis portion of this project.\n\n\n\nExploring The Data\n\nTidying Data Types\nUsing the glimpse function, we can examine each table to see the type/mode for each column. At first glance, the majority of the columns appear to be a character (string) vector. Some of these columns should be numerical values instead, but due to missing values R converted the missing numerical data into characters instead. We will need to fix this issue by using the mutate function along with as.numeric() and as.logical() on the desired columns. The following code will update the desired columns to the correct format.\n\n\nCode\n# Clean the NAMES_BASICS table, replace missing string values to numeric NA values\nNAME_BASICS &lt;- NAME_BASICS |&gt;\n  mutate(\n    birthYear = as.numeric(birthYear),\n    deathYear = as.numeric(deathYear)\n  )\n\n# TITLE_BASICS has 4 column types to correct, isAdult, startYear, endYear and runtimeMinutes\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n  mutate(\n    isAdult = as.logical(isAdult),\n    startYear = as.numeric(startYear),\n    endYear = as.numeric(endYear),\n    runtimeMinutes = as.numeric(runtimeMinutes)\n  )\n\n# TITLE_EPISODES has 2 column types to correct, seasonNumber and episodeNumber\nTITLE_EPISODES &lt;- TITLE_EPISODES |&gt;\n  mutate(\n    seasonNumber = as.numeric(seasonNumber),\n    episodeNumber = as.numeric(episodeNumber)\n  )\n\n# TITLE_RATINGS has no columns to correct\n\n# TITLE_CREW has the correct column types but I want to convert the \\\\N values to NA instead\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n  mutate(\n    directors = na_if(directors, \"\\\\N\"),\n    writers = na_if(writers, \"\\\\N\")\n  )\n\n# TITLE_PRINCIPALS has the correct column types but I want to convert the \\\\N values to NA instead\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt;\n  mutate(\n    job = na_if(job, \"\\\\N\"),\n    characters = na_if(characters, \"\\\\N\")\n  )\n\n\n\n\n\n\n\n\nCaution\n\n\n\n\n\nThere are a few columns that contain multiple pieces of data in one cell. For example in the NAME_BASICS table, the primaryProfession and knownForTitles columns combine multiple values into one cell. We can use the separate_longer_delim function to break these values into multiple rows. To keep the analysis simple, we will use this function later when answering specific questions instead of breaking up the data beforehand.\n\n\n\n\n\n\nUncovering Insights From the IMDb Data\nThe first step in the analysis is to get a better understanding of the data we are working with. A series of questions are provided to get a grasp of what information we are able to derive from the provided data. To make things easier, creating a schema map of the tables helps us understand the relationships between each table. We will need to use multiple tables to derive insights to our questions.\n\n\n\nSchema of Tables Used\n\n\n\n\n\n\n\n\nThe first set of metrics of interest are:\n\n\n\n\nHow many movies are in our data set? How many TV series? How many TV episodes?\nWho is the oldest living person in our data set?\nThere is one TV Episode in this data set with a perfect 10/10 rating and 200,000 IMDb ratings. What is it? What series does it belong to?\nWhat four projects is the actor Mark Hamill most known for?\nWhat TV series, with more than 12 episodes, has the highest average rating?\nThe TV series Happy Days (1974-1984) gives us the common idiom “jump the shark”. The phrase comes from a controversial fifth season episode (aired in 1977) in which a lead character literally jumped over a shark on water skis. Idiomatically, it is used to refer to the moment when a once-great show becomes ridiculous and rapidly looses quality.\n\nIs it true that episodes from later seasons of Happy Days have lower average ratings than the early seasons?\n\n\n\nThe first thing I want to do is get a breakdown of data in titleType to understand how the media is categorized by IMDb. The simplest way to do so is by running the following code:\n\n\nunique(TITLE_BASICS$titleType)\n\n [1] \"short\"        \"movie\"        \"tvSeries\"     \"tvShort\"      \"tvMovie\"     \n [6] \"tvEpisode\"    \"tvMiniSeries\" \"video\"        \"tvSpecial\"    \"videoGame\"   \n\n\nThe output lets us know exactly what I want to filter for to answer this question. Now that I know what parameters to filter on, the following code will count the amount of media that falls into each category.\n\n\nCode\ncount_types &lt;- TITLE_BASICS |&gt;\n  filter(titleType %in% c(\"movie\", \"tvSeries\", \"tvEpisode\")) |&gt; # for this question I will only use movie, tvSeries, and tvEpisode\n  group_by(titleType) |&gt;\n  summarise(count = n())\n\ndatatable(setNames(count_types, c(\"Type\", \"Total\")),\n  options = list(pageLength = 10, autoWidth = TRUE),\n  caption = \"Table 1: Number of movies, TV series, and TV episodes\"\n)\n\n\n\n\n\n\n\n\nThere are some things to consider when working with the provided data. In the NAME_BASICS table we are provided the columns birthYear and deathYear. The issue at hand to consider is that the value of NA in the deathYear column can either indicate the person is either still alive or the record is incomplete. To tidy this up, as of October 2024 the oldest person alive in the world was born in 1908. I will use this as a benchmark for the birth year when filtering for this person. Another thing to keep in mind is that the data does not provide a month or date, so we cannot distinguish who is older for people that share the same birth year. The following is a list of the 10 oldest living people in the data, however we cannot distinguish any further from the available data.\n\n\n\nCode\noldest_living_person &lt;- NAME_BASICS |&gt;\n  filter(\n    is.na(deathYear), # NA in deathYear indicates person is living\n    birthYear &gt;= 1908\n  ) |&gt; # data is incomplete from deathYear, only considering people born after 1908 as there are incomplete entries (the oldest verified living person in the world as of Oct 2024 was born in 1908)\n  arrange(birthYear) |&gt; # order from oldest to youngest\n  slice_head(n = 10) |&gt;\n  select(primaryName)\n# note that only the birth year is available, no months or dates so this may not be as accurate as I'd like\n\ndatatable(setNames(oldest_living_person, c(\"Name\")),\n  options = list(pageLength = 10, autoWidth = TRUE),\n  caption = \"Table 2: Oldest Living Person in IMDb\"\n)\n\n\n\n\n\n\n\n\nIMDb has their rating system set up so that both a TV episode and TV series has its own separate ratings. For this metric, we want to focus strictly on the TV episode that has a perfect 10/10 rating with 200,000+ ratings. The first step is to identify that TV episode first. Once that has been identified, we can join that specific TV episode with the TITLE_EPISODES and TITLE_BASICS tables to get the series name. We find that the answer is none other than Breaking Bad - Ozymandias.\n\n\n\nCode\n# Find the episode with a perfect rating and over 200,000 ratings\nperfect_episode &lt;- TITLE_BASICS |&gt;\n  left_join(\n    TITLE_RATINGS,\n    join_by(tconst == tconst)\n  ) |&gt;\n  filter(\n    titleType == \"tvEpisode\",\n    averageRating == 10,\n    numVotes &gt;= 200000\n  )\n\n# Join back to TITLE_EPISODES and TITLE_BASICS to get the series name\nperfect_episode_series &lt;- perfect_episode |&gt;\n  left_join(\n    TITLE_EPISODES,\n    join_by(tconst == tconst)\n  ) |&gt;\n  left_join(\n    TITLE_BASICS,\n    join_by(parentTconst == tconst)\n  ) |&gt;\n  rename(\n    episode_title = primaryTitle.x, # rename column to episode title\n    series_title = primaryTitle.y\n  ) |&gt; # rename column to series title\n  select(\n    series_title,\n    episode_title\n  )\nprint(perfect_episode_series)\n\n\n  series_title episode_title\n1 Breaking Bad    Ozymandias\n\n\n\n\nWithin the NAME_BASICS table, each person has an associated list of titles they are known for found in the column knownTitles. This is a case where we want to use the separate_longer_delim function. We can get the titles easily by filtering specifically for Mark Hamill, but the results are stored as the identifier tconst. The last thing we need to do is join our results to the TITLE_BASICS table to get the actual name of the media Mark is known for.\n\n\n\nCode\n# Find the four projects Mark Hamill is known for first\nmark_hamill &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Mark Hamill\") |&gt;\n  separate_longer_delim(knownForTitles, \",\") |&gt;\n  select(knownForTitles)\n# this gives us the IDs, so we need to make further joins to get the names of the projects\n\n# Join results to TITLE_BASICS to get the name of the projects\nmark_hamill_projects &lt;- mark_hamill |&gt;\n  left_join(\n    TITLE_BASICS,\n    join_by(knownForTitles == tconst)\n  ) |&gt;\n  select(primaryTitle)\n\ndatatable(setNames(mark_hamill_projects, c(\"Project Title\")),\n  options = list(pageLength = 10, autoWidth = TRUE),\n  caption = \"Table 3: The 4 Projects Mark Hamill Is Most Known For\"\n)\n\n\n\n\n\n\n\n\nThere are two trains of thoughts I have when approaching this question. A TV series and TV episode both have their own ratings. The first approach is to strictly focus on the average ratings on the TV series itself. The second approach is to find the average ratings among the episodes within a TV series. I will demonstrate how these two distinct approaches produce different answers. Before doing either analysis, I want to create a table that only includes series that have more than 12 episodes.\n\n\n\nCode\ntv_series_12 &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"tvSeries\") |&gt; # Only want TV series\n  left_join(\n    TITLE_EPISODES,\n    join_by(tconst == parentTconst)\n  ) |&gt; # Join with TITLE_EPISODES to count number of episodes\n  group_by(tconst, primaryTitle) |&gt; # Group by series to count episodes\n  summarise(total_episodes = n()) |&gt; # Count total number of episodes\n  filter(total_episodes &gt; 12)\n\n\nNow that we have a list of TV series with more than 12 episodes, we can continue our analysis. The first analysis will look at the TV series ratings itself:\n\n\nCode\nhighest_rated_tv_series &lt;- tv_series_12 |&gt;\n  left_join(\n    TITLE_RATINGS,\n    join_by(tconst == tconst)\n  ) |&gt; # Join with TITLE_RATINGS to get average ratings\n  ungroup() |&gt;\n  arrange(desc(averageRating)) |&gt;\n  select(tconst, primaryTitle, averageRating) |&gt;\n  slice_head(n = 10)\n\ndatatable(setNames(highest_rated_tv_series, c(\"ID\", \"Title\", \"Average Rating\")),\n  options = list(pageLength = 10, autoWidth = TRUE),\n  caption = \"Table 4: Highest Rated TV Series (Series Ratings)\"\n)\n\n\n\n\n\n\nThe second approach is averaging the episode ratings for the series:\n\n\nCode\n# find the series and average rating by aggregating the episode ratings\nseries_ratings_byEpisode &lt;- TITLE_EPISODES |&gt;\n  semi_join(\n    tv_series_12,\n    join_by(parentTconst == tconst)\n  ) |&gt;\n  left_join(\n    TITLE_RATINGS,\n    join_by(tconst == tconst)\n  ) |&gt;\n  group_by(parentTconst) |&gt;\n  summarise(avg_ratings = mean(averageRating)) |&gt;\n  arrange(desc(avg_ratings))\n# join onto the TITLE_BASICS table to get the name of the series\nseries_ratings_byEpisode &lt;- series_ratings_byEpisode |&gt;\n  left_join(\n    TITLE_BASICS,\n    join_by(parentTconst == tconst)\n  ) |&gt;\n  ungroup() |&gt;\n  select(parentTconst, primaryTitle, avg_ratings) |&gt;\n  slice_head(n = 10)\n\ndatatable(setNames(series_ratings_byEpisode, c(\"ID\", \"Title\", \"Average Rating\")),\n  options = list(pageLength = 10, autoWidth = TRUE),\n  caption = \"Table 5: Highest Rated TV Series (Episode Ratings)\"\n) |&gt;\n  formatRound(columns = \"Average Rating\", digits = 2)\n\n\n\n\n\n\nWe can see right away that the results from both approaches are very different from one another. Depending on the context, either answer is suitable. This all depends on how we define the average rating of a series. Some things to consider are for the second approach, it is possible for some episodes to have no ratings at all which can create some skewing of the mean rating. The TV series rating itself can also be different from the average ratings of the episodes.\n\n\nHappy Days(1974 - 1984) has a total of 11 seasons. Since we can’t split it up evenly, I will define the early seasons as seasons 1-6 and the later seasons as season 7-11. To make sure I am looking at the correct series, the first thing to do is find the identifier for Happy Days(1974 - 1984).\n\n\n\nCode\n# Get tconst for the series first so we can find the episode ratings after\nhappy_days_id &lt;- TITLE_BASICS |&gt;\n  filter(\n    primaryTitle == \"Happy Days\",\n    titleType == \"tvSeries\",\n    startYear == 1974,\n    endYear == 1984\n  ) |&gt;\n  select(tconst)\n\n\nNow that tconst has been identified, we can find the average ratings of the early and later seasons.\n\n\nCode\nearly_happy_days_episode_ratings &lt;- TITLE_EPISODES |&gt;\n  semi_join(\n    happy_days_id,\n    join_by(parentTconst == tconst)\n  ) |&gt;\n  left_join(\n    TITLE_RATINGS,\n    join_by(tconst == tconst)\n  ) |&gt;\n  drop_na() |&gt; # get rid of any ratings that are NA\n  filter(seasonNumber &lt; 7) |&gt; # only care about seasons 1-6\n  summarize(avg_rating = mean(averageRating)) |&gt;\n  mutate(season = \"Early Seasons (1-6)\")\n\nlater_happy_days_episode_ratings &lt;- TITLE_EPISODES |&gt;\n  semi_join(\n    happy_days_id,\n    join_by(parentTconst == tconst)\n  ) |&gt;\n  left_join(\n    TITLE_RATINGS,\n    join_by(tconst == tconst)\n  ) |&gt;\n  drop_na() |&gt; # get rid of any ratings that are NA\n  filter(seasonNumber &gt;= 7) |&gt; # only care about seasons 7-11\n  summarize(avg_rating = mean(averageRating)) |&gt;\n  mutate(season = \"Later Seasons (7-11)\")\n\ncombined_ratings &lt;- bind_rows(early_happy_days_episode_ratings, later_happy_days_episode_ratings)\n\nggplot(combined_ratings, aes(x = season, y = avg_rating, fill = season)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(\n    title = \"Average Ratings of Happy Days\",\n    x = \"Seasons\",\n    y = \"Average Rating\"\n  ) +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"Early Seasons (1-6)\" = \"blue\", \"Later Seasons (7-11)\" = \"red\")) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nBased on the data provided, the later seasons did indeed have a lower average rating compared to the earlier seasons."
  },
  {
    "objectID": "mp02.html#quantifying-success",
    "href": "mp02.html#quantifying-success",
    "title": "Mini-Project #02: The Business of Show Business",
    "section": "Quantifying Success",
    "text": "Quantifying Success\nRecall that the goal of the analysis is to develop a new data driven model for coming up with new movie ideas. We want to quantify a movie’s success based on the data we have available to work with. Two metrics we can incorporate into a success factor is the IMDb ratings and number of voters. The IMDb ratings acts as an indicator of quality while the number of voters is an indicator of public popularity.\nI will employ a success score for each movie based on IMDb ratings and number of voters. There are some things to consider beforehand, for example a movie can have a nearly perfect rating but have a small number of voters. We need to account for such cases when scoring the movies. To account for the effects of having a small number of voters, I will be employing a Bayesian average inspired model for my success scoring system. I will not delve into the mathematical theory behind this, but in general terms the Bayesian average will help account for title ratings with a lower number of voters. I encourage you to do your own research on the topic if it interests you.\n\nCustom Success Metric\nBased on the Bayesian average, I will define a new metric called weighted_rating for each movie title as a new rating measurement. The metric will be defined as:\n\\[ weighted\\_rating = [(averageRating * numVotes) / (numVotes + avg\\_num\\_voters)] \\] \\[ + [avg\\_num\\_voters/(numVotes + avg\\_num\\_voters)]*avg\\_movie\\_rating \\]\nAs it stands, the variables avg_num_voters and avg_movie_rating are undefined in our data, but will be defined momentarily. The variable names are representative of what they define, we will be sampling the average number of voters and the average movie rating across all movies in our data. Having these metrics will give us a baseline to work with when quantifying the success of a movie.\nHere we define the average number of voters:\n\n\nCode\navg_num_voters &lt;- TITLE_RATINGS |&gt;\n  left_join(\n    TITLE_BASICS,\n    join_by(tconst == tconst)\n  ) |&gt;\n  filter(titleType == \"movie\") |&gt;\n  summarize(avg_votes = mean(numVotes))\nprint(avg_num_voters)\n\n\n  avg_votes\n1  8691.113\n\n\nBased on the sample, the average number of voters is 8691.414 but for simplicity lets round this to 8700.\nNext, we define the average movie rating:\n\n\nCode\navg_movie_rating &lt;- TITLE_RATINGS |&gt;\n  left_join(\n    TITLE_BASICS,\n    join_by(tconst == tconst)\n  ) |&gt;\n  filter(titleType == \"movie\") |&gt;\n  summarize(avg_rating = mean(averageRating))\nprint(avg_movie_rating)\n\n\n  avg_rating\n1   5.923033\n\n\nThe average movie rating is about 5.9 from our sample.\nWith our baselines variables established, I will now create a new table that consists of movie titles and their weighted rating. Note that this is not the final criteria used to define success. The weighted score is just a new rating number that accounts for the number of people that left a rating on the movie.\n\n\nCode\nmovie_ratings &lt;- TITLE_RATINGS |&gt;\n  left_join(\n    TITLE_BASICS,\n    join_by(tconst == tconst)\n  ) |&gt;\n  filter(titleType == \"movie\") |&gt;\n  mutate(weighted_rating = ((averageRating * numVotes) / (numVotes + 8700)) + ((8700 * 5.9) / (numVotes + 8700))) |&gt;\n  select(tconst, primaryTitle, weighted_rating) |&gt;\n  arrange(desc(weighted_rating))\n\nlimited_movie_ratings &lt;- movie_ratings[1:1000, ] # limit to the first 1000 rows intending to save memory usage\n\ndatatable(setNames(limited_movie_ratings, c(\"ID\", \"Title\", \"Weighted Rating\")),\n  options = list(pageLength = 10, autoWidth = TRUE),\n  caption = \"Table 6: Movie Titles With Weighted Rating\"\n) |&gt;\n  formatRound(columns = \"Weighted Rating\", digits = 2)\n\n\n\n\n\n\nThe next step I will take is creating a new popularity factor to help define success. First, I am taking a look at the maximum & lowest number of voters a movie has.\n\n\nCode\nmax(\n  TITLE_RATINGS |&gt;\n    left_join(TITLE_BASICS, join_by(tconst == tconst)) |&gt;\n    filter(titleType == \"movie\") |&gt;\n    pull(numVotes)\n) # returns 2946100\n\n\n[1] 2952726\n\n\nCode\nmin(\n  TITLE_RATINGS |&gt;\n    left_join(TITLE_BASICS, join_by(tconst == tconst)) |&gt;\n    filter(titleType == \"movie\") |&gt;\n    pull(numVotes)\n) # returns 100\n\n\n[1] 100\n\n\nSince there is such a large discrepancy between the maximum and minimum, a logarithmic scale should be used to scale the number of votes down to a reasonable factor.\n\n\nCode\npopularity_scaling &lt;- TITLE_RATINGS |&gt;\n  left_join(\n    TITLE_BASICS,\n    join_by(tconst == tconst)\n  ) |&gt;\n  filter(titleType == \"movie\") |&gt;\n  mutate(scaled_numVotes = log(numVotes + 1)) |&gt;\n  select(tconst, primaryTitle, scaled_numVotes) |&gt;\n  arrange(desc(scaled_numVotes))\n\nlimited_popularity_scaling &lt;- popularity_scaling[1:1000, ] # limit to the first 1000 rows intending to save memory usage\n\ndatatable(setNames(limited_popularity_scaling, c(\"ID\", \"Title\", \"Scaled Number of Votes\")),\n  options = list(pageLength = 10, autoWidth = TRUE),\n  caption = \"Table 7: Movie Titles With Scaled Votes\"\n) |&gt;\n  formatRound(columns = \"Scaled Number of Votes\", digits = 2)\n\n\n\n\n\n\nTying it all together now, I will define a success score by multiplying the weighted rating and the scaled popularity factor. I’ve defined the new table as TITLE_RATINGS_MOVIES, and will use my new success_score metric for further analysis.\n\n\nCode\nTITLE_RATINGS_MOVIES &lt;- TITLE_RATINGS |&gt;\n  left_join(\n    movie_ratings,\n    join_by(tconst == tconst)\n  ) |&gt;\n  left_join(\n    popularity_scaling,\n    join_by(tconst == tconst)\n  ) |&gt;\n  left_join(\n    TITLE_BASICS,\n    join_by(tconst == tconst)\n  ) |&gt;\n  mutate(success_score = weighted_rating * scaled_numVotes) |&gt;\n  select(tconst, primaryTitle.x, success_score, averageRating, numVotes, startYear) |&gt;\n  arrange(desc(success_score))\n\n\n\n\n\n\n\n\nTo check that the new success metric functions as anticpated, validation will be confirmed with the following:\n\n\n\n\nChoose the top 5-10 movies on your metric and confirm that they were indeed box office successes.\nChoose 3-5 movies with large numbers of IMDb votes that score poorly on your success metric and confirm that they are indeed of low quality.\nChoose a prestige actor or director and confirm that they have many projects with high scores on your success metric.\nPerform at least one other form of ‘spot check’ validation.\nCome up with a numerical threshold for a project to be a ‘success’; that is, determine a value such that movies above are all “solid” or better.\n\n\n\n\nTaking a look at the top 10 movies using my success score, all the movies are box office successes with the exception of The Shawshank Redemption (however this would go onto be a financial success through other mediums). Box office financial information can be found here.\n\n\n\nCode\ntop10_TITLE_RATINGS_MOVIES &lt;- TITLE_RATINGS_MOVIES[1:10, ] # limiting output\n\ntreemap(\n  top10_TITLE_RATINGS_MOVIES,\n  index = \"primaryTitle.x\",\n  vSize = \"success_score\",\n  vColor = \"primaryTitle.x\",\n  type = \"index\",\n  title = \"Top 10 Movies by Success Score\",\n  fontsize.labels = 10,\n  fontsize.title = 14,\n  draw.legend = FALSE\n)\n\n\n\n\n\n\n\n\n\n\n\nTaking a look at the generated list, we can pick out movies with lower success scores and a high number of voters. The 10 lowest scoring movies had at least 20,000 or more voters. Taking a look at the IMDb average rating to measure quality, none of these films were above a 2.0 rating.\n\n\n\nCode\nbad_movies &lt;- TITLE_RATINGS_MOVIES |&gt;\n  arrange(success_score, desc(numVotes))\n\nten_bad_movies &lt;- bad_movies[1:10, ]\n\ndatatable(setNames(ten_bad_movies, c(\"ID\", \"Title\", \"Success Score\", \"Average IMDb Rating\", \"Number of Voters\", \"Release  Year\")),\n  options = list(pageLength = 10, autoWidth = TRUE),\n  caption = \"Table 8: Bad Movies\"\n) |&gt;\n  formatRound(columns = \"Success Score\", digits = 2)\n\n\n\n\n\n\n\n\nFor this validation method, I will be taking a look at Christopher Nolan’s work. Nolan’s works do score high under my defined success metric.\n\n\n\nCode\nNAME_BASICS |&gt; filter(primaryName == \"Christopher Nolan\") # nm0634240, there are more than 1 Christopher Nolan in the database, so I'm making sure I'm choosing the correct one\n\n\n     nconst       primaryName birthYear deathYear        primaryProfession\n1 nm0634240 Christopher Nolan      1970        NA writer,producer,director\n2 nm3059799 Christopher Nolan        NA        NA                      \\\\N\n3 nm9782801 Christopher Nolan        NA        NA        camera_department\n                           knownForTitles\n1 tt6723592,tt0816692,tt1375666,tt0482571\n2 tt1238854,tt0385423,tt0824052,tt1397480\n3         tt16711020,tt10365464,tt5247284\n\n\nCode\nnolan_projects &lt;- NAME_BASICS |&gt;\n  filter(nconst == \"nm0634240\") |&gt;\n  left_join(\n    TITLE_PRINCIPALS,\n    join_by(nconst == nconst)\n  ) |&gt;\n  left_join(\n    TITLE_RATINGS_MOVIES,\n    join_by(tconst == tconst)\n  ) |&gt;\n  select(tconst, primaryTitle.x, success_score) |&gt;\n  drop_na() |&gt;\n  distinct()\n\ndatatable(setNames(nolan_projects, c(\"ID\", \"Title\", \"Success Score\")),\n  options = list(pageLength = 11, autoWidth = TRUE),\n  caption = \"Table 9: Christopher Nolan Projects\"\n) |&gt;\n  formatRound(columns = \"Success Score\", digits = 2)\n\n\n\n\n\n\n\n\nI will check the success scores of the last 5 Oscar winners: Oppenheimer, Everything Everywhere All at Once, CODA, Nomadland & Parasite. All the oscar winners mentioned performed well under my success score.\n\n\n\nCode\noscar_winners &lt;- TITLE_RATINGS_MOVIES |&gt;\n  filter(\n    (primaryTitle.x == \"Oppenheimer\" & startYear == 2023) |\n      (primaryTitle.x == \"Everything Everywhere All at Once\" & startYear == 2022) |\n      (primaryTitle.x == \"CODA\" & startYear == 2021) |\n      (primaryTitle.x == \"Nomadland\" & startYear == 2020) |\n      (primaryTitle.x == \"Parasite\" & startYear == 2019)\n  )\n\ndatatable(setNames(oscar_winners, c(\"ID\", \"Title\", \"Success Score\", \"Average IMDb Rating\", \"Number of Voters\", \"Release Year\")),\n  options = list(pageLength = 10, autoWidth = TRUE),\n  caption = \"Table 10: Last 5 Oscar Winners\"\n) |&gt;\n  formatRound(columns = \"Success Score\", digits = 2)\n\n\n\n\n\n\n\n\nTo come up with a baseline for a “solid” movie, I want to take a look at the distribution of the scores in my rating system. I can use this by looking at the quantile distribution of the success_score. The 75% quantile shows us that 75% of the films fall under 43.6 points, setting this as a benchmark for success. Any movie with a success_score that is greater than or equal 43.6 is a “solid” movie.\n\n\n\nCode\nTITLE_RATINGS_MOVIES |&gt;\n  pull(success_score) |&gt;\n  quantile(na.rm = TRUE)\n\n\n       0%       25%       50%       75%      100% \n 16.98349  31.11881  36.10112  43.59991 138.40482"
  },
  {
    "objectID": "mp02.html#examining-success-by-genre-and-decade",
    "href": "mp02.html#examining-success-by-genre-and-decade",
    "title": "Mini-Project #02: The Business of Show Business",
    "section": "Examining Success by Genre and Decade",
    "text": "Examining Success by Genre and Decade\nNow that a “successful” movie is quantifiable, it is time to uncover trends found over time. Deriving insight from history can help determine what type of movies have been the most successful and point us in a direction when coming up with new movie ideas. The following questions can help determine what type of movie genre should be pursued for a Hollywood success.\n\n\n\n\n\n\nTrends in Success Over Time\n\n\n\n\nWhat was the genre with the most “successes” in each decade?\nWhat genre consistently has the most “successes”? What genre used to reliably produced “successes” and has fallen out of favor?\nWhat genre has produced the most “successes” since 2010? Does it have the highest success rate or does it only have a large number of successes because there are many productions in that genre?\nWhat genre has become more popular in recent years?\n\n\n\n\nIn order to identify the genre with the most “successes”, a new column will need to be created representing the decade the movie was released in. Once a decade has been assigned to each movie, we can further manipulate the data by grouping the genres together by decade to count the total number of movies in each section. Based on my own defined success score, the movie genre drama had the most successes in each decade represented by the data. Keeping this trend in mind, we will dive deeper into the success of dramas shortly.\n\n\n\nCode\nmovie_genre_success &lt;- TITLE_RATINGS_MOVIES |&gt;\n  left_join(\n    TITLE_BASICS,\n    join_by(tconst == tconst)\n  ) |&gt;\n  filter(!is.na(genres) & !is.na(startYear.x)) |&gt;\n  separate_longer_delim(genres, \",\") |&gt;\n  mutate(decade = floor(startYear.x / 10) * 10) |&gt; # create a decade column\n  select(tconst, primaryTitle.x, success_score, startYear.x, decade, genres)\n\ndecade_success &lt;- movie_genre_success |&gt;\n  filter(success_score &gt;= 43.6) |&gt;\n  group_by(decade, genres) |&gt;\n  summarize(\n    total_movies = n(), # Count the total number of successful movies\n    .groups = \"drop\"\n  ) |&gt;\n  group_by(decade) |&gt;\n  slice_max(total_movies, n = 1) |&gt;\n  ungroup()\n\nggplot(decade_success, aes(x = factor(decade), y = total_movies, fill = genres)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") + # Grouped bars\n  labs(\n    title = \"Top Genre per Decade\",\n    x = \"Decade\",\n    y = \"Number of Successful Movies\",\n    fill = \"Genres\"\n  ) +\n  theme_minimal() + # Clean minimal theme\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nOnce again, dramas dominate the genre field over time. Romance movies showed success in the past but have fallen out of favor in recent times. The following plot shows the top 10 genres by decade, the plot is interactive allowing users to manipulate the legend to hide any outputs. While the top genres are easily distinguishable, there are some overlapping data points that can be viewed better by hiding selective genre. Dramas have held onto the top spot throughout the time series, the 1980s was the only decade where comedies came close to taking the number 1 spot.\n\n\n\nCode\n# look at success of genre type by decade again, but this time including 10 genres to gauge how it changes\ndecade_success_top10 &lt;- movie_genre_success |&gt;\n  filter(success_score &gt;= 43.6) |&gt;\n  group_by(decade, genres) |&gt;\n  summarize(\n    total_movies = n(), # Count the total number of successful movies\n    .groups = \"drop\"\n  ) |&gt;\n  group_by(decade) |&gt;\n  slice_max(total_movies, n = 10, with_ties = FALSE) |&gt;\n  mutate(rank = dense_rank(desc(total_movies))) |&gt;\n  ungroup() |&gt;\n  arrange(decade, rank) # Sort by decade and rank\n\n# visually plot\ninteractive_decade_success &lt;- ggplot(decade_success_top10, aes(x = decade, y = total_movies, color = genres, group = genres)) +\n  geom_line(linewidth = 1) + # Plot the lines\n  geom_point(size = 3) + # Add points at each decade\n  labs(\n    title = \"Top 10 Genres per Decade\",\n    x = \"Decade\",\n    y = \"Number of Successful Movies\",\n    color = \"Genres\",\n  ) +\n  scale_x_continuous(breaks = seq(min(decade_success_top10$decade), max(decade_success_top10$decade), by = 10)) + # Set breaks every 10 years\n  theme_minimal() + # Use a clean theme\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"bottom\"\n  )\n# Convert ggplot to interactive plot using plotly\nggplotly(interactive_decade_success)\n\n\n\n\n\n\n\n\nThere is an ongoing trend here, the genre with most successes since 2010 is once again dramas. Taking a look at how many movies in each genre was produced since 2010 can help give context as well. There is definitely an impact by the sheer number of drama movies produced that help it have the highest success rate.\n\n\n\nCode\n# count number of successful projects since 2010 by genre\nsuccesses_since_2010 &lt;- movie_genre_success |&gt;\n  filter(\n    success_score &gt;= 43.6,\n    decade &gt;= 2010\n  ) |&gt;\n  group_by(genres) |&gt;\n  summarize(total_movies = n(), .groups = \"drop\") |&gt;\n  arrange(desc(total_movies))\n\n# count how many movies were made per genre since 2010\nmovie_genres_produced &lt;- movie_genre_success |&gt;\n  filter(decade &gt;= 2010) |&gt;\n  group_by(genres) |&gt;\n  summarize(total_movies_produced = n(), .groups = \"drop\") |&gt;\n  arrange(desc(total_movies_produced))\n\n# combine data\ngenre_distribution_2010 &lt;- left_join(successes_since_2010, movie_genres_produced, by = \"genres\") |&gt;\n  mutate(`Unsuccessful Movies` = total_movies_produced - total_movies) |&gt;\n  arrange(desc(total_movies_produced)) |&gt;\n  slice_head(n = 10)\n\n# pivot the data for long format\ngenre_distribution_2010 &lt;- genre_distribution_2010 |&gt;\n  pivot_longer(\n    cols = c(`Unsuccessful Movies`, total_movies),\n    names_to = \"Type\",\n    values_to = \"Count\"\n  ) |&gt;\n  mutate(Type = ifelse(Type == \"total_movies\", \"Successful Movies\", Type))\n\n# create a dumbbell plot\nggplot(genre_distribution_2010, aes(x = Count, y = reorder(genres, Count), group = Type)) +\n  geom_segment(aes(xend = 0, yend = reorder(genres, Count)), color = \"grey\", size = 1) + # Draw lines to x = 0\n  geom_point(aes(color = Type), size = 3) + # Points for each type\n  labs(\n    title = \"Distribution of Movie Success by Genre Since 2010\",\n    x = \"Number of Movies\",\n    y = \"Genre\"\n  ) +\n  scale_color_manual(values = c(\"Successful Movies\" = \"blue\", \"Unsuccessful Movies\" = \"red\")) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nIf we refer back to the Top 10 Genres per Decade plot, we see that the action genre separated itself from the other genres after the 2000s and has claimed to be the 3rd most popular genre since. Below we can see how the popularity changes over time for each genre to get a better individual understanding.\n\n\n\nCode\nanimate_decade_genre &lt;- ggplot(decade_success_top10, aes(x = decade, y = total_movies, color = genres, group = genres)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 1) +\n  labs(\n    title = \"Top 10 Genres per Decade by Number of Successful Movies\",\n    x = \"Decade\",\n    y = \"Number of Successful Movies\"\n  ) +\n  scale_x_continuous(breaks = seq(min(decade_success_top10$decade), max(decade_success_top10$decade), by = 10)) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 90, hjust = 1),\n    legend.position = \"none\"\n  ) +\n  facet_wrap(~genres) +\n  transition_reveal(decade) # animate the line growth over time (decade)\n\nanimate(animate_decade_genre, renderer = gifski_renderer(file = paste0(save_directory, \"/decade_success_animation.gif\")))\n\n\n\n\n\n\n\n\n\nBased on the insights we’ve uncovered, the genre I would like to pursue for my movie project is drama. According to IMDb, “the drama genre is a broad category that features stories portraying human experiences, emotions, conflicts, and relationships in a realistic and emotionally impactful way”. Taking a closer look at the most successful movies from my own metric in the table below, we see that dramas occupy 9/10 on the list. Due note that the majority of the titles have multiple genres. Drama coexists with other genres on this list, letting us know that success can be created by blending other genres together with drama. While I do want the main focus to be drama, I definitely will want to blend other elements in to give a more refined story.\n\n\nCode\ntop_10 &lt;- TITLE_RATINGS_MOVIES |&gt;\n  left_join(\n    TITLE_BASICS,\n    join_by(tconst == tconst)\n  ) |&gt;\n  select(primaryTitle.x, genres) |&gt;\n  slice_head(n = 10)\n\ndatatable(setNames(top_10, c(\"Title\", \"Genre(s)\")),\n  options = list(pageLength = 10, autoWidth = TRUE),\n  caption = \"Table 11: Top 10 Successful Movies\"\n)"
  },
  {
    "objectID": "mp02.html#successful-personnel-in-the-genre",
    "href": "mp02.html#successful-personnel-in-the-genre",
    "title": "Mini-Project #02: The Business of Show Business",
    "section": "Successful Personnel in the Genre",
    "text": "Successful Personnel in the Genre\nSelecting personnel is an important factor in producing a successful movie. For my movie, I would like to have Christopher Nolan as the director. The first actor I would like to pair is someone who has had success in the past with Nolan. I’ve chosen Cillian Murphy to join my project. The pair has demonstrated they work well together and have successful movies. The graph below shows the four projects they have worked on together. The dotted red line represents the solid success score as discussed earlier. The success scoring system has all of the titles exceeding the baseline, indicating that this director-actor duo is a good choice based on the provided data.\n\n\nCode\nmurphy_nolan_projects &lt;- TITLE_PRINCIPALS |&gt;\n  filter(nconst %in% c(\"nm0634240\", \"nm0614165\")) |&gt;\n  distinct(tconst, nconst) |&gt; # ensure only distinct person-title pairs\n  group_by(tconst) |&gt;\n  summarize(duo_works = n()) |&gt; # counts distinct personnel\n  filter(duo_works == 2) # only keep titles where both are present\n\nmurphy_nolan_projects &lt;- murphy_nolan_projects |&gt;\n  left_join(\n    TITLE_RATINGS_MOVIES,\n    join_by(tconst == tconst)\n  ) |&gt;\n  select(tconst, primaryTitle.x, success_score, startYear)\n\nggplot(murphy_nolan_projects, aes(x = reorder(primaryTitle.x, success_score), y = success_score)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") + # Bar chart with success scores\n  geom_hline(yintercept = 43.6, linetype = \"dashed\", color = \"red\", linewidth = 1) + # horizontal line at y = 43.6 represents solid score\n  labs(\n    title = \"Success Scores of Murphy and Nolan Projects\",\n    x = \"Project Title\",\n    y = \"Success Score\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe next actor I have in mind is an upcoming actor that could potentially become a Hollywood star in the future. I’ve chosen Paul Mescal to join the film, he has experience working in the drama genre and has recently started becoming more popular. At the time of this report, Mescal is set to star in Gladiator 2 which may end up being his big break cementing his name in Hollywood. The table below shows Mescal’s most known works. You can see that dramas are in all four titles, as well as some overlapping genres as well. Mescal’s familiarity with the genre will be an asset for success.\n\n\nCode\nmescal &lt;- NAME_BASICS |&gt;\n  filter(nconst == \"nm8958770\") |&gt;\n  separate_longer_delim(knownForTitles, \",\") |&gt;\n  select(knownForTitles)\n\nmescal &lt;- mescal |&gt;\n  left_join(\n    TITLE_BASICS,\n    join_by(knownForTitles == tconst)\n  ) |&gt;\n  select(primaryTitle, genres)\n\ndatatable(setNames(mescal, c(\"Project Title\", \"Genre(s)\")),\n  options = list(pageLength = 10, autoWidth = TRUE),\n  caption = \"Table 12: Paul Mescal Known Works\"\n)\n\n\n\n\n\n\n\nNostalgia and Remakes\nThe movie I want to remake is Metropolis from 1927. The movie has an average IMDb rating of 8.3 with over 188,000 voters. The movie has drama and sci-fi listed as its genres. It is confirmed there has been no remake of the film since its initial release. I would have Cillian Murphy play Joh Fredersen and Paul Mescal play his son, Freder Fredersen.\n\n\nCode\nmetropolis &lt;- TITLE_BASICS |&gt;\n  filter(primaryTitle == \"Metropolis\",\n         titleType == \"movie\")\n# if we print this original output, we will get two results for Metropolis. However upon further inspection, the Metropolis form 2001 has nothing in common with the movie I am looking to remake. We can move on by specifying the year.\n\nmetropolis &lt;- TITLE_BASICS |&gt;\n  filter(\n    primaryTitle == \"Metropolis\",\n    titleType == \"movie\",\n    startYear == 1927\n  ) |&gt;\n  left_join(\n    TITLE_RATINGS,\n    join_by(tconst == tconst)\n  ) |&gt;\n  select(primaryTitle, startYear, genres, averageRating, numVotes)\n\ndatatable(setNames(metropolis, c(\"Project Title\", \"Release Year\", \"Genre(s)\", \"Average IMDb Rating\", \"Number of Votes\")),\n  caption = \"Table 13: Metropolis\"\n)\n\n\n\n\n\n\nSince the movie is from 1927, the likelihood of any of the key personnel involved in the original film being around is unlikely. We can still double check this using the data provided by IMDb. According to the Metropolis IMDb page the key people are Fritz Lang (director & writer), Thea von Harbou (writer), Brigitte Helm (star), Alfred Abel (star) and Gustav Fröhlich (star). The table below confirms that the key personnel are no longer alive.\n\n\nCode\nmetropolis_personnel &lt;- TITLE_PRINCIPALS |&gt;\n  filter(tconst == \"tt0017136\") |&gt;\n  left_join(NAME_BASICS, join_by(nconst == nconst)) |&gt;\n  select(primaryName, birthYear, deathYear) |&gt;\n  distinct() |&gt;\n  filter(primaryName == \"Fritz Lang\" | primaryName == \"Thea von Harbou\" | primaryName == \"Brigitte Helm\" | primaryName == \"Alfred Abel\" | primaryName == \"Gustav Fröhlich\")\n\ndatatable(setNames(metropolis_personnel, c(\"Name\", \"Birth Year\", \"Death Year\")),\n  caption = \"Table 14: Metropolis Key Personnel\"\n)"
  },
  {
    "objectID": "mp02.html#studio-pitch",
    "href": "mp02.html#studio-pitch",
    "title": "Mini-Project #02: The Business of Show Business",
    "section": "Studio Pitch",
    "text": "Studio Pitch\nDramas have been the bestselling genre in the movie industry in the last century. Between the 1980s to the 2010s, the number of successful drama movies has increased by 394%. Part of the success comes from intertwining the drama genre with other genres to create a more profound story. Among the top ten successful movies, 90% are dramas. Taking a deeper look, 70% of the top ten successful movies blend drama with another genre type. Dramas captivate an audience using emotions and relatability. Tying in another genre adds another layer of complexity that enhances the viewing experience.\nAll of Christopher Nolan’s directorial works have been well received. 58% of the movies Nolan directed included drama as a genre. Cillian Murphy has been a frequent collaborator of Nolan. All their films together have been hits, their most recent work in Oppenheimer won an Oscar award. Recently, Paul Mescal has garnered attention for his work in the drama genre but has yet to reach stardom. Imagine bringing the three individuals together for a drama project. From Christopher Nolan, the visionary mind behind Inception; and from Cillian Murphy, beloved star of Oppenheimer; and from Paul Mescal, television icon of TV dramas, comes the timeless tail of Metropolis, a story of social inequality, dystopian society, and the effects of industrialization coming soon to a theater near you. Metropolis is primed for a remake, with the star power behind the project a box office hit is guaranteed.\n\n\n\nPromotional Metropolis Remake Poster, generated by ChatGPT"
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "",
    "text": "The US electoral college system allocates votes for presidential candidates during the election year. The fine details have changed over the years in history but some basic principles remain the same. Each state receives R+2 electoral votes (ECVs), where R is the number of representatives a state has in the US House of Representatives. States are free to allocate the votes as they like, and the presidential candidate that wins is whoever wins the majority of the electoral college votes.\nThe Constitution has no ruling about how the states need to allocate the ECVs. Each individual state is responsible for their own system implementation. Throughout history, states have chosen to use the following schemes for electoral vote allocation:\n\nState-Wide Winner-Take-All\nDistrict-Wide Winner-Take-All + State-Wide “At Large” Votes\nState-Wide Proportional\nNational Proportional\n\nEach of these allocation strategies will be discussed in-depth later.\nThe objective of this project is to identify how the outcome of the US presidential elections would have changed under different ECVs allocation rules. This will be done by exploring historical congressional election data that is available online."
  },
  {
    "objectID": "mp03.html#introduction",
    "href": "mp03.html#introduction",
    "title": "Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "",
    "text": "The US electoral college system allocates votes for presidential candidates during the election year. The fine details have changed over the years in history but some basic principles remain the same. Each state receives R+2 electoral votes (ECVs), where R is the number of representatives a state has in the US House of Representatives. States are free to allocate the votes as they like, and the presidential candidate that wins is whoever wins the majority of the electoral college votes.\nThe Constitution has no ruling about how the states need to allocate the ECVs. Each individual state is responsible for their own system implementation. Throughout history, states have chosen to use the following schemes for electoral vote allocation:\n\nState-Wide Winner-Take-All\nDistrict-Wide Winner-Take-All + State-Wide “At Large” Votes\nState-Wide Proportional\nNational Proportional\n\nEach of these allocation strategies will be discussed in-depth later.\nThe objective of this project is to identify how the outcome of the US presidential elections would have changed under different ECVs allocation rules. This will be done by exploring historical congressional election data that is available online."
  },
  {
    "objectID": "mp03.html#downloading-importing-the-dataset",
    "href": "mp03.html#downloading-importing-the-dataset",
    "title": "Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Downloading & Importing the Dataset",
    "text": "Downloading & Importing the Dataset\nThe following packages will be used for this analysis: readr, sf, dplyr, tidyr, tidyverse, ggplot2, DT and gganimate. If these packages have not been installed in the system, they can be with the following code:\nif (!require(\"readr\")) install.packages(\"readr\")\nif (!require(\"sf\")) install.packages(\"sf\")\nif (!require(\"dplyr\")) install.packages(\"dplyr\")\nif (!require(\"tidyr\")) install.packages(\"tidyr\")\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif (!require(\"DT\")) install.packages(\"DT\")\nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\nif (!require(\"gganimate\")) install.packages(\"gganimate\")\nLoad the required libraries:\n\n\nCode\nlibrary(readr)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(tidyverse)\nlibrary(DT)\nlibrary(ggplot2)\nlibrary(gganimate)\n\n\nThe first two data files that will need to be downloaded are provided by MIT Election Data Science Lab.1 The data sets we want in particular are the 1976–2022 U.S. House Elections and the 1976-2020 US Presidential Elections. These can be downloaded from the provided links in your browser.\nThe following code will read the CSV files and store them into dataframes to analyze.\n\n\nCode\n# Data 1: US House Election Votes from 1976 to 2022\nhouse_rep_vote_count &lt;- read_csv(\"1976-2022-house.csv\")\npresidential_vote_count &lt;- read_csv(\"1976-2020-president.csv\")\n\n\nThe next set of data needed are the congressional districts shapefiles from 1976 to 2012. The files are provided by Jeffrey B. Lewis, Brandon DeVine, and Lincoln Pritcher with Kenneth C. Martis which can be found here. The following code will automatically download all the necessary files needed.\n\n\nCode\n# Task 1: import congressional district data from cdmaps from 1976 to 2012\nget_cdmaps_file &lt;- function(fname) {\n  BASE_URL &lt;- \"https://cdmaps.polisci.ucla.edu/shp/\"\n  fname_ext &lt;- paste0(fname, \".zip\")\n  if (!file.exists(fname_ext)) {\n    FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n    download.file(FILE_URL,\n                  destfile = fname_ext\n    )\n  }\n}\noptions(timeout = 180) # keep the downloads from potentially timing out\n\nget_cdmaps_file(\"districts112\") # January 5, 2011 to January 3, 2013\nget_cdmaps_file(\"districts111\") # January 6, 2009 to December 22, 2010\nget_cdmaps_file(\"districts110\") # January 4, 2007 to January 3, 2009\nget_cdmaps_file(\"districts109\") # January 4, 2005 to December 9, 2006\nget_cdmaps_file(\"districts108\") # January 7, 2003 to December 8, 2004\nget_cdmaps_file(\"districts107\") # January 3, 2001 to November 22, 2002\nget_cdmaps_file(\"districts106\") # January 6, 1999 to December 15, 2000\nget_cdmaps_file(\"districts105\") # January 7, 1997 to December 19, 1998\nget_cdmaps_file(\"districts104\") # January 4, 1995 to October 4, 1996\nget_cdmaps_file(\"districts103\") # January 5, 1993 to December 1, 1994 \nget_cdmaps_file(\"districts102\") # January 3, 1991 to October 9, 1992\nget_cdmaps_file(\"districts101\") # January 3, 1989 to October 28, 1990\nget_cdmaps_file(\"districts100\") # January 6, 1987 to October 22, 1988\nget_cdmaps_file(\"districts099\")  # January 3, 1985 to October 18, 1986\nget_cdmaps_file(\"districts098\")  # January 3, 1983 to October 12, 1984\nget_cdmaps_file(\"districts097\")  # January 5, 1981 to December 23, 1982\nget_cdmaps_file(\"districts096\")  # January 15, 1979 to December 16, 1980\nget_cdmaps_file(\"districts095\")  # January 4, 1977 to October 15, 1978\nget_cdmaps_file(\"districts094\")  # January 14, 1975 to October 1, 1976\n\n\nThe last set of data needed are the congressional districts shapefiles from 2014 to the present. The files will be taken from the US Census Bureau. The following code will download all necessary files as well.\n\n\nCode\n# Task 2: import data from census\n# download shape files for 113th congress\nif (!file.exists(\"districts113.zip\")) {\n  download.file(\"https://www2.census.gov/geo/tiger/TIGER2013/CD/tl_2013_us_cd113.zip\",\n    destfile = \"districts113.zip\"\n  )\n}\n\n# download shape files for 114th congress\nif (!file.exists(\"districts114.zip\")) {\n  download.file(\"https://www2.census.gov/geo/tiger/TIGER2014/CD/tl_2014_us_cd114.zip\",\n    destfile = \"districts114.zip\"\n  )\n}\n\n# download shape files for 115th congress\nif (!file.exists(\"districts115.zip\")) {\n  download.file(\"https://www2.census.gov/geo/tiger/TIGER2016/CD/tl_2016_us_cd115.zip\",\n    destfile = \"districts115.zip\"\n  )\n}\n\n# download shape files for 116th congress\nif (!file.exists(\"districts116.zip\")) {\n  download.file(\"https://www2.census.gov/geo/tiger/TIGER2018/CD/tl_2018_us_cd116.zip\",\n    destfile = \"districts116.zip\"\n  )\n}"
  },
  {
    "objectID": "mp03.html#analysis",
    "href": "mp03.html#analysis",
    "title": "Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Analysis",
    "text": "Analysis\n\nExploration of Vote Count Data\nNow that all the necessary data has been imported and ready to use, we can begin exploring the information we have available to us. The following questions can be answered by exploring the data.\nWhich states have gained and lost the most seats in the US House of Representatives between 1976 and 2022?\n\n\nCode\n# find the number of seats each state has in 1976 and 2022\nhouse_seats_1976_2022 &lt;- house_rep_vote_count |&gt;\n  filter(year %in% c(1976, 2022)) |&gt;\n  group_by(year, state) |&gt;\n  summarise(total_seats = n_distinct(district)) |&gt; # count number of seats by number of electoral districts\n  select(year, state, total_seats)\n\n# pivot table to find difference easily: state, 1976 seats, 2022 seats\nhouse_seats_1976_2022_wide &lt;- house_seats_1976_2022 |&gt;\n  pivot_wider(names_from = year, values_from = total_seats, names_prefix = \"total_seats_\") |&gt;\n  mutate(difference = total_seats_2022 - total_seats_1976)\n\n# find the change in seats from 2022 to 1976\nseat_changes &lt;- house_seats_1976_2022_wide |&gt;\n  select(state, difference)\n\n# visual representation\nseat_changes_filtered &lt;- seat_changes |&gt; # excluding zero for visual aesthetic \n  filter(difference != 0)\n\nggplot(seat_changes_filtered, aes(x = reorder(state, difference), y = difference, fill = difference &gt; 0)) +\n  geom_bar(stat = \"identity\", show.legend = FALSE) +\n  scale_fill_manual(values = c(\"salmon2\", \"cornflowerblue\")) +  # Blue for increases, red for decreases\n  coord_flip() +  # Flip coordinates for horizontal bars\n  labs(title = \"House Seats Gained/Lost by State (1976-2022)\",\n       x = \"State\",\n       y = \"Change in Seats\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nEvidently, Texas gained the most number of seats in the house of representatives between 1976-2022. On the other hand, New York lost the most since in the same time frame. This indicates that Texas had the largest proportional change in population since 1976. The overall population of the US has shifted away from New York and into other states over the years. The tables below show the exact number of seats changed over the time period.\n\n\nCode\n# take a closer look at the exact number of seats gained\ngained_seats &lt;- seat_changes |&gt;\n  arrange(desc(difference)) |&gt;\n  filter(difference &gt; 0)\n\ndatatable(setNames(gained_seats, c(\"State\", \"Seats Gained\")),\n          options = list(pageLength = 10, autoWidth = TRUE),\n          caption = \"Table 1: House Seats Gained (1976-2022)\"\n)\n\n\n\n\n\n\n\n\nCode\n# take a closer look at the exact number of seats lost\nlost_seats &lt;- seat_changes |&gt;\n  arrange(difference) |&gt;\n  filter(difference &lt; 0)\n\ndatatable(setNames(lost_seats, c(\"State\", \"Seats Lost\")),\n          options = list(pageLength = 10, autoWidth = TRUE),\n          caption = \"Table 2: House Seats Lost (1976-2022)\"\n)\n\n\n\n\n\n\n\nAre there any elections in our data where the election would have had a different outcome if the “fusion” system was not used and candidates only received the votes their received from their “major party line” (Democrat or Republican) and not their total number of votes across all lines?\nA “fusion” voting system is when one candidate can appear on multiple “lines” on the ballot and their vote counts are totaled.\n\n\nCode\n# find the historical winner based on the total votes received on tickets (which includes the fusion system)\nhouse_election_winner &lt;- house_rep_vote_count |&gt;\n  group_by(year, state, district, candidate) |&gt;\n  summarize(total_votes = sum(candidatevotes), .groups = 'drop') |&gt; # aggregate votes across different parties on ticket for fusion ticket candidates\n  group_by(year, state, district) |&gt;\n  slice_max(order_by = total_votes, n = 1, with_ties = FALSE) |&gt; # find the winner based on who has the most total votes\n  rename(historical_winner = candidate) # rename for conventional understanding\n  \n# find winner without fusion system\nprimary_party_winner &lt;- house_rep_vote_count |&gt;\n  group_by(year, state, district, candidate, party) |&gt;\n  summarize(primary_party_votes = sum(candidatevotes), .groups = \"drop\") |&gt; \n  group_by(year, state, district) |&gt;\n  slice_max(order_by = primary_party_votes, n = 1, with_ties = FALSE) |&gt;\n  rename(single_party_winner = candidate) |&gt; # rename for conventional understanding\n  select(-party) # deselecting since I'm only interested in the candidate name and votes\n\n# find any elections where the historical winner is not the same as the single major party winner\npotential_election_changes &lt;- house_election_winner |&gt;\n  left_join(primary_party_winner, by = c(\"year\", \"state\", \"district\")) |&gt;\n  mutate(different_outcome = historical_winner != single_party_winner) |&gt; # create a logic column checks if the outcomes were the same or not\n  filter(different_outcome == TRUE) |&gt; # filter where the historical winner is not the same as the single party vote winner\n  select(-different_outcome)\n\ndatatable(setNames(potential_election_changes, c(\"Year\", \"State\", \"District\", \"Historical Winner\", \"Votes (with fusion)\", \"Single Party Winner\", \"Single Party Votes\")),\n          options = list(pageLength = 10, autoWidth = TRUE),\n          caption = \"Table 3: Potential Differences in House Elections Due to Fusion Voting\"\n)  \n\n\n\n\n\n\n\n \n\nThere are 24 district elections in history where the outcome could have been different if the fusion voting system was not in place. The majority of these occurrences take place in New York.\n\nDo presidential candidates tend to run ahead of or run behind congressional candidates in the same state? That is, does a Democratic candidate for president tend to get more votes in a given state than all Democratic congressional candidates in the same state?\nDoes this trend differ over time? Does it differ across states or across parties? Are any presidents particularly more or less popular than their co-partisans?\n\n\nCode\n# aggregate votes by year, state, and party for house candidates\ncongressional_party_votes &lt;- house_rep_vote_count |&gt;\n  group_by(year, state, party) |&gt;\n  summarize(total_congressional_votes = sum(candidatevotes, na.rm = TRUE), .groups = \"drop\")\n\n# aggregate votes by year, state, and party for presidential candidates\npresidential_party_votes &lt;- presidential_vote_count |&gt;\n  group_by(year, state, party_detailed) |&gt;\n  summarize(total_presidential_votes = sum(candidatevotes, na.rm = TRUE), .groups = \"drop\") |&gt;\n  rename(party = party_detailed) # renaming for joining convention\n\n# find difference between presidential votes and congressional\nvote_disparity &lt;- presidential_party_votes |&gt;\n  inner_join(congressional_party_votes, by = c(\"year\", \"state\", \"party\")) |&gt;\n  mutate(vote_difference = total_presidential_votes - total_congressional_votes) |&gt;\n  select(-total_presidential_votes, -total_congressional_votes)\n\n# focus on Democrat and Republican parties for trends since these are the two major parties\n\n# group by year and party summing across the United States\nvote_disparity_year &lt;- vote_disparity |&gt;\n  filter(party %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  group_by(year, party) |&gt;\n  summarize(total_vote_difference = sum(vote_difference))\n\nggplot(vote_disparity_year,\n       aes(x = year,\n           y = total_vote_difference,\n           color = party)) +\n  geom_point() +\n  geom_line() +\n  scale_color_manual(values = c(\"DEMOCRAT\" = \"blue\", \"REPUBLICAN\" = \"red\")) +\n  labs(title = \"Presidential Votes Disparity\",\n       subtitle = \"Difference between presidential votes and house rep votes aggregated over all states.\",\n       x = \"Year\",\n       y = \"Vote Difference\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nOver time, the Democratic presidential nominee has gained more votes in the US than their house constituents. The Republican presidential nominee has slowly lost popularity and received similar votes to their house constituents over time.\nThe 1984 and 1988 presidential elections were the only elections where the Republican candidate received at least 10 million more votes than their co-partisans running for the house of representatives. The candidates at the time were Ronald Reagan and George H.W. Bush respectively, both of which ended up winning the presidential election.\n\n\nCode\n# group by state and party\nvote_disparity_state &lt;- vote_disparity |&gt;\n  filter(party %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  group_by(state, party) |&gt;\n  summarize(average_vote_difference = round(mean(vote_difference), digits = 0))\n\ndatatable(setNames(vote_disparity_state, c(\"State\", \"Party\", \"Average Vote Difference\")),\n  options = list(pageLength = 10, autoWidth = TRUE),\n  caption = \"Table 4: Average Vote Difference Presidential Candidate Receives vs House Reps\"\n)\n\n\n\n\n\n\n\n \n\nThe table above shows the average vote difference that the presidential candidate receives as compared to their house representative counterparts. The average varies from state to state, exploring the entire table can provide insight into how citizens vote in each state. An interesting insight is that Massachusetts has historically leaned blue in the presidential race, but on average the presidential candidate receives less votes than the constituents combined. Florida tends to be a battleground state in presidential election years, this is seen as both parties draw a significant amount of more votes in for the presidential candidate than the house representatives.\n\n\nChoropleth Maps & Shapefiles\nEarlier we downloaded zip files for the US congressional districts. In order to access the shapefiles, we can automate a zip file extraction with some code.\n\n\nCode\n# Task 4: Automate Zip File Extraction\n# the following code writes a function that can read any zip file and extract the shapefile within\nread_shp_from_zip &lt;- function(zip_file) {\n  temp_dir &lt;- tempdir() # Create a temporary directory\n  zip_contents &lt;- unzip(zip_file, exdir = temp_dir) # Unzip the contents and\n  shp_file &lt;- zip_contents[grepl(\"\\\\.shp$\", zip_contents)] # filter for .shp files\n  sf_object &lt;- read_sf(shp_file) # Read the .shp file into an sf object\n  return(sf_object) # Return the sf object\n}\n\n\nTo get a better idea of what can be done with the shapefiles, a chloropleth visualization of the electoral college results for the 2000 presidential election will be created.\n\n\nCode\nzip_file &lt;- \"districts106.zip\" # district 106 corresponds to when the 2000 elections took place\nshapefile_us &lt;- read_shp_from_zip(zip_file)\n\n# Task 5: Choropleth Visualization of 2000 Electoral College Results\n\n# find election results in each state\nwinner_2000_election &lt;- presidential_vote_count |&gt;\n  filter(year == 2000) |&gt; # 2000 election\n  group_by(state, party_simplified) |&gt; # I want to find the total votes by state and party\n  summarize(total_votes = sum(candidatevotes), .groups = \"drop\") |&gt; # sum the votes across all districts\n  group_by(state) |&gt; # group by state to find the top candidate in each state\n  slice_max(total_votes, n = 1) |&gt;\n  ungroup() |&gt;\n  select(state, party_simplified) |&gt; # don't care for the amount of votes, just the winner\n  rename(winning_party = party_simplified) # rename for convention, will use to fill the choropleth\n\n# join the shape file to election results\nshapefile_us_2000 &lt;- shapefile_us |&gt;\n  mutate(STATENAME = toupper(trimws(STATENAME))) |&gt; # need to match the characters from both tables to join correctly\n  left_join(\n    winner_2000_election,\n    join_by(STATENAME == state)\n  )\n\n\n# create Choropleth of contiguous US first\ncontiguous_us &lt;- ggplot(shapefile_us_2000,\n  aes(\n    geometry = geometry,\n    fill = winning_party\n  ),\n  color = \"black\"\n) +\n  geom_sf() +\n  scale_fill_manual(values = c(\"DEMOCRAT\" = \"skyblue3\", \"REPUBLICAN\" = \"firebrick1\")) +\n  theme_minimal() +\n  labs(\n    title = \"Presidential Election State Results 2000\",\n    subtitle = \"Choropleth Map of U.S. Districts\",\n    fill = \"Winning Party\"\n  ) +\n  theme(legend.position = \"bottom\") +\n  coord_sf(xlim = c(-130, -60), ylim = c(20, 50), expand = FALSE)\n\n# Alaska inset\nalaska_sf &lt;- shapefile_us_2000[shapefile_us_2000$STATENAME == \"ALASKA\", ] # pull Alaska sf info to plot individually\ninset_alaska &lt;- ggplot(alaska_sf,\n  aes(\n    geometry = geometry,\n    fill = winning_party\n  ),\n  color = \"black\"\n) +\n  geom_sf() +\n  scale_fill_manual(values = c(\"DEMOCRAT\" = \"skyblue3\", \"REPUBLICAN\" = \"firebrick1\")) +\n  theme_void() +\n  theme(legend.position = \"none\") + # will display legend on the contiguous plot\n  coord_sf(xlim = c(-180, -140), ylim = c(50, 72), expand = FALSE)\n\n# Hawaii inset\nhawaii_sf &lt;- shapefile_us_2000[shapefile_us_2000$STATENAME == \"HAWAII\", ] # pull Hawaii sf info to plot individually\ninset_hawaii &lt;- ggplot(hawaii_sf,\n  aes(\n    geometry = geometry,\n    fill = winning_party\n  ),\n  color = \"black\"\n) +\n  geom_sf() +\n  scale_fill_manual(values = c(\"DEMOCRAT\" = \"skyblue3\", \"REPUBLICAN\" = \"firebrick1\")) +\n  theme_void() +\n  theme(legend.position = \"none\") + # will display legend on the contiguous plot\n  coord_sf(xlim = c(-161, -154), ylim = c(18, 23), expand = FALSE)\n\ncombined_map &lt;- contiguous_us +\n  annotation_custom(ggplotGrob(inset_alaska),\n    xmin = -120, xmax = -130, # Adjust position for Alaska\n    ymin = 15, ymax = 40\n  ) +\n  annotation_custom(ggplotGrob(inset_hawaii),\n    xmin = -115, xmax = -100, # Adjust position for Hawaii\n    ymin = 20, ymax = 30\n  ) # Adjust these values to fit\n\n# Print the combined map\nprint(combined_map)\n\n\n\n\n\n\n\n\n\n\nTaking it a step further, we can see how the election results changed over the years in each states with an animated plot.\n\n\nCode\n# Task 6: Advanced Choropleth Visualization of Electoral College Results\n# Modify your previous code to make an animated version showing election results over time.\n\n# write a function that gets the presidential election winners in each desired election year\n\nelection_years &lt;- c(1976, 1980, 1984, 1988, 1992, 1996, 2000, 2004, 2008, 2012, 2016, 2020) # election years in the data\n\n# majority of the code is the same as above used to find the winner in 2000, but this time including the year\nget_winner_election_year &lt;- function(input_year) {\n  presidential_vote_count |&gt;\n    filter(year == input_year) |&gt; # Filter for the specific year\n    group_by(state, party_simplified) |&gt;\n    summarize(total_votes = sum(candidatevotes), .groups = \"drop\") |&gt;\n    group_by(state) |&gt;\n    slice_max(total_votes, n = 1) |&gt;\n    ungroup() |&gt;\n    select(state, party_simplified) |&gt;\n    rename(winning_party = party_simplified) |&gt;\n    mutate(year = input_year) # add year to the table\n}\n\n# bind the results into one table for time frame animation\nelection_winner_by_year &lt;- bind_rows(lapply(election_years, get_winner_election_year)) # lapply() will take the arguments in election_years and input it into the get_winner_election_year function\n\n# unzip shape file for states\nshapefile_states &lt;- read_shp_from_zip(\"tl_2018_us_state.zip\")\n\n# join the US states shapefile to the list of election winners\n\nshapefile_states &lt;- shapefile_states|&gt;\n  mutate(NAME = toupper(trimws(NAME))) |&gt; # need to match the characters from both tables to join correctly\n  left_join(election_winner_by_year,\n    join_by(NAME == state),\n    relationship = \"many-to-many\"\n  ) |&gt;\n  filter(!is.na(year))\n\n# animated plot\nanimate_election &lt;- ggplot(shapefile_states,\n  aes(\n    geometry = geometry,\n    fill = winning_party\n  ),\n  color = \"black\"\n) +\n  geom_sf() +\n  scale_fill_manual(values = c(\"DEMOCRAT\" = \"skyblue3\", \"REPUBLICAN\" = \"firebrick1\")) +\n  theme_minimal() +\n  labs(\n    title = \"Presidential Election State Results {closest_state}\",\n    subtitle = \"Choropleth Map of U.S. States\",\n    fill = \"Winning Party\"\n  ) +\n  theme(legend.position = \"bottom\") +\n  transition_states(year, transition_length = 0, state_length = 1) +\n  coord_sf(xlim = c(-175, -60), expand = FALSE)\n\nanimate(animate_election, renderer = gifski_renderer(file = paste0(save_directory, \"/election_results_animation.gif\"))) # save as a GIF\n\n\n\n\n\n\n\n\n\n\n\n\nComparing the Effects of ECV Allocation Rules\nTaking a look at the ECVs allocation schemes mentioned earlier, we can explore historical data to compare the derive the winning presidential candidate under each scheme and compare it to the actual historical winner.\nThe first task at hand is to identify the number of electoral votes each state has in each election year as it has changed over time.\n\n\nCode\n# Task 7: Evaluating Fairness of ECV Allocation Schemes\n# find number of electoral votes each state has each year\nelectoral_votes_by_state &lt;- house_rep_vote_count |&gt;\n  group_by(year, state) |&gt;\n  summarize(total_reps = n_distinct(district)) |&gt;\n  mutate(electoral_votes = total_reps + 2) |&gt; # R+2 votes\n  select(year, state, electoral_votes)\n\n# add DC votes\nelectoral_votes_by_state &lt;- electoral_votes_by_state |&gt;\n  ungroup() |&gt;\n  add_row(year = 1976, state = \"DISTRICT OF COLUMBIA\", electoral_votes = 3) |&gt;\n  add_row(year = 1980, state = \"DISTRICT OF COLUMBIA\", electoral_votes = 3) |&gt;\n  add_row(year = 1984, state = \"DISTRICT OF COLUMBIA\", electoral_votes = 3) |&gt;\n  add_row(year = 1988, state = \"DISTRICT OF COLUMBIA\", electoral_votes = 3) |&gt;\n  add_row(year = 1992, state = \"DISTRICT OF COLUMBIA\", electoral_votes = 3) |&gt;\n  add_row(year = 1996, state = \"DISTRICT OF COLUMBIA\", electoral_votes = 3) |&gt;\n  add_row(year = 2000, state = \"DISTRICT OF COLUMBIA\", electoral_votes = 3) |&gt;\n  add_row(year = 2004, state = \"DISTRICT OF COLUMBIA\", electoral_votes = 3) |&gt;\n  add_row(year = 2008, state = \"DISTRICT OF COLUMBIA\", electoral_votes = 3) |&gt;\n  add_row(year = 2012, state = \"DISTRICT OF COLUMBIA\", electoral_votes = 3) |&gt;\n  add_row(year = 2016, state = \"DISTRICT OF COLUMBIA\", electoral_votes = 3) |&gt;\n  add_row(year = 2020, state = \"DISTRICT OF COLUMBIA\", electoral_votes = 3) |&gt;\n  distinct() # avoid any duplicate entries in case\n\n\n\nState-Wide Winner-Take-All\nWhichever candidate receives the most votes in the state wins all the electoral votes.\n\n\nCode\n# find the candidate with the most votes each year in each state\nstate_wide_winner_take_all &lt;- presidential_vote_count |&gt;\n  group_by(year, state, candidate) |&gt;\n  summarize(total_votes = sum(candidatevotes), .groups = \"drop\") |&gt;\n  group_by(year, state) |&gt;\n  slice_max(order_by = total_votes, n = 1, with_ties = FALSE) |&gt; # find the winner of each state based on who has the most total votes\n  rename(winner = candidate) # rename for conventional understanding\n\n# join the state, winner, and number of electoral votes & sum which candidate that gets the most electoral votes\nstate_wide_winner_take_all &lt;- state_wide_winner_take_all |&gt;\n  left_join(electoral_votes_by_state,\n    by = c(\"year\", \"state\")\n  ) |&gt;\n  group_by(year, winner) |&gt;\n  summarize(total_electoral_votes = sum(electoral_votes)) |&gt; # sum electoral votes across all states by candidate each year\n  slice_max(order_by = total_electoral_votes, n = 1, with_ties = FALSE)\n\ndatatable(setNames(state_wide_winner_take_all, c(\"Year\", \"Winning Candidate\", \"Electoral Votes\")),\n          options = list(pageLength = 12, autoWidth = TRUE),\n          caption = \"Table 5: State-Wide Winner-Take-All: Presidential Winning Candidate\"\n)\n\n\n\n\n\n\n\n\nDistrict-Wide Winner-Take-All + State-Wide “At Large” Votes\nEach district electoral vote is assigned to the district winner, and whichever candidate wins the popular vote in the state wins the remaining 2 at large votes.\nAssume the winning party of the house district is the same for the president.\n\n\nCode\n# find number of districts each party won to represent electoral votes won in each state\ndistrict_winner &lt;- house_rep_vote_count |&gt;\n  group_by(year, state, district) |&gt;\n  slice_max(order_by = candidatevotes, n = 1, with_ties = FALSE) |&gt;\n  select(year, state, district, party) |&gt;\n  group_by(year, state, party) |&gt;\n  summarize(districts_won = n()) # number of electoral votes received by each party\n\n# find popular vote winner in the state\nat_large_winner &lt;- house_rep_vote_count |&gt;\n  group_by(year, state) |&gt;\n  slice_max(order_by = candidatevotes, n = 1, with_ties = FALSE) |&gt;\n  select(year, state, party) |&gt;\n  add_column(at_large_votes = 2) # designating the vote count\n\n# join tables together to find total electoral votes the presidential party receives in each state\ndistrict_wide_winner_take_all &lt;- district_winner |&gt;\n  left_join(at_large_winner,\n    by = c(\"year\", \"state\", \"party\")\n  ) |&gt;\n  mutate(across(where(is.numeric), ~ ifelse(is.na(.), 0, .))) |&gt; # set NA to 0 for the rows that had no resulting joins\n  mutate(total_electoral_votes = districts_won + at_large_votes) |&gt;\n  select(-districts_won, -at_large_votes) |&gt;\n  rename(party_simplified = party) |&gt; # rename for easier joining convention\n  left_join(presidential_vote_count,\n    by = c(\"year\", \"state\", \"party_simplified\")\n  ) |&gt; # join to presidential candidate\n  select(year, state, total_electoral_votes, candidate) |&gt;\n  group_by(year, candidate) |&gt;\n  summarize(electoral_votes = sum(total_electoral_votes)) |&gt;\n  slice_max(order_by = electoral_votes, n = 1, with_ties = FALSE) |&gt;\n  drop_na() # get rid of the non-presidential election years\n\ndatatable(setNames(district_wide_winner_take_all, c(\"Year\", \"Winning Candidate\", \"Electoral Votes\")),\n          options = list(pageLength = 12, autoWidth = TRUE),\n          caption = \"Table 6: District-Wide Winner-Take-All: Presidential Winning Candidate\"\n)\n\n\n\n\n\n\n\n\nState-Wide Proportional\nElectoral votes are distributed based on the percentage of the popular vote the candidate receives in the state.\n\n\nCode\n# find the percentage of the votes received in each state\nstate_wide_proportional &lt;- presidential_vote_count |&gt;\n  select(year, state, candidate, candidatevotes, totalvotes) |&gt;\n  mutate(percentage_state_votes = (candidatevotes / totalvotes)) |&gt;\n  select(-candidatevotes, -totalvotes)\n\n# find the number of electoral votes received by each candidate\nstate_wide_proportional &lt;- state_wide_proportional |&gt;\n  left_join(electoral_votes_by_state,\n    by = c(\"year\", \"state\")\n  ) |&gt;\n  mutate(votes_received = round(percentage_state_votes * electoral_votes, digits = 0)) |&gt;\n  select(-percentage_state_votes, -electoral_votes)\n\n# sum total votes and find presidential winner\nstate_wide_proportional &lt;- state_wide_proportional |&gt;\n  group_by(year, candidate) |&gt;\n  summarize(total_electoral_votes = sum(votes_received)) |&gt;\n  slice_max(order_by = total_electoral_votes, n = 1, with_ties = FALSE) |&gt;\n  rename(winner = candidate)\n\ndatatable(setNames(state_wide_proportional, c(\"Year\", \"Winning Candidate\", \"Electoral Votes\")),\n          options = list(pageLength = 12, autoWidth = TRUE),\n          caption = \"Table 7: State-Wide Proportional: Presidential Winning Candidate\"\n)\n\n\n\n\n\n\n\n\nNational Proportional\nTake a look at the overall popular vote across the US, the candidate receives the number of electoral votes based on the percentage of the popular vote they received.\n\n\nCode\n# find total number of electoral votes available\nelectoral_votes_available &lt;- electoral_votes_by_state |&gt;\n  group_by(year) |&gt;\n  summarize(electoral_college_votes = sum(electoral_votes))\n\n# find percentage of popular vote each candidate received\nnational_proportional &lt;- presidential_vote_count |&gt;\n  select(year, state, candidate, candidatevotes) |&gt;\n  group_by(year, candidate) |&gt;\n  summarize(total_electoral_votes = sum(candidatevotes)) |&gt;\n  group_by(year) |&gt;\n  mutate(population_vote_count = sum(total_electoral_votes)) |&gt; # find total number of votes cast in election year\n  ungroup() |&gt;\n  mutate(percentage_population_vote = (total_electoral_votes / population_vote_count)) |&gt;\n  select(-total_electoral_votes, -population_vote_count) |&gt;\n  # find the proportion of the electoral votes received based on the popular vote percentage\n  left_join(\n    electoral_votes_available,\n    join_by(year == year)\n  ) |&gt;\n  mutate(electoral_votes_received = round(percentage_population_vote * electoral_college_votes, digits = 0)) |&gt;\n  select(-percentage_population_vote, -electoral_college_votes) |&gt;\n  group_by(year) |&gt;\n  slice_max(order_by = electoral_votes_received, n = 1, with_ties = FALSE) |&gt;\n  rename(winner = candidate)\n\ndatatable(setNames(national_proportional, c(\"Year\", \"Winning Candidate\", \"Electoral Votes\")),\n          options = list(pageLength = 12, autoWidth = TRUE),\n          caption = \"Table 8: National Proportional: Presidential Winning Candidate\"\n)\n\n\n\n\n\n\n\n \n\nTo do so, you should first determine which allocation scheme you consider “fairest”. You should then see which schemes give different results, if they ever do. To make your fact check more compelling, select one election where the ECV scheme had the largest impact–if one exists–and explain how the results would have been different under a different ECV scheme.\nEach electoral college vote allocation scheme has its own pros and drawbacks. I believe that the national proportional allocation scheme is the “fairest” strategy out of all schemes. It is representative of the entire nation’s voting populations interests. The president is the governing entity of the entire nation, the national proportional scheme represents the entire nation’s interests as a whole. The population of the United States is not distributed evenly across all the states, thus this scheme is fair since it accounts for everyone’s vote regardless of where they reside.\nThe state-wide proportional scheme is the next “fairest” allocation system that can be implemented. It is similar to the national-proportional in nature but it takes a step down and looks at the interests of the voters by state instead. Since different opinions can exist throughout a state, this allocates a fairer distribution of votes. The only drawback is that the representation is not on a national level, but on a state by state level. There can be a slight skew in favoritism since each state has a different amount of electoral votes available.\nIn my opinion, the state-wide winner-take-all and district-wide winner-take-all are the least “fair” allocation systems. They do not offer a true representation of the nation’s interests, but focus on a subset of opinions that may not be commonly held by the entire nation. The state-wide winner-take-all system suffers from representing the entire sentiment of the voters in a state. A small majority win can tip the balance to one party, whilst ignoring the remaining voter’s opinions in the state. The district-wide winner-take-all system examines the opinions of voter’s on a district level. Although this may seem fair since each district has a voice, the distribution can be skewed since the districts don’t represent the population size located within. Political tactics like gerrymandering can greatly effect the voices of voters and not reflect the overall voter preferences.\nThe 2000 presidential election is an example of where the electoral college vote scheme had the greatest impact on the results. The national proportional scheme solely determined that Al Gore would have won the election. Every other scheme resulted in George W. Bush winning the election, including the historical one."
  },
  {
    "objectID": "mp03.html#footnotes",
    "href": "mp03.html#footnotes",
    "title": "Mini-Project #03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMIT Election Data + Science Lab. (n.d.). MIT Election Lab. MIT Election Data + Science Lab. https://electionlab.mit.edu/↩︎"
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "",
    "text": "This mini project will explore an example of personal financial decision making with R. Within the scope of this project, the following will be done:\n\nUse a password-protected API to acquire financial data\nUse resampling inference to estimate complex probability distributions\nSee how the optimal financial decision varies as a function of market returns\nInvestigate how demographic and actuarial assumptions, as well as individual risk-tolerances, change the optimal decision.\n\nThis will specifically focus on CUNY employee retirement plans. Newly hired faculty have 30 days to choose between two retirement plans. It can be difficult to assess which plan would be better in the long run dependent on a multitude of factors in life. This project will explore historical financial data and use a bootstrap inference strategy to estimate the probability that one plan is better than the other dependent on circumstances.\n\n\n\n\n\n\nThe first plan CUNY offers is a Teachers Retirement System (TRS) plan. The TRS plan is a traditional pension program, the employer will pay a fraction of the employee’s salary until death. The TRS is a defined benefit plan where the employer takes on the market risk and covers the expenses in the long term if the market under performs.\nThe TRS plan outline:\n\nEmployees pay a fixed percentage of their paycheck into the pension fund. For CUNY employees joining after March 31, 2012–which you may assume for this project–the so-called “Tier VI” contribution rates are based on the employee’s annual salary and increase as follows:\n\n$45,000 or less: 3%\n$45,001 to $55,000: 3.5%\n$55,001 to $75,000: 4.5%\n$75,001 to $100,000: 5.75%\n$100,001 or more: 6%\n\nThe retirement benefit is calculated based on the Final Average Salary of the employee: following 2024 law changes, the FAS is computed based on the final three years salary. (Previously, FAS was computed based on 5 years: since salaries tend to increase monotonically over time, this is a major win for TRS participants.)\nIf \\(N\\) is the number of years served, the annual retirement benefit is:\n\n\\(1.67\\% * \\text{FAS} * N\\) if \\(N \\leq 20\\)\n\\(1.75\\% * \\text{FAS} * N\\) if \\(N = 20\\)\n\\((35\\% + 2\\% * N) * \\text{FAS}\\) if \\(N \\geq 20\\)[^3]\n\nIn each case, the benefit is paid out equally over 12 months.\nThe benefit is increased annually by 50% of the CPI, rounded up to the nearest tenth of a percent: e.g., a CPI of 2.9% gives an inflation adjustment of 1.5%. The benefit is capped below at 1% and above at 3%, so a CPI of 10% leads to a 3% inflation adjustment while a CPI of 0% leads to a 1% inflation adjustment.\nThe inflation adjustement is effective each September and the CPI used is the aggregate monthly CPI of the previous 12 months; so the September 2024 adjustment depends on the CPI from September 2023 to August 2024.\n\n\n\n\nThe optional retirement plan (OPR) is in line with a traditional 401(k) plan. Both the employer and employee make contributions into a retirement account. The investments are made in the employee’s choice of mutual funds. At retirement, the employee will be able to access those funds and withdraw as they like. For this analysis it can be assumed that the investments are made with the following allocation:\n\nAge 25 to Age 49:\n\n54% US Equities\n36% International Equities\n10% Bonds\n\nAge 50 to Age 59:\n\n47% US Equities\n32% International Equities\n21% Bonds\n\nAge 60 to Age 74:\n\n34% US Equities\n23% International Equities\n43% Bonds\n\nAge 75 or older:\n\n19% US Equities\n13% International Equities\n62% Bonds\n6% Short-Term Debt\n\n\nUnder the ORP, both the employee and the employer make monthly contributions to the employee’s ORP account. These returns are calculated as a percentage of the employee’s annual salary. Specifically, the employee contributes at the same rate as the TRS:\n\n$45,000 or less: 3%\n$45,001 to $55,000: 3.5%\n$55,001 to $75,000: 4.5%\n$75,001 to $100,000: 5.75%\n$100,001 or more: 6%\n\nThe employer contribution is fixed at:\n\n8% for the first seven years of employment at CUNY.\n10% for all years thereafter.\n\nYou may assume that the contributions are immediately invested according to the asset allocations above.\n\n\n\nThe power of compounding interest allows for investments to grow at an exponential rate over time. More detailed explanation of compounding interest can be found here. The final value of the investment should be much greater than what was contributed over time."
  },
  {
    "objectID": "mp04.html#introduction",
    "href": "mp04.html#introduction",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "",
    "text": "This mini project will explore an example of personal financial decision making with R. Within the scope of this project, the following will be done:\n\nUse a password-protected API to acquire financial data\nUse resampling inference to estimate complex probability distributions\nSee how the optimal financial decision varies as a function of market returns\nInvestigate how demographic and actuarial assumptions, as well as individual risk-tolerances, change the optimal decision.\n\nThis will specifically focus on CUNY employee retirement plans. Newly hired faculty have 30 days to choose between two retirement plans. It can be difficult to assess which plan would be better in the long run dependent on a multitude of factors in life. This project will explore historical financial data and use a bootstrap inference strategy to estimate the probability that one plan is better than the other dependent on circumstances.\n\n\n\n\n\n\nThe first plan CUNY offers is a Teachers Retirement System (TRS) plan. The TRS plan is a traditional pension program, the employer will pay a fraction of the employee’s salary until death. The TRS is a defined benefit plan where the employer takes on the market risk and covers the expenses in the long term if the market under performs.\nThe TRS plan outline:\n\nEmployees pay a fixed percentage of their paycheck into the pension fund. For CUNY employees joining after March 31, 2012–which you may assume for this project–the so-called “Tier VI” contribution rates are based on the employee’s annual salary and increase as follows:\n\n$45,000 or less: 3%\n$45,001 to $55,000: 3.5%\n$55,001 to $75,000: 4.5%\n$75,001 to $100,000: 5.75%\n$100,001 or more: 6%\n\nThe retirement benefit is calculated based on the Final Average Salary of the employee: following 2024 law changes, the FAS is computed based on the final three years salary. (Previously, FAS was computed based on 5 years: since salaries tend to increase monotonically over time, this is a major win for TRS participants.)\nIf \\(N\\) is the number of years served, the annual retirement benefit is:\n\n\\(1.67\\% * \\text{FAS} * N\\) if \\(N \\leq 20\\)\n\\(1.75\\% * \\text{FAS} * N\\) if \\(N = 20\\)\n\\((35\\% + 2\\% * N) * \\text{FAS}\\) if \\(N \\geq 20\\)[^3]\n\nIn each case, the benefit is paid out equally over 12 months.\nThe benefit is increased annually by 50% of the CPI, rounded up to the nearest tenth of a percent: e.g., a CPI of 2.9% gives an inflation adjustment of 1.5%. The benefit is capped below at 1% and above at 3%, so a CPI of 10% leads to a 3% inflation adjustment while a CPI of 0% leads to a 1% inflation adjustment.\nThe inflation adjustement is effective each September and the CPI used is the aggregate monthly CPI of the previous 12 months; so the September 2024 adjustment depends on the CPI from September 2023 to August 2024.\n\n\n\n\nThe optional retirement plan (OPR) is in line with a traditional 401(k) plan. Both the employer and employee make contributions into a retirement account. The investments are made in the employee’s choice of mutual funds. At retirement, the employee will be able to access those funds and withdraw as they like. For this analysis it can be assumed that the investments are made with the following allocation:\n\nAge 25 to Age 49:\n\n54% US Equities\n36% International Equities\n10% Bonds\n\nAge 50 to Age 59:\n\n47% US Equities\n32% International Equities\n21% Bonds\n\nAge 60 to Age 74:\n\n34% US Equities\n23% International Equities\n43% Bonds\n\nAge 75 or older:\n\n19% US Equities\n13% International Equities\n62% Bonds\n6% Short-Term Debt\n\n\nUnder the ORP, both the employee and the employer make monthly contributions to the employee’s ORP account. These returns are calculated as a percentage of the employee’s annual salary. Specifically, the employee contributes at the same rate as the TRS:\n\n$45,000 or less: 3%\n$45,001 to $55,000: 3.5%\n$55,001 to $75,000: 4.5%\n$75,001 to $100,000: 5.75%\n$100,001 or more: 6%\n\nThe employer contribution is fixed at:\n\n8% for the first seven years of employment at CUNY.\n10% for all years thereafter.\n\nYou may assume that the contributions are immediately invested according to the asset allocations above.\n\n\n\nThe power of compounding interest allows for investments to grow at an exponential rate over time. More detailed explanation of compounding interest can be found here. The final value of the investment should be much greater than what was contributed over time."
  },
  {
    "objectID": "mp04.html#data-sources",
    "href": "mp04.html#data-sources",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Data Sources",
    "text": "Data Sources\nThe data sources used in this project will be accessed from Alpha Vantage and the Federal Reserve Economic Data (FRED). A free API key can be obtained for both sites to access the data used in this analysis. All that’s required is to sign up for an account on the site to access the API keys.\nThe Alpha Vantage API key can be claimed here and be used with the supporting documentation.\nThe FRED API key can be claimed here and be used with the supporting documentation.\nThe API keys are linked to the account you signed up with. In the following code I provide, I will read into R my API keys from a text file, and reproduction should be done so as well to not publish your personal API key.\n\n\nCode\n# Task 1 & 2\n# import API keys\nalpha_vantage_key &lt;- readLines(\"AlphaVantageAPIKey.txt\", warn = FALSE)[1] # api key located on first line of txt file\nFRED_key &lt;- readLines(\"FREDAPIKey.txt\", warn = FALSE)[1]"
  },
  {
    "objectID": "mp04.html#analysis",
    "href": "mp04.html#analysis",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Analysis",
    "text": "Analysis\nThe following libraries will be used in this project:\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(httr2)\nlibrary(lubridate)\nlibrary(DT)\nlibrary(ggplot2)\n\n\nData Acquisition\nThe relevant data series we need to acquire for our analysis is the following:\n\nWage growth\nInflation\nUS Equity Market total returns\nInternational Equity Market total returns\nBond market total returns\nShort-term debt returns\n\nThe following code chunks will import the data using both the Alpha Vantage API and FRED API. The first four code chunks will import data using the Alpha Vantage API. The last two code chunks will use the FRED API to import the data.\nThe first data series to be imported will be the US equity market total returns. For the purposes of this project, I will be using SPY as the tracker. There are other equivalent indicators that can be used too.\n\n\nCode\n# Task 3: Data Acquisition\n# Alpha Vantage API documentation lets us know exactly what data we can find\n\n# US Equity Market total returns, represented by SPY for our case\n\n# Function to get TIME_SERIES_DAILY data\nget_sp500_data &lt;- function(api_key) {\n  response &lt;- request(\"https://www.alphavantage.co/query\") |&gt;\n    req_url_query(\n      `function` = \"TIME_SERIES_DAILY\", # Backticks for reserved word\n      symbol = \"SPY\", # S&P 500 ETF symbol\n      apikey = api_key,\n      outputsize = \"full\", # Full dataset\n      datatype = \"json\" # JSON format\n    ) |&gt;\n    req_perform()\n\n  # Parse the response as JSON\n  data &lt;- resp_body_json(response, simplifyVector = TRUE)\n  return(data)\n}\n\nsp500_data &lt;- get_sp500_data(alpha_vantage_key)\n\n# Extract the time series data\nspy_time_series &lt;- sp500_data$`Time Series (Daily)`\n\n# Convert the nested list to a data frame\nspy &lt;- as.data.frame(do.call(rbind, spy_time_series))\n\n# Add the dates as a column\nspy &lt;- cbind(date = rownames(spy), spy)\n\n# Reset rownames\nrownames(spy) &lt;- NULL\n\n# Rename columns for the regular time series data\ncolnames(spy) &lt;- c(\n  \"date\",\n  \"open\",\n  \"high\",\n  \"low\",\n  \"close\",\n  \"volume\"\n)\n\n# Convert columns to correct types\nspy &lt;- transform(spy,\n  date = as.Date(date),\n  open = as.numeric(open),\n  high = as.numeric(high),\n  low = as.numeric(low),\n  close = as.numeric(close),\n  volume = as.numeric(volume)\n)\n\n\nThe next series to be imported is the international equity market total returns. In this analysis, FDIVX will be used to measure the international market.\n\n\nCode\n# International Equity Market total returns\n# Function to get TIME_SERIES_DAILY data\nget_FDIVX_data &lt;- function(api_key) {\n  response &lt;- request(\"https://www.alphavantage.co/query\") |&gt;\n    req_url_query(\n      `function` = \"TIME_SERIES_DAILY\", # Backticks for reserved word\n      symbol = \"FDIVX\", # Fidelity International Growth Fund symbol\n      apikey = api_key,\n      outputsize = \"full\", # Full dataset\n      datatype = \"json\" # JSON format\n    ) |&gt;\n    req_perform()\n\n  # Parse the response as JSON\n  data &lt;- resp_body_json(response, simplifyVector = TRUE)\n  return(data)\n}\n\nFDIVX_data &lt;- get_FDIVX_data(alpha_vantage_key)\n\n# Extract the time series data\nFDIVX_time_series &lt;- FDIVX_data$`Time Series (Daily)`\n\n# Convert the nested list to a data frame\nFDIVX &lt;- as.data.frame(do.call(rbind, FDIVX_time_series))\n\n# Add the dates as a column\nFDIVX &lt;- cbind(date = rownames(FDIVX), FDIVX)\n\n# Reset rownames\nrownames(FDIVX) &lt;- NULL\n\n# Rename columns for the regular time series data\ncolnames(FDIVX) &lt;- c(\n  \"date\",\n  \"open\",\n  \"high\",\n  \"low\",\n  \"close\",\n  \"volume\"\n)\n\n# Convert columns to correct types\nFDIVX &lt;- transform(FDIVX,\n  date = as.Date(date),\n  open = as.numeric(open),\n  high = as.numeric(high),\n  low = as.numeric(low),\n  close = as.numeric(close),\n  volume = as.numeric(volume)\n)\n\n\nTo track inflation, I will use the consumer price index (CPI) as an indicator of inflation over time.\n\n\nCode\n# Inflation\nget_inflation_data &lt;- function(api_key) {\n  response &lt;- request(\"https://www.alphavantage.co/query\") |&gt;\n    req_url_query(\n      `function` = \"CPI\", # Backticks for reserved word, CPI is widely regarded as the barometer of inflation levels in the broader economy\n      apikey = api_key,\n      outputsize = \"full\", # Full dataset\n      datatype = \"json\" # JSON format\n    ) |&gt;\n    req_perform()\n\n  # Parse the response as JSON\n  data &lt;- resp_body_json(response, simplifyVector = TRUE)\n  return(data)\n}\n\ninflation_data &lt;- get_inflation_data(alpha_vantage_key)\n\ninflation &lt;- inflation_data$data\n\ncolnames(inflation) &lt;- c(\n  \"date\",\n  \"index_value\"\n)\n\ninflation &lt;- transform(inflation,\n  date = as.Date(date),\n  index_value = as.numeric(index_value)\n)\n\n\nFor the short term debt returns, I will be using the 2 year US Treasury Yield as an indicator.\n\n\nCode\n# short-term debt returns, using the 2 year us treasury yield\n\nget_short_term_yield_data &lt;- function(api_key) {\n  response &lt;- request(\"https://www.alphavantage.co/query\") |&gt;\n    req_url_query(\n      `function` = \"TREASURY_YIELD\", # Backticks for reserved word\n      maturity = \"2year\",\n      apikey = api_key,\n      outputsize = \"full\", # Full dataset\n      datatype = \"json\" # JSON format\n    ) |&gt;\n    req_perform()\n  \n  # Parse the response as JSON\n  data &lt;- resp_body_json(response, simplifyVector = TRUE)\n  return(data)\n}\n\nshort_term_yield_data &lt;- get_short_term_yield_data(alpha_vantage_key)\n\nshort_term_yield &lt;- short_term_yield_data$data\n\ncolnames(short_term_yield) &lt;- c(\n  \"date\",\n  \"percent_value\"\n)\n\nshort_term_yield &lt;- transform(short_term_yield,\n  date = as.Date(date),\n  percent_value = as.numeric(percent_value)\n)\n\n\nFor wage growth, I will be using the median weekly earnings in America. The specific series from FRED is LES1252881600Q.\n\n\nCode\n# FRED API documentation isn't as direct as Alpha Vantage's\n\n# Need to search specifically for data of interest to find what we want. This can be done with the API or on the website itself.\n\n# Wage growth\n\nsearch_fred_series &lt;- function(search_text, api_key) {\n  response &lt;- request(\"https://api.stlouisfed.org/fred/series/search\") |&gt;\n    req_url_query(\n      api_key = api_key,\n      file_type = \"json\", # Response in JSON format\n      search_text = search_text # keyword to search\n    ) |&gt;\n    req_perform()\n\n  # Parse the JSON response\n  data &lt;- resp_body_json(response, simplifyVector = TRUE)\n\n  # Extract and return the relevant series data\n  series_data &lt;- data$seriess\n  return(series_data)\n}\n\n# Search for wage growth-related series using \"wage growth\"\nwage_series &lt;- search_fred_series(\"wage growth\", FRED_key)\n\n# View the first few results\nhead(wage_series[, c(\"id\", \"title\", \"frequency\", \"units\")])\n# going with FRBATLWGTUMHWGO after looking into it more on FRED site\n\n# function that imports FRED data with series_id\nfetch_fred_data &lt;- function(series_id, api_key) {\n  response &lt;- request(\"https://api.stlouisfed.org/fred/series/observations\") |&gt;\n    req_url_query(\n      series_id = series_id, # Use the provided series_id\n      api_key = api_key, # Use the provided API key\n      file_type = \"json\" # Desired file type (json in this case)\n    ) |&gt;\n    req_perform()\n\n  # Parse the JSON response\n  data &lt;- response |&gt;\n    resp_body_json()\n\n  # Extract the observations data from the response\n  observations &lt;- data$observations\n\n  # Convert the data into a data frame for easier manipulation\n  df &lt;- data.frame(\n    date = sapply(observations, function(x) x$date),\n    value = sapply(observations, function(x) as.numeric(x$value))\n  )\n\n  # Return the data frame\n  return(df)\n}\n\n# Call the function with a specific series_id FRBATLWGTUMHWGO: Median weekly earnings quarterly\nwage_growth &lt;- fetch_fred_data(\"FRBATLWGTUMHWGO\", FRED_key)\n\ncolnames(wage_growth) &lt;- c(\n  \"date\",\n  \"wage_percent_change_from_a_year_ago\"\n)\n\nwage_growth &lt;- transform(wage_growth,\n  date = as.Date(date)\n)\n\n\nTo track bond market total returns, the GS10 (Market Yield on 10 year U.S. Treasury Securities) will be used.\n\n\nCode\n# bond market total returns, will use GS10 as it is a good indicator in the bond market overall\n\nbond_returns &lt;- fetch_fred_data(\"GS10\", FRED_key)\n\ncolnames(bond_returns) &lt;- c(\n  \"date\",\n  \"percent_value\"\n)\n\nbond_returns &lt;- transform(bond_returns,\n  date = as.Date(date)\n)\n\n\nNow that all the necessary data series have been imported, the next step is to combine everything into a single data frame to analyze. Unfortunately, all the data series do not use the same measurements of time and units of measurement. For example the SPY series is on a daily basis with multiple measurements of the share price namely in USD. The wage growth series operates under a quarterly time basis, CPI is measured as an index and so on. The following code will take into consideration all these circumstances and join them together as best as possible. The combined data will be aggregated over a monthly basis over 20 years of available data. The date range will be from October 2004 until October 2024.\n\n\nCode\n# lets bring these together all into one data frame, monthly should be fine since we're doing a long run analysis over 20 years and will bootstrap\n# defining 20 year range to be 10/1/2004 to 10/1/2024\n\n# adjust SPY to monthly and return per share\nspy_monthly &lt;- spy |&gt;\n  filter(date &gt; as.Date(\"2004-09-30\") & date &lt; as.Date(\"2024-10-02\")) |&gt;\n  group_by(date = floor_date(date, \"month\")) |&gt;\n  summarise(spy_average_close = round(mean(close), 2), .groups = 'drop') |&gt;\n  mutate(US_Equity_Total_Returns_USD = 100 * (spy_average_close - lag(spy_average_close)) / lag(spy_average_close)) |&gt;\n  mutate(US_Equity_Total_Returns_USD = round(ifelse(is.na(US_Equity_Total_Returns_USD), 0, US_Equity_Total_Returns_USD), 2)) # Rounding after replacing NA\nhead(spy_monthly)\n\n# adjust FDIVX to monthly and return per share\nFDIVX_monthly &lt;- FDIVX |&gt;\n  filter(date &gt; as.Date(\"2004-09-30\") & date &lt; as.Date(\"2024-10-02\")) |&gt;\n  group_by(date = floor_date(date, \"month\")) |&gt;\n  summarise(FDIVX_average_close = round(mean(close), 2), .groups = 'drop') |&gt;\n  mutate(International_Equity_Total_Returns_USD = 100 * (FDIVX_average_close - lag(FDIVX_average_close)) / lag(FDIVX_average_close)) |&gt;\n  mutate(International_Equity_Total_Returns_USD = round(ifelse(is.na(International_Equity_Total_Returns_USD), 0, International_Equity_Total_Returns_USD), 2)) # Rounding after replacing NA\nhead(FDIVX_monthly)\n\n# adjust inflation to join, should in theory be an average since its a monthly value & convert to a percentage\ninflation_monthly &lt;- inflation |&gt;\n  filter(date &gt; as.Date(\"2004-09-30\") & date &lt; as.Date(\"2024-10-02\")) |&gt;\n  rename(inflation_index_value = index_value) |&gt;\n  mutate(inflation_percent_change = round((inflation_index_value / lag(inflation_index_value) - 1) * 100, 2))\n\n# adjust short term yield to join, should in theory be an average since its a monthly value\nshort_term_yield_monthly &lt;- short_term_yield |&gt;\n  filter(date &gt; as.Date(\"2004-09-30\") & date &lt; as.Date(\"2024-10-02\")) |&gt;\n  rename(short_term_yield_percent_value = percent_value)\n\n# adjust wage growth \nwage_growth_monthly &lt;- wage_growth |&gt;\n  filter(date &gt; as.Date(\"2004-09-30\") & date &lt; as.Date(\"2024-10-02\"))\n\n# adjust bond returns, should be an average in theory\nbond_returns_monthly &lt;- bond_returns |&gt;\n  filter(date &gt; as.Date(\"2004-09-30\") & date &lt; as.Date(\"2024-10-02\")) |&gt;\n  rename(bond_return_percent_value = percent_value)\n\n# join all into one df\nhistorical_financial_series &lt;- spy_monthly |&gt;\n  left_join(FDIVX_monthly, by = \"date\") |&gt;\n  left_join(inflation_monthly, by = \"date\") |&gt;\n  left_join(short_term_yield_monthly, by = \"date\") |&gt;\n  left_join(wage_growth_monthly, by = \"date\") |&gt;\n  left_join(bond_returns_monthly, by = \"date\") |&gt;\n  mutate(spy_return_percent = round(US_Equity_Total_Returns_USD / lag(spy_average_close),4),\n         spy_return_percent = replace(spy_return_percent, is.na(spy_return_percent), 0)) |&gt;\n  mutate(FDIVX_return_percent = round(International_Equity_Total_Returns_USD / lag(FDIVX_average_close),4),\n         FDIVX_return_percent = replace(FDIVX_return_percent, is.na(FDIVX_return_percent), 0))\n\n\nHere is a sample table with randomly selected data points of what the combined data frame looks like.\n\n\nCode\nsample_n(historical_financial_series, 10) |&gt; \n  rename(\n    Date = date,\n    `SPY Value USD` = spy_average_close,\n    `US Equity Return USD Per Share` = US_Equity_Total_Returns_USD,\n    `US Equity Return % Per Share` = spy_return_percent,\n    `FDIVX Value USD` = FDIVX_average_close,\n    `International Equity Return USD Per Share` = International_Equity_Total_Returns_USD,\n    `International Equity Return % Per Share` = FDIVX_return_percent,\n    `Inflation Index Value` = inflation_index_value,\n    `Inflation Change %` = inflation_percent_change,\n    `Short Term Yield Return %` = short_term_yield_percent_value,\n    `Wage Growth % Change From Last Year` = wage_percent_change_from_a_year_ago,\n    `Bond Return %` = bond_return_percent_value\n  ) |&gt; \n  DT::datatable(caption = \"Sample Overview of Historical Financial Data\")\n\n\n\n\n\n\n\n\nInvestigation and Visualization of Input Data\nNow that the data is ready to be analyzed, some exploratory analysis can help us gain some insights into the different data series.\n\nLong Run Monthly Averages\nThe first insight to uncover is the long run monthly averages of each data series.\n\n\nCode\nlong_run_monthly_averages &lt;- historical_financial_series |&gt;\n  summarize(\n    avg_us_equity_return = 100*round(mean(spy_return_percent, na.rm = TRUE),4),\n    avg_international_equity_return = 100*round(mean(FDIVX_return_percent, na.rm = TRUE) ,4),\n    avg_bond_return = round(mean(bond_return_percent_value),2), # a percentage\n    avg_short_term_yield_return = round(mean(short_term_yield_percent_value),2), # a percentage\n    avg_inflation_change = round(abs(mean(inflation_percent_change, na.rm = TRUE)),2), # a percentage\n    avg_median_wage_change = round(mean(wage_percent_change_from_a_year_ago, na.rm = TRUE),2) # a percentage\n  )\n\n# creating new df for renaming purposes for data table display only, keeping original since it'll be easier to type the variables out later for other analysis\nlong_run_monthly_averages_dt &lt;- long_run_monthly_averages |&gt;\n  rename(\n    `Average US Equity Return (%)` = avg_us_equity_return,\n    `Average International Equity Return (%)` = avg_international_equity_return,\n    `Average Bond Return (%)` = avg_bond_return,\n    `Average Short Term Yield Return (%)` = avg_short_term_yield_return,\n    `Average Inflation Change (%)` = avg_inflation_change,\n    `Average Median Wage Change From a Year Ago(%)` = avg_median_wage_change,\n  )\n\ndatatable(long_run_monthly_averages_dt, caption = \"Long Run Monthly Averages\")\n\n\n\n\n\n\n\n\nCorrelation Factors\nNext, we examine the correlation factors among the different data series.\n\n\nCode\n# correlations\ncorrelation_data &lt;- historical_financial_series |&gt;\n  select(-date, -inflation_percent_change, -US_Equity_Total_Returns_USD, -International_Equity_Total_Returns_USD, -spy_return_percent, -FDIVX_return_percent) |&gt;\n  rename(`SPY Share Value` = spy_average_close,\n         `FDIVX Share Value` = FDIVX_average_close,\n         `Inflation Index` = inflation_index_value,\n         `Short Term Yield` = short_term_yield_percent_value,\n         `Wage Growth` = wage_percent_change_from_a_year_ago,\n         `Bond Return` = bond_return_percent_value)\n\ncorrelation_matrix &lt;- round(cor(correlation_data),2) # using statistical correlation function\n\n# convert matrix to a long-format data frame for plotting\ncorrelation_df &lt;- as.data.frame(as.table(correlation_matrix))\n\n# heat map with ggplot2\nggplot(correlation_df, aes(Var1, Var2, fill = Freq)) +\n  geom_tile(color = \"white\") +  # Tile borders\n  scale_fill_gradient2(low = \"red\", mid = \"white\", high = \"green\", midpoint = 0) +\n  labs(x = \"\", y = \"\", fill = \"Correlation\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nWe can see visually in the heat map how some factors are high correlated with each other while some have no correlation at all. Bond returns only have a significant correlation with the short term yield return. In theory this makes sense since they are both considered safe investment vehicles with low volatility. The value of SPY is highly correlated with the international market and inflation index, giving us insight into how the economy interacts with each other. Wage growth has some correlation with the other factors, however this correlation is relatively small in terms of meaningfulness. It seems that historically wage growth is not progressing as far as it needs to be compared to other financial factors.\n\n\nVariances\n\n\nCode\n# variances\n# probably makes sense to conduct variance analysis on percentages/returns only\n`US Equity Total Returns Variance` &lt;- round(var(historical_financial_series$spy_return_percent),4)\n`International Equity Total Returns Variance` &lt;- round(var(historical_financial_series$FDIVX_return_percent),4)\n`Inflation Variance` &lt;- round(var(historical_financial_series$inflation_percent_change, na.rm = TRUE),2)\n`Wage Growth Variance` &lt;- round(var(historical_financial_series$wage_percent_change_from_a_year_ago, na.rm = TRUE),2)\n`Short Term Yield Variance` &lt;- round(var(historical_financial_series$short_term_yield_percent_value),2)\n`Bond Return Variance` &lt;- round(var(historical_financial_series$bond_return_percent_value),2)\n\nseries_names &lt;- c(\"US Equity Total Returns Variance\", \n                    \"International Equity Total Returns Variance\", \n                    \"Inflation Variance\", \n                    \"Wage Growth Variance\", \n                    \"Short Term Yield Variance\", \n                    \"Bond Return Variance\")\n\n# combine into a data frame\nvariance_data &lt;- data.frame(Series = series_names,\n                            Variance = c(\n                              `US Equity Total Returns Variance`,\n                              `International Equity Total Returns Variance`,\n                              `Inflation Variance`,\n                              `Wage Growth Variance`,\n                              `Short Term Yield Variance`,\n                              `Bond Return Variance`\n                            ))\n\ndatatable(variance_data, caption = \"Data Series Variance\")\n\n\n\n\n\n\nInsert commentary about variance. I might want to change the variance of the first two to the percent return to align with the other data points.\n\n\n\nHistorical Comparison of TRS and ORP\nThe next step is to compare the values of the TRS and OPR plans for the first month of retirement. To do so, some assumptions will need to be made about this employee being examined. It will be assumed the the employee joined CUNY on the first month of the data set and retired on the last month of the data set. Their starting salary will be set at $60,000 and they began their tenure at age 42.\nFirst we will examine the TRS plan:\n\n\nCode\n# Task 5: Historical Comparison\n\n# set starting salary\nstarting_salary &lt;- 60000\n\n# define tenure of job before retirement (start of data to end of data)\nyears_worked &lt;- floor(as.numeric(difftime(max(historical_financial_series$date), min(historical_financial_series$date)))/365)\n\n# define salary earned in entire tenure, based on the long run monthly average of wage growth of 3.51%\nsalary_growth_rate &lt;- 0.0351\ntenured_salary &lt;- starting_salary * (1 + salary_growth_rate)^(0:years_worked)\n\n\n# TRS Plan\n# assuming the employee gets the full benefit of (35% + 2%*N) * FAS\n\n# define FAS as the average of the last 3 years of salary before retirement\nFAS = mean(tail(tenured_salary,3))\n\n# the annual retirement benefit\nannual_retirement_benefit = (0.35 + 0.02*years_worked)*FAS\n\n# inflation adjustment is made to the annual retirement benefit annually, this will be taken into consideration later in the analysis\n\n# first month's retirement income\nTRS_first_month_value = round(annual_retirement_benefit/12,0) # round to the nearest dollar\n\nprint(paste(\"The estimated benefit during the first month of retirement with the TRS plan is $\", TRS_first_month_value))\n\n\n[1] \"The estimated benefit during the first month of retirement with the TRS plan is $ 7225\"\n\n\nNext we can compare this to the OPR plan:\n\n\nCode\n# OPR Plan\n# making assumptions here, lets say the employee is hired at age 42 and retires at 62\n\n# initialize the retirement data frame\nretirement_account &lt;- data.frame(\n  period = 1:nrow(historical_financial_series), # of months contributions are made  \n  \n  # initialize columns for each asset return\n  allocation_us = rep(0, nrow(historical_financial_series)), \n  allocation_international = rep(0, nrow(historical_financial_series)),\n  allocation_bonds = rep(0, nrow(historical_financial_series)),\n  allocation_short_term = rep(0, nrow(historical_financial_series)),\n  \n  # define the assets returns based on historical data, dividing to get decimal value\n  us_returns = historical_financial_series$spy_return_percent/100, \n  international_returns = historical_financial_series$FDIVX_return_percent/100,\n  bonds_returns = historical_financial_series$bond_return_percent_value/100,\n  short_term_returns = historical_financial_series$short_term_yield_percent_value/100,\n  \n  # initialize columns for contributions made\n  contribution_employee = rep(0, nrow(historical_financial_series)), \n  contribution_employer = rep(0, nrow(historical_financial_series))\n)\n\n# populate contributions and allocations as before\nfor (month in 1:nrow(retirement_account)) {\n  year &lt;- ceiling(month / 12) # round to the closest year to define number of years worked\n  age &lt;- 42 + (year - 1) # assuming employee starts at age 42\n  annual_salary &lt;- tenured_salary[year] # grab the salary dependent on work year\n\n  # calculate contributions based on salary bracket\n  if (annual_salary &lt;= 45000) {\n    retirement_account$contribution_employee[month] &lt;- annual_salary * 0.03 / 12\n  } else if (annual_salary &lt;= 55000) {\n    retirement_account$contribution_employee[month] &lt;- annual_salary * 0.035 / 12\n  } else if (annual_salary &lt;= 75000) {\n    retirement_account$contribution_employee[month] &lt;- annual_salary * 0.045 / 12\n  } else if (annual_salary &lt;= 100000) {\n    retirement_account$contribution_employee[month] &lt;- annual_salary * 0.0575 / 12\n  } else {\n    retirement_account$contribution_employee[month] &lt;- annual_salary * 0.06 / 12\n  }\n  retirement_account$contribution_employer[month] &lt;- if (year &lt;= 7) annual_salary * 0.08 / 12 else annual_salary * 0.10 / 12\n\n  # define allocations based on age\n  if (age &lt;= 49) {\n    retirement_account$allocation_us[month] &lt;- 0.54\n    retirement_account$allocation_international[month] &lt;- 0.36\n    retirement_account$allocation_bonds[month] &lt;- 0.10\n    retirement_account$allocation_short_term[month] &lt;- 0.00\n  } else if (age &lt;= 59) {\n    retirement_account$allocation_us[month] &lt;- 0.47\n    retirement_account$allocation_international[month] &lt;- 0.32\n    retirement_account$allocation_bonds[month] &lt;- 0.21\n    retirement_account$allocation_short_term[month] &lt;- 0.00\n  } else if (age &lt;= 74) {\n    retirement_account$allocation_us[month] &lt;- 0.34\n    retirement_account$allocation_international[month] &lt;- 0.23\n    retirement_account$allocation_bonds[month] &lt;- 0.43\n    retirement_account$allocation_short_term[month] &lt;- 0.00\n  } else {\n    retirement_account$allocation_us[month] &lt;- 0.19\n    retirement_account$allocation_international[month] &lt;- 0.13\n    retirement_account$allocation_bonds[month] &lt;- 0.62\n    retirement_account$allocation_short_term[month] &lt;- 0.06\n  }\n}\n\n# calculate future value using compound interest\nretirement_account_value &lt;- retirement_account |&gt;\n  mutate(\n    # total contribution per month\n    contribution_total = contribution_employee + contribution_employer,\n\n    # net total return factors for each asset with compounding\n    net_return_us = order_by(desc(period), cumprod(1 + lead(us_returns, default = 0))),\n    net_return_international = order_by(desc(period), cumprod(1 + lead(international_returns, default = 0))),\n    net_return_bonds = order_by(desc(period), cumprod(1 + lead(bonds_returns, default = 0))),\n    net_return_short_term = order_by(desc(period), cumprod(1 + lead(short_term_returns, default = 0))),\n\n    # future value of contributions per asset class\n    fv_us = contribution_total * allocation_us * net_return_us,\n    fv_international = contribution_total * allocation_international * net_return_international,\n    fv_bonds = contribution_total * allocation_bonds * net_return_bonds,\n    fv_short_term = contribution_total * allocation_short_term * net_return_short_term\n  ) |&gt;\n  summarize(\n    future_value = sum(fv_us + fv_international + fv_bonds + fv_short_term) # total future value of the retirement account\n  )\n\nprint(paste(\"The estimated retirement account value at the first month of retirement with the OPR plan is $\", round(retirement_account_value,0)))\n\n\n[1] \"The estimated retirement account value at the first month of retirement with the OPR plan is $ 2061342\"\n\n\nWe can see that the OPR plan yields more value in the first month of retirement. However, we need to analyze the yields even further into the future to get a better understanding about which plan may be more suitable for a potential employee. Keep in mind that the final value of the OPR plan is how much that person has for the remainder of their life and can be exhausted. The money can be used however they like in terms of withdrawal, and the value will continue to grow annually dependent on the equity allocations. On the other hand, the TRS plan guarantees that same income value monthly (adjusted for inflation annually) until death.\n\n\nLong-Term Average Analysis\nNow we will modify our analysis from above to look at the plans in a different timeline, from retirement until death. Life expectancy unpredictable and is a result of multiple factors. For this analysis, we’ll arbitrarily define the life expectancy to be 80 years old.\nLooking at the TRS plan:\n\n\nCode\n# Task 6: Fixed-Rate Analysis\n\n# TRS inflation adjusted for the next 18 years\n# The benefit is increased annually by 50% of the CPI, rounded up to the nearest tenth of a percent\nCPI_benefit = round((long_run_monthly_averages$avg_inflation_change*12)/2,1)/100\nannual_retirement_benefit_inflation_adjusted = round(annual_retirement_benefit * (1 + CPI_benefit)^(0:17), 0) # (0:17) creates a vector for the next 18 years of inflation adjusted income\n\n# monthly retirement income in each year\nmonthly_annual_retirement_benefit_inflation_adjusted = round(annual_retirement_benefit_inflation_adjusted/12,0)\n\n# average monthly income post retirement\nTRS_avg_monthly_income = round(mean(monthly_annual_retirement_benefit_inflation_adjusted),0)\n\n# convert the numeric vector to a data frame with ages\nannual_retirement_benefit_inflation_adjusted &lt;- data.frame(\n  age = 63:80, # Create a column for ages from 63 to 80\n  annual_benefit = annual_retirement_benefit_inflation_adjusted,\n  monthly_benefit = round(annual_retirement_benefit_inflation_adjusted/12,0)\n)\n\n# new variable for display purposes only\nannual_retirement_benefit_inflation_adjusted_dt &lt;- annual_retirement_benefit_inflation_adjusted |&gt;\n  rename(\n    Age = age,\n    `Annual Benefit (USD)` = annual_benefit,\n    `Monthly Benefit (USD)` = monthly_benefit\n  )\n\ndatatable(annual_retirement_benefit_inflation_adjusted_dt, caption = \"TRS Benefit During Retirement\")\n\n\n\n\n\n\nLooking at the OPR plan:\n\n\nCode\n#### reloading data from last chunk in this new chunk to keep variables going\n\n# making assumptions here, lets say the employee is hired at age 42 and retires at 62\n\n# initialize the retirement data frame\nretirement_account &lt;- data.frame(\n  period = 1:nrow(historical_financial_series), # of months contributions are made  \n  \n  # initialize columns for each asset return\n  allocation_us = rep(0, nrow(historical_financial_series)), \n  allocation_international = rep(0, nrow(historical_financial_series)),\n  allocation_bonds = rep(0, nrow(historical_financial_series)),\n  allocation_short_term = rep(0, nrow(historical_financial_series)),\n  \n  # define the assets returns based on historical data, dividing to get decimal value\n  us_returns = historical_financial_series$spy_return_percent/100, \n  international_returns = historical_financial_series$FDIVX_return_percent/100,\n  bonds_returns = historical_financial_series$bond_return_percent_value/100,\n  short_term_returns = historical_financial_series$short_term_yield_percent_value/100,\n  \n  # initialize columns for contributions made\n  contribution_employee = rep(0, nrow(historical_financial_series)), \n  contribution_employer = rep(0, nrow(historical_financial_series))\n)\n\n# populate contributions and allocations as before\nfor (month in 1:nrow(retirement_account)) {\n  year &lt;- ceiling(month / 12) # round to the closest year to define number of years worked\n  age &lt;- 42 + (year - 1) # assuming employee starts at age 42\n  annual_salary &lt;- tenured_salary[year] # grab the salary dependent on work year\n\n  # calculate contributions based on salary bracket\n  if (annual_salary &lt;= 45000) {\n    retirement_account$contribution_employee[month] &lt;- annual_salary * 0.03 / 12\n  } else if (annual_salary &lt;= 55000) {\n    retirement_account$contribution_employee[month] &lt;- annual_salary * 0.035 / 12\n  } else if (annual_salary &lt;= 75000) {\n    retirement_account$contribution_employee[month] &lt;- annual_salary * 0.045 / 12\n  } else if (annual_salary &lt;= 100000) {\n    retirement_account$contribution_employee[month] &lt;- annual_salary * 0.0575 / 12\n  } else {\n    retirement_account$contribution_employee[month] &lt;- annual_salary * 0.06 / 12\n  }\n  retirement_account$contribution_employer[month] &lt;- if (year &lt;= 7) annual_salary * 0.08 / 12 else annual_salary * 0.10 / 12\n\n  # define allocations based on age\n  if (age &lt;= 49) {\n    retirement_account$allocation_us[month] &lt;- 0.54\n    retirement_account$allocation_international[month] &lt;- 0.36\n    retirement_account$allocation_bonds[month] &lt;- 0.10\n    retirement_account$allocation_short_term[month] &lt;- 0.00\n  } else if (age &lt;= 59) {\n    retirement_account$allocation_us[month] &lt;- 0.47\n    retirement_account$allocation_international[month] &lt;- 0.32\n    retirement_account$allocation_bonds[month] &lt;- 0.21\n    retirement_account$allocation_short_term[month] &lt;- 0.00\n  } else if (age &lt;= 74) {\n    retirement_account$allocation_us[month] &lt;- 0.34\n    retirement_account$allocation_international[month] &lt;- 0.23\n    retirement_account$allocation_bonds[month] &lt;- 0.43\n    retirement_account$allocation_short_term[month] &lt;- 0.00\n  } else {\n    retirement_account$allocation_us[month] &lt;- 0.19\n    retirement_account$allocation_international[month] &lt;- 0.13\n    retirement_account$allocation_bonds[month] &lt;- 0.62\n    retirement_account$allocation_short_term[month] &lt;- 0.06\n  }\n}\n\n# calculate future value using compound interest\nretirement_account_value &lt;- retirement_account |&gt;\n  mutate(\n    # total contribution per month\n    contribution_total = contribution_employee + contribution_employer,\n\n    # net total return factors for each asset with compounding\n    net_return_us = order_by(desc(period), cumprod(1 + lead(us_returns, default = 0))),\n    net_return_international = order_by(desc(period), cumprod(1 + lead(international_returns, default = 0))),\n    net_return_bonds = order_by(desc(period), cumprod(1 + lead(bonds_returns, default = 0))),\n    net_return_short_term = order_by(desc(period), cumprod(1 + lead(short_term_returns, default = 0))),\n\n    # future value of contributions per asset class\n    fv_us = contribution_total * allocation_us * net_return_us,\n    fv_international = contribution_total * allocation_international * net_return_international,\n    fv_bonds = contribution_total * allocation_bonds * net_return_bonds,\n    fv_short_term = contribution_total * allocation_short_term * net_return_short_term\n  ) |&gt;\n  summarize(\n    account_value = sum(fv_us + fv_international + fv_bonds + fv_short_term) # total future value of the retirement account\n  )\n\n\n####\n\n\n# OPR for the next 18 years\n\n# Initial account value at retirement\npost_retirement_account_value &lt;- retirement_account_value\n\n# Define the withdrawal rate (4%)\nwithdrawal_rate &lt;- 0.04\n\n# Initialize an empty data frame to store results\npost_retirement_account_df &lt;- data.frame(age = integer(), account_value = numeric())\n\n# Initialize previous_account_value to store the starting value\nprevious_account_value &lt;- post_retirement_account_value\n\n# Create a loop for each year of retirement (from age 62 to 80)\nfor (age in 62:80) {\n  \n  # Calculate the annual withdrawal based on the previous year's account value\n  annual_withdrawal &lt;- previous_account_value * withdrawal_rate\n  \n  # Subtract the withdrawal before applying growth\n  post_retirement_account_value &lt;- previous_account_value - annual_withdrawal\n  \n  # Define allocations based on age\n  if (age &lt;= 74) {\n    post_allocation_us &lt;- 0.34\n    post_allocation_international &lt;- 0.23\n    post_allocation_bonds &lt;- 0.43\n    post_allocation_short_term &lt;- 0.00\n  } else {\n    post_allocation_us &lt;- 0.19\n    post_allocation_international &lt;- 0.13\n    post_allocation_bonds &lt;- 0.62\n    post_allocation_short_term &lt;- 0.06\n  }\n  \n  # Define the asset returns (adjusted for decimal values)\n  us_returns &lt;- historical_financial_series$spy_return_percent / 100\n  international_returns &lt;- historical_financial_series$FDIVX_return_percent / 100\n  bonds_returns &lt;- historical_financial_series$bond_return_percent_value / 100\n  short_term_returns &lt;- historical_financial_series$short_term_yield_percent_value / 100\n  \n  # Adjust for monthly growth using returns (assuming monthly data for growth)\n  post_net_return_us &lt;- cumprod(1 + lead(us_returns, default = 0))  \n  post_net_return_international &lt;- cumprod(1 + lead(international_returns, default = 0))  \n  post_net_return_bonds &lt;- cumprod(1 + lead(bonds_returns, default = 0))  \n  post_net_return_short_term &lt;- cumprod(1 + lead(short_term_returns, default = 0))  \n  \n  # Calculate the future value per asset class for the current year\n  post_fv_us &lt;- post_retirement_account_value * post_allocation_us * post_net_return_us\n  post_fv_international &lt;- post_retirement_account_value * post_allocation_international * post_net_return_international\n  post_fv_bonds &lt;- post_retirement_account_value * post_allocation_bonds * post_net_return_bonds\n  post_fv_short_term &lt;- post_retirement_account_value * post_allocation_short_term * post_net_return_short_term\n  \n  # Calculate the total value after growth\n  post_retirement_account_value &lt;- post_fv_us + post_fv_international + post_fv_bonds + post_fv_short_term\n  \n  # Store the account value for the current year\n  post_retirement_account_df &lt;- rbind(post_retirement_account_df, data.frame(age = age, account_value = round(post_retirement_account_value,0)))\n  \n  # Update previous_account_value for the next year\n  previous_account_value &lt;- post_retirement_account_value\n}\n\n# add columns for annual and monthly incomes at each age\npost_retirement_account_df &lt;- post_retirement_account_df |&gt;\n  mutate(annual_income = round(account_value*0.04,0),\n         monthly_income = round(annual_income/12,0))\n\n# average monthly income post retirement\nOPR_avg_monthly_income = round(mean(post_retirement_account_df$monthly_income),0)\n\n# adjusted for display purposes only\npost_retirement_account_df_table &lt;- post_retirement_account_df |&gt;\n  filter(age != 62) |&gt;\n  rename(\n    Age = age,\n    `Retirement Account Value (USD)` = account_value,\n    `Annual Withdrawal (USD)` = annual_income,\n    `Monthly Income (USD)` = monthly_income\n  )\n\ndatatable(post_retirement_account_df_table, caption = \"ORP Income During Retirement\")\n\n\n\n\n\n\nWe see that each plan offers a different amount of returns during the retirement period. From a quick glance, the TRS plan provides a higher income in retirement compared to the ORP plan. In fact, the TRS income increases over time while the ORP income decreases as the funds are exhausted from withdrawing.\n\n\nCode\n# plot the comparison\nplan_comparison &lt;- annual_retirement_benefit_inflation_adjusted |&gt;\n  left_join(\n    post_retirement_account_df,\n    join_by(age == age)\n  ) |&gt;\n  select(age, monthly_benefit, monthly_income)\n\nggplot(plan_comparison, aes(x = age)) +\n  geom_line(aes(y = monthly_benefit, color = \"TRS Plan\")) +\n  geom_line(aes(y = monthly_income, color = \"ORP Plan\")) +\n  labs(title = \"Comparison of TRS & ORP Plans by Age\",\n       x = \"Age\",\n       y = \"Monthly Income\",\n       color = \"Retirement Plan\") +\n  scale_color_manual(values = c(\"blue\", \"red\")) +  # Customize colors\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nBased on the data we’ve acquired we can uncover some insights:\nWhether the employee runs out of funds before death and/or has funds to leave to heirs (ORP only)\nAssuming that the life expectancy is 80 years old, there is left over money in the retirement account to be inherited. This is assuming a constant 4% annual withdrawal rate until death. If the person lives longer or withdraws more this may change.\n\n\nCode\nORP_funds &lt;- post_retirement_account_df_table |&gt;\n  filter(Age == 80) |&gt;\n  select(Age, `Retirement Account Value (USD)`)\n\ndatatable(ORP_funds, caption = \"Funds Left at Death\")\n\n\n\n\n\n\nAverage monthly income (TRS vs ORP)\nOn average the TRS plan provides a higher average monthly income. Again keep in mind that this is based on factors we’ve established beforehand: starting salary, starting employment age, and number of years worked before retirement. We’ll take a look at how the outcomes could be different when the parameters change later.\n\n\nCode\nprint(paste(\"The average monthly income with the TRS plan is\", TRS_avg_monthly_income, \"dollars.\"))\n\n\n[1] \"The average monthly income with the TRS plan is 8082 dollars.\"\n\n\nCode\nprint(paste(\"The average monthly income with the ORP plan is\", OPR_avg_monthly_income, \"dollars.\"))\n\n\n[1] \"The average monthly income with the ORP plan is 5586 dollars.\"\n\n\nMaximum and minimum gap in monthly income between TRS and ORP\n\n\nCode\nincome_gap &lt;- annual_retirement_benefit_inflation_adjusted |&gt;\n  left_join(\n    post_retirement_account_df,\n    join_by(age == age)\n  ) |&gt;\n  mutate(income_difference = monthly_benefit - monthly_income) |&gt;\n  select(age, income_difference) |&gt;\n  filter(income_difference == max(income_difference) | income_difference == min(income_difference))\n\n# extract the two ages for plotting use\nages_to_display &lt;- unique(income_gap$age)\n\n# display the two specific ages on the plot\nincome_gap &lt;- income_gap |&gt;\n  mutate(age = factor(age, levels = ages_to_display))\n\nggplot(income_gap, aes(x = age, y = income_difference)) + \n  geom_bar(stat = \"identity\", fill = \"cadetblue2\" ) +\n  labs(title = \"Maximum & Minimum Income Difference by Age\", x = \"Age\", y = \"Income Difference\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe smallest income gap happens during the first year of retirement with a difference of $656. The largest income gap occurs at the end of the life expectancy with a difference of $4252.\n\n\nBootstrap (Monte Carlo) Comparison\nBootstrapping data or performing a Monte Carlo analysis allows us to dive into further uncertainty in our data. By simulating a wide range of possible outcomes based on a model’s input variables, this method provides a comprehensive view of potential results, allowing analysts to evaluate the likelihood of various scenarios. It helps quantify risks, enabling decision-makers to make informed choices by assessing the probabilities of success or failure associated with different strategies. We’ll take a look at bootstrapping our historical data to get a better understanding of our results from beforehand.\nBy introducing a Monte Carlo analysis, we can generate more potential scenarios for the TRS and ORP plan results. The above analysis was strictly based on on instance in history and not representative of all outcomes possible. The following bootstrap comparison will show us that there are other potential outcomes in each plan.\nFirst we can take a look at the chances of exhausting the ORP plan before death. This is assuming that the life expectancy is 80 and the withdrawal rate is still 4% annually.\n\n\nCode\n# Task 7: Monte Carlo Analysis\n\n# Calculate the probability of savings exhaustion\n\n# initialize parameters\nset.seed(123)\nn_bootstrap &lt;- 200\nsavings_exhausted &lt;- numeric(n_bootstrap) # set vector for any bootstrap accounts exhausted\npost_retirement_results &lt;- vector(\"list\", n_bootstrap) # combine all iterations into one list\n\n# Main loop for bootstrap samples\nfor (i in 1:n_bootstrap) {\n  # Bootstrap sample from historical financial series\n  bootstrap_sample &lt;- historical_financial_series |&gt; \n    slice_sample(n = nrow(historical_financial_series), replace = TRUE)\n  \n  # Initialize retirement data frame\n  retirement_account &lt;- data.frame(\n    period = 1:nrow(bootstrap_sample),\n    allocation_us = rep(0, nrow(bootstrap_sample)),\n    allocation_international = rep(0, nrow(bootstrap_sample)),\n    allocation_bonds = rep(0, nrow(bootstrap_sample)),\n    allocation_short_term = rep(0, nrow(bootstrap_sample)),\n    us_returns = bootstrap_sample$spy_return_percent / 100,\n    international_returns = bootstrap_sample$FDIVX_return_percent / 100,\n    bonds_returns = bootstrap_sample$bond_return_percent_value / 100,\n    short_term_returns = bootstrap_sample$short_term_yield_percent_value / 100,\n    contribution_employee = rep(0, nrow(bootstrap_sample)),\n    contribution_employer = rep(0, nrow(bootstrap_sample))\n  )\n  \n  \n  bootstrap_salary_growth_rate &lt;- mean(bootstrap_sample$wage_percent_change_from_a_year_ago/100)\n  bootstrap_tenured_salary &lt;- starting_salary * (1 + bootstrap_salary_growth_rate)^(0:years_worked)\n  \n  # Populate contributions and allocations based on age and salary\n  for (month in 1:nrow(retirement_account)) {\n    year &lt;- ceiling(month / 12)\n    age &lt;- 42 + (year - 1)\n    annual_salary &lt;- bootstrap_tenured_salary[year]\n    \n    # calculate contributions based on salary bracket\n    if (annual_salary &lt;= 45000) {\n      retirement_account$contribution_employee[month] &lt;- annual_salary * 0.03 / 12\n    } else if (annual_salary &lt;= 55000) {\n      retirement_account$contribution_employee[month] &lt;- annual_salary * 0.035 / 12\n    } else if (annual_salary &lt;= 75000) {\n      retirement_account$contribution_employee[month] &lt;- annual_salary * 0.045 / 12\n    } else if (annual_salary &lt;= 100000) {\n      retirement_account$contribution_employee[month] &lt;- annual_salary * 0.0575 / 12\n    } else {\n      retirement_account$contribution_employee[month] &lt;- annual_salary * 0.06 / 12\n    }\n    retirement_account$contribution_employer[month] &lt;- if (year &lt;= 7) annual_salary * 0.08 / 12 else annual_salary * 0.10 / 12\n    \n    # Allocations based on age\n    if (age &lt;= 49) {\n      retirement_account$allocation_us[month] &lt;- 0.54\n      retirement_account$allocation_international[month] &lt;- 0.36\n      retirement_account$allocation_bonds[month] &lt;- 0.10\n      retirement_account$allocation_short_term[month] &lt;- 0.00\n    } else if (age &lt;= 59) {\n      retirement_account$allocation_us[month] &lt;- 0.47\n      retirement_account$allocation_international[month] &lt;- 0.32\n      retirement_account$allocation_bonds[month] &lt;- 0.21\n      retirement_account$allocation_short_term[month] &lt;- 0.00\n    } else if (age &lt;= 74) {\n      retirement_account$allocation_us[month] &lt;- 0.34\n      retirement_account$allocation_international[month] &lt;- 0.23\n      retirement_account$allocation_bonds[month] &lt;- 0.43\n      retirement_account$allocation_short_term[month] &lt;- 0.00\n    } else {\n      retirement_account$allocation_us[month] &lt;- 0.19\n      retirement_account$allocation_international[month] &lt;- 0.13\n      retirement_account$allocation_bonds[month] &lt;- 0.62\n      retirement_account$allocation_short_term[month] &lt;- 0.06\n    }\n  }\n  \n  # calculate future value using compound interest\n  retirement_account_value &lt;- retirement_account |&gt;\n    mutate(\n      # total contribution per month\n      contribution_total = contribution_employee + contribution_employer,\n      \n      # net total return factors for each asset with compounding\n      net_return_us = order_by(desc(period), cumprod(1 + lead(us_returns, default = 0))),\n      net_return_international = order_by(desc(period), cumprod(1 + lead(international_returns, default = 0))),\n      net_return_bonds = order_by(desc(period), cumprod(1 + lead(bonds_returns, default = 0))),\n      net_return_short_term = order_by(desc(period), cumprod(1 + lead(short_term_returns, default = 0))),\n      \n      # future value of contributions per asset class\n      fv_us = contribution_total * allocation_us * net_return_us,\n      fv_international = contribution_total * allocation_international * net_return_international,\n      fv_bonds = contribution_total * allocation_bonds * net_return_bonds,\n      fv_short_term = contribution_total * allocation_short_term * net_return_short_term\n    ) |&gt;\n    summarize(\n      account_value = sum(fv_us + fv_international + fv_bonds + fv_short_term) # total future value of the retirement account\n    ) |&gt;\n    pull(account_value)\n  \n  # Post-retirement calculations for ages 62 to 80\n  post_retirement_account_df &lt;- data.frame(age = integer(), account_value = numeric())\n  withdrawal_rate &lt;- 0.07\n  previous_account_value &lt;- retirement_account_value\n  \n  # Loop through ages 62 to 80, applying withdrawals and growth\n  for (age in 62:80) {\n    # Calculate the withdrawal amount based on withdrawal rate\n    computed_withdrawal &lt;- previous_account_value * withdrawal_rate\n    \n    # Set the annual withdrawal to at least $60,000 for minimum living expenses\n    annual_withdrawal &lt;- max(computed_withdrawal, 60000)\n    \n    # Deduct withdrawal from account\n    post_retirement_account_value &lt;- previous_account_value - annual_withdrawal\n    \n    # Define allocations based on age\n    if (age &lt;= 74) {\n      post_allocation_us &lt;- 0.34\n      post_allocation_international &lt;- 0.23\n      post_allocation_bonds &lt;- 0.43\n      post_allocation_short_term &lt;- 0.00\n    } else {\n      post_allocation_us &lt;- 0.19\n      post_allocation_international &lt;- 0.13\n      post_allocation_bonds &lt;- 0.62\n      post_allocation_short_term &lt;- 0.06\n    }\n    \n    # Calculate future value of the remaining balance after applying returns for the year\n    # Apply returns based on age (mapping current year returns for the remaining account value)\n    post_fv_us &lt;- post_retirement_account_value * post_allocation_us * (1 + retirement_account$us_returns[age - 61])\n    post_fv_international &lt;- post_retirement_account_value * post_allocation_international * (1 + retirement_account$international_returns[age - 61])\n    post_fv_bonds &lt;- post_retirement_account_value * post_allocation_bonds * (1 + retirement_account$bonds_returns[age - 61])\n    post_fv_short_term &lt;- post_retirement_account_value * post_allocation_short_term * (1 + retirement_account$short_term_returns[age - 61])\n    \n    # Total value after applying returns to the remaining balance\n    post_retirement_account_value &lt;- post_fv_us + post_fv_international + post_fv_bonds + post_fv_short_term\n    \n    # Store the account value at the end of each year\n    post_retirement_account_df &lt;- rbind(post_retirement_account_df, data.frame(age = age, account_value = round(post_retirement_account_value, 0)))\n    \n    # Update the previous account value for the next loop iteration\n    previous_account_value &lt;- post_retirement_account_value\n  }\n  \n  # Add income columns for post-retirement\n  post_retirement_account_df &lt;- post_retirement_account_df |&gt; \n    mutate(annual_income = round(account_value * withdrawal_rate, 0),\n           monthly_income = round(annual_income / 12, 0))\n  \n  # Store the results\n  post_retirement_results[[i]] &lt;- post_retirement_account_df\n  \n  # Check if any savings were exhausted\n  if (any(post_retirement_account_df$account_value &lt;= 0)) {\n    savings_exhausted[i] &lt;- 1\n  }\n}\n\nprint(paste(\"The probability of exhausting the ORP plan before death is\", mean(savings_exhausted),\".\"))\n\n\n[1] \"The probability of exhausting the ORP plan before death is 0 .\"\n\n\nThe chances of exhausting the ORP funds in this simulation is 0. However, we should take a deeper look into the breakdown of the account value of the years to get a full understanding of how much is being exhausted each year. We’ll take a look at a sample bootstrapped output for the ORP plan. We can see that even with the bootstrapped data, there are still plenty of funds left over for inheritance.\n\n\nCode\nORP_bootstrap_example &lt;- post_retirement_results[[1]] |&gt; \n  rename(\n    Age = age,\n    `Account Value (USD)` = account_value,\n    `Annual Withdrawal (USD)` = annual_income,\n    `Monthly Income (USD)` = monthly_income\n  )\n\ndatatable(ORP_bootstrap_example, caption = \"Sample Bootstrapped ORP Output\")\n\n\n\n\n\n\nNext we examine the probability that an ORP employee has a higher monthly income in retirement than a TRS employee. Using the 200 bootstrapped history simulation, we compare the average monthly income in each instance to see how many times the ORP plan outperformed the TRS plan.\n\n\nCode\n# get TRS dataframe list from bootstrap history first to compare the ORP already calculated \nset.seed(123)\nn_bootstrap &lt;- 200\n\n# Initialize an empty list to store the results\nbootstrap_TRS_results &lt;- vector(\"list\", n_bootstrap)\n\nfor (i in 1:n_bootstrap) {\n  # Bootstrap sample from historical financial series\n  bootstrap_sample &lt;- historical_financial_series |&gt; \n    slice_sample(n = nrow(historical_financial_series), replace = TRUE)\n  \n  # Calculate growth rate and tenured salary\n  bootstrap_salary_growth_rate &lt;- mean(bootstrap_sample$wage_percent_change_from_a_year_ago) / 100\n  bootstrap_tenured_salary &lt;- starting_salary * (1 + bootstrap_salary_growth_rate)^(0:years_worked)\n  \n  # Calculate the Final Average Salary (FAS)\n  bootstrap_FAS &lt;- mean(tail(bootstrap_tenured_salary, 3))\n  \n  # Calculate the annual retirement benefit\n  bootstrap_annual_retirement_benefit &lt;- (0.35 + 0.02 * years_worked) * bootstrap_FAS\n  \n  # Calculate CPI benefit\n  bootstrap_CPI_benefit &lt;- abs(mean(bootstrap_sample$inflation_percent_change, na.rm = TRUE)) * 12 /100\n  bootstrap_annual_retirement_benefit_inflation_adjusted &lt;- round(bootstrap_annual_retirement_benefit * (1 + bootstrap_CPI_benefit)^(0:17), 0) # (0:17) creates a vector for the next 18 years of inflation adjusted income\n  \n  # Convert the numeric vector to a data frame with ages\n  bootstrap_annual_retirement_benefit_inflation_adjusted_df &lt;- data.frame(\n    age = 63:80, # Create a column for ages from 63 to 80\n    annual_benefit = bootstrap_annual_retirement_benefit_inflation_adjusted,\n    monthly_benefit = round(bootstrap_annual_retirement_benefit_inflation_adjusted / 12, 0)\n  )\n  \n  # Store the data frame in the list\n  bootstrap_TRS_results[[i]] &lt;- bootstrap_annual_retirement_benefit_inflation_adjusted_df\n}\n\n\n# mean monthly income of each TRS bootstrap history dataframe\nmean_TRS_monthly_benefits &lt;- sapply(bootstrap_TRS_results, function(df) mean(df$monthly_benefit, na.rm = TRUE))\n\n# mean monthly income of each ORP bootstrap history dataframe\nmean_ORP_monthly_income &lt;- sapply(post_retirement_results, function(df) mean(df$monthly_income, na.rm = TRUE))\n\n# compare the two plans\n# Calculate the number of times ORP monthly income is greater than TRS monthly benefits\ngreater_than_count &lt;- sum(mean_ORP_monthly_income &gt; mean_TRS_monthly_benefits)\n\n# Calculate the total number of comparisons (which should be the same length for both vectors)\ntotal_comparisons &lt;- length(mean_TRS_monthly_benefits)\n\n# Calculate the probability\nprobability_ORP_greater &lt;- (greater_than_count / total_comparisons)*100\n\n# Print the results\ncat(\"The probability that the ORP plan has a greater monthly income than TRS is:\", probability_ORP_greater, \"%.\")\n\n\nThe probability that the ORP plan has a greater monthly income than TRS is: 72.5 %.\n\n\nTo get a better idea of this calculated probability, we can look into the distribution of the mean income by age in each plan. The ORP plan has a wider distribution of potential monthly income where as the TRS is more concentrated with less variability.\n\n\nCode\naverage_monthly_plan_incomes &lt;- data.frame(\n  Value = c(mean_TRS_monthly_benefits, mean_ORP_monthly_income),\n  Group = c(rep(\"TRS\", length(mean_TRS_monthly_benefits)), rep(\"ORP\", length(mean_ORP_monthly_income)))\n)\n\nggplot(average_monthly_plan_incomes, aes(x = Group, y = Value)) +\n  geom_jitter(width = 0.2, height = 0, alpha = 0.5) +\n  labs(title = \"Comparing TRS vs. ORP Plan Results\", x = \"Plan Type\", y = \"Average Monthly Income (USD)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nLastly, we can compare the distribution of the average monthly income in each plan generated from the bootstrap history through a histogram. The TRS distribution closely follows a normal distribution curve, while the ORP distribution shows similarity to a bimodal distribution instead.\n\n\nCode\nggplot(data.frame(Income = mean_ORP_monthly_income), aes(x = Income)) +\n  geom_histogram(binwidth = 100, fill = \"darkseagreen3\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Histogram of Average Monthly Income (ORP)\",\n       subtitle = \"Generated From 200 Bootstrap Histories\",\n       x = \"Average Monthly Income (USD)\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(data.frame(Income = mean_TRS_monthly_benefits), aes(x = Income)) +\n  geom_histogram(binwidth = 100, fill = \"palevioletred\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Histogram of Average Monthly Income (TRS)\",\n       subtitle = \"Generated From 200 Bootstrap Histories\",\n       x = \"Average Monthly Income (USD)\",\n       y = \"Frequency\") +\n  theme_minimal()"
  },
  {
    "objectID": "mp04.html#data-driven-decision-recommendation",
    "href": "mp04.html#data-driven-decision-recommendation",
    "title": "Mini-Project #04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "Data-Driven Decision Recommendation",
    "text": "Data-Driven Decision Recommendation\nBased on the analysis, the ORP plan may be the better retirement option for an employee. This is not financial advice that should be taken by every potential employee. The specific circumstances for this employee lead us to choosing this plan. Instead we need to consider the personal factors of anyone deciding on what plan to use. That employee should consider their own age, starting salary, length of tenure, and their risk factor for investing.\nIf the employee is younger (20-30s) and is willing to risk their investments in the market, I would recommend taking the ORP plan. They’ll have more time for their investment to compound and grow, assuming they’ll retire after working more than 20 years. For those who are older, having a shorter tenure or are risk averse; I would recommend signing up for the TRS plan instead as the income will be guaranteed and steady.\nThe factor I’d like to emphasize the most is risk. Those who want a guaranteed safety blanket should stick with the TRS plan. They don’t have to worry about downturns in the market, unexpected long life expectancy or running out of income one day. If these factors don’t deter you, you may want to consider taking the ORP plan instead."
  },
  {
    "objectID": "individual_report.html#r-packages",
    "href": "individual_report.html#r-packages",
    "title": "Course Project Individual Report: Does a Country’s Geographic Characteristics Affect Performance at the Olympics?",
    "section": "R Packages",
    "text": "R Packages\nThe following R packages will be used.\n\n\nCode\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(readr)\nlibrary(rvest)\nlibrary(sf)\nlibrary(viridis)\nlibrary(ggplot2)\nlibrary(WDI)\nlibrary(DT)\nlibrary(gganimate)\nlibrary(shiny)\nlibrary(shinyWidgets)\nlibrary(rsconnect)\n\n\nThe backbone of the data analysis will be conducted on the historical outcomes of the Olympic Games. This data set will be acquired from Kaggle, found here. Kaggle requires signing into a free account to download files. Due to this restriction, hand downloading the file is the simplest way to get the data. The specific CSV we’ll work with from the available files is Olympic_Medal_Tally_History.csv.\n\n\nCode\n### Importing Data ###\n\n# import medal count data by reading the CSV file, the file path directory may need to be changed depending where you save it\nmedal_tally &lt;- read_csv(\"Olympic_Medal_Tally_History.csv\")\n\n\nThe next set of data are coordinate systems for each country in the world. These are readily available from Google Developers. This data can be web scraped and imported into the work environment directly.\nOnce the country coordinate data has been loaded, we inspect any issues related to joining it with our Olympic Games data. An anti-join is used to generate a list of participants in Olympic history that don’t appear in the country coordinate data. There are a multitude of reasons why some participants may not show up in the coordinate data:\n\nNations that no longer exist (e.g., USSR, West Germany, East Germany).\nAdjusted naming schemes (e.g., Czechia ~ Czech Republic, Great Britain ~ United Kingdom).\nSpecial case naming schemes (e.g., ROC ~ Russia, Unified Team ~ former Soviet Union).\n\nAs a result, some participants will be adjusted to their modern day location. In the context of this analysis, this should not generate drastic bias since the analysis will be focused on geographic location and features. For example, grouping the medals won by East and West Germany together won’t be an issue since the physical location of the country is the subject of study. Nevertheless, take this into consideration when following along with the analysis.\n\n\nCode\n# import country longitude & latitude data from Google Developers\n# load the webpage\ncoordinates_webpage &lt;- read_html(\"https://developers.google.com/public-data/docs/canonical/countries_csv\")\n\n# extract the table/CSV directly from embedded webpage\ncountry_coordinates &lt;- coordinates_webpage |&gt; html_table()\n\n# convert table into a dataframe\ncountry_coordinates &lt;- country_coordinates[[1]] # only 1 table exists on the html page\n\n# organize the coordinates table, limitations are the coordinates are like an average since some countries are huge\ncountry_coordinates &lt;- country_coordinates |&gt;\n  select(name, latitude, longitude) |&gt;\n  rename(country = name) |&gt; # renaming to make joining conventions\n  mutate(hemisphere = ifelse(latitude &gt;= 0, \"Northern\", \"Southern\")) # designating geographical categories, starting with hemispheres\n\n# some Olympic competing country names are not the same as the coordinates, let's look at what these are:\nmissing_countries &lt;- medal_tally |&gt;\n  anti_join(country_coordinates, by = \"country\") |&gt;\n  distinct(country) |&gt;\n  select(country) |&gt;\n  arrange((country))\nprint(missing_countries)\n\n# I'm renaming these countries to their modern locations as listed in the coordinates data since analysis will be done by geography/location, don't particularly care about its history\n# there are a few that wont get designated such as the Mixed Team and West Indies Federation\nmedal_tally &lt;- medal_tally |&gt;\n  mutate(country = case_when(\n    country == \"Australasia\" ~ \"Australia\",\n    country == \"Bohemia\" ~ \"    Czech Republic\",\n    country == \"Chinese Taipei\" ~ \"Taiwan\",\n    country == \"Czechia\" ~ \"Czech Republic\",\n    country == \"Czechoslovakia\" ~ \"Czech Republic\",\n    country == \"Democratic People's Republic of Korea\" ~ \"North Korea\",\n    country == \"East Germany\" ~ \"Germany\",\n    country == \"Great Britain\" ~ \"United Kingdom\",\n    country == \"Hong Kong, China\" ~ \"Hong Kong\",\n    country == \"Islamic Republic of Iran\" ~ \"Iran\",\n    country == \"Kingdom of Saudi Arabia\" ~ \"Saudi Arabia\",\n    country == \"North Macedonia\" ~ \"Macedonia [FYROM]\",\n    country == \"People's Republic of China\" ~ \"China\",\n    country == \"ROC\" ~ \"Russia\",\n    country == \"Republic of Korea\" ~ \"South Korea\",\n    country == \"Republic of Moldova\" ~ \"Moldova\",\n    country == \"Russian Federation\" ~ \"Russia\",\n    country == \"Serbia and Montenegro\" ~ \"Serbia\",\n    country == \"Soviet Union\" ~ \"Russia\",\n    country == \"Syrian Arab Republic\" ~ \"Syria\",\n    country == \"The Bahamas\" ~ \"Bahamas\",\n    country == \"Türkiye\" ~ \"Turkey\",\n    country == \"Unified Team\" ~ \"Russia\",\n    country == \"United Republic of Tanzania\" ~ \"Tanzania\",\n    country == \"United States Virgin Islands\" ~ \"U.S. Virgin Islands\",\n    country == \"West Germany\" ~ \"Germany\",\n    country == \"Yugoslavia\" ~ \"Serbia\",\n    TRUE ~ country # don't change anything else mentioned outside of the above list\n  )) |&gt;\n  filter(year &gt; 1960) # only looking at more recent Olympic Games to get a better picture of insights\n\n# although some names of been updated and some share the same country name, the aggregation of medals will come later\n\n\nLastly, we will ingest population data from the World Bank. Luckily, this data is readily available in the library(WDI) package. Alternatively, the same data can be obtained from the World Bank API but there are limitations using the API. To keep it as simple as possible, using the dedicated R package is the best idea. The population data is used to help normalize the analysis and conclusions about population can be drawn from the normalization.\nAs discussed earlier, there will be data that is incompatible join to the Olympic Games data. Once again we’ll take a look at what those are and identify similar reasoning as discussed earlier.\n\n\nCode\n# WDI population\n# generate the sequence of years: every two years from 1960 to 2024 (data limited to 1960 and onward), every two years for Olympic years\nyears_of_interest &lt;- seq(1960, 2024, by = 2)\n\n# fetch population data for all years (WDI does not directly support filtering by step years)\npopulation_data &lt;- WDI(indicator = \"SP.POP.TOTL\", start = 1960, end = 2024)\n\n# filter the data to include only the years in the sequence\nfiltered_population_data &lt;- population_data[population_data$year %in% years_of_interest, ]\n\nfiltered_population_data &lt;- filtered_population_data |&gt;\n  select(-iso2c, -iso3c) |&gt; # get rid of unnecessary columns\n  rename(population = SP.POP.TOTL)\n\n# rename country's in population data to join properly\nmissing_population_countries &lt;- medal_tally |&gt;\n  anti_join(filtered_population_data, by = \"country\") |&gt;\n  distinct(country) |&gt;\n  select(country) |&gt;\n  arrange((country))\nprint(missing_population_countries)\n# Taiwan left out due to political reasons, grouped with China. Taking a deeper look they only won a handful of medals so leaving this data out shouldn't cause a large influence in the data analysis\n\nfiltered_population_data &lt;- filtered_population_data |&gt;\n  mutate(country = case_when(\n    country == \"Bahamas, The\" ~ \"Bahamas\",\n    country == \"Czechia\" ~ \"Czech Republic\",\n    country == \"Cote d'Ivoire\" ~ \"Côte d'Ivoire\",\n    country == \"Egypt, Arab Rep.\" ~ \"Egypt\",\n    country == \"Hong Kong SAR, China\" ~ \"Hong Kong\",\n    country == \"Iran, Islamic Rep.\" ~ \"Iran\",\n    country == \"Kyrgyz Republic\" ~ \"Kyrgyzstan\",\n    country == \"North Macedonia\" ~ \"Macedonia [FYROM]\",\n    country == \"Korea, Dem. People's Rep.\" ~ \"North Korea\",\n    country == \"Korea, Rep.\" ~ \"South Korea\",\n    country == \"Russian Federation\" ~ \"Russia\",\n    country == \"Slovak Republic\" ~ \"Slovakia\",\n    country == \"Syrian Arab Republic\" ~ \"Syria\",\n    country == \"Turkiye\" ~ \"Turkey\",\n    country == \"Virgin Islands (U.S.)\" ~ \"U.S. Virgin Islands\",\n    country == \"Venezuela, RB\" ~ \"Venezuela\",\n    country == \"Viet Nam\" ~ \"Vietnam\",\n    TRUE ~ country # don't change anything else mentioned outside of the above list\n  ))\n\n\n# join to medal tally data frame\nmedal_tally &lt;- medal_tally |&gt;\n  left_join(filtered_population_data,\n    by = c(\"country\" = \"country\", \"year\" = \"year\")\n  )\n\n\nWe will be focusing on Olympic Games from 1960 and onward to get a better picture of national participation. Additionally, a weighted medal system will be implemented in the analysis. The worth of a gold medal will be three times a bronze and the worth of a silver medal will be twice a bronze. Keep this in mind when interpreting the count of medals in the analysis.\n\n\nCode\n# weigh the medal counts by different value, gold is ranked 3 times more than bronze, silver is ranked 2 times more than bronze\nmedal_tally &lt;- medal_tally |&gt;\n  mutate(\n    gold = gold * 3,\n    silver = silver * 2,\n    total = gold + silver + bronze\n  )\n\n### end of data import ###"
  },
  {
    "objectID": "individual_report.html#exploration-of-olympic-data",
    "href": "individual_report.html#exploration-of-olympic-data",
    "title": "Course Project Individual Report: Does a Country’s Geographic Characteristics Affect Performance at the Olympics?",
    "section": "Exploration of Olympic Data",
    "text": "Exploration of Olympic Data\n\nThe analysis will take a look at the Summer and Winter Olympics separately. The difference in the amount of medals distributed in the Summer Games versus the Winter Games is drastic. Let’s take a look at historically how many medals have been given out.\n\n\nCode\n# calculate total of weighted medals awarded for Winter\nwinter_games_total_medal_count &lt;- medal_tally |&gt;\n  filter(str_detect(edition, \"Winter\")) |&gt;\n  summarize(total_medals_awarded = sum(total))\n\nprint(paste0(\"The total weighted amount of medals distributed throughout the Winter Olympics is \", winter_games_total_medal_count, \" medals.\"))\n\n\n[1] \"The total weighted amount of medals distributed throughout the Winter Olympics is 6067 medals.\"\n\n\nCode\n# calculate total of weighted medals awarded for Summer\nsummer_games_total_medal_count &lt;- medal_tally |&gt;\n  filter(str_detect(edition, \"Summer\")) |&gt;\n  summarize(total_medals_awarded = sum(total))\n\nprint(paste0(\"The total weighted amount of medals distributed throughout the Summer Olympics is \", summer_games_total_medal_count, \" medals.\"))\n\n\n[1] \"The total weighted amount of medals distributed throughout the Summer Olympics is 23120 medals.\"\n\n\n\nNext we’ll take a look at each country’s aggregated historical performance at the Olympic Games.\n\n\nCode\n# get sf files for plotting\nurl &lt;- \"https://datacatalogfiles.worldbank.org/ddh-published/0038272/DR0046659/wb_countries_admin0_10m.zip?versionId=2024-05-14T14:58:01.5696428Z\"\n\n# Define the destination file path with the new name\ndestfile &lt;- \"world_map.zip\"\n\n# Check if the file already exists and download only if it does not\nif (!file.exists(destfile)) {\n  download.file(url, destfile, mode = \"wb\")\n}\n\n# extract sf file function\nread_shp_from_zip &lt;- function(zip_file) {\n  temp_dir &lt;- tempdir() # create a temporary directory\n  zip_contents &lt;- unzip(zip_file, exdir = temp_dir) # unzip the contents and\n  shp_file &lt;- zip_contents[grepl(\"\\\\.shp$\", zip_contents)] # filter for .shp files\n  sf_object &lt;- read_sf(shp_file) # read the .shp file into an sf object\n  return(sf_object) # return the sf object\n}\n\nworld_shapefile &lt;- read_shp_from_zip(\"world_map.zip\")\n\n# adjust names\nworld_shapefile &lt;- world_shapefile |&gt;\n  mutate(NAME_EN = case_when(\n    NAME_EN == \"United States of America\" ~ \"United States\",\n    NAME_EN == \"People's Republic of China\" ~ \"China\",\n    TRUE ~ NAME_EN\n  ))\n\n# find all time medal count by country in the winter Olympics\nwinter_medal_count &lt;- medal_tally |&gt;\n  filter(str_detect(edition, \"Winter\")) |&gt;\n  group_by(country) |&gt;\n  summarize(\n    total_medals = sum(total), # count total medals won all time\n    total_gold = sum(gold), # count total gold medals won all time\n    total_silver = sum(silver), # count total silver medals won all time\n    total_bronze = sum(bronze)\n  ) |&gt; # count total bronze medals won all time\n  select(country, total_medals, total_gold, total_silver, total_bronze) |&gt;\n  arrange(desc(total_medals))\n\n# join winter medal count to sf file\nwinter_medal_world_map &lt;- world_shapefile |&gt;\n  left_join(\n    winter_medal_count,\n    join_by(NAME_EN == country)\n  )\n\n# plot medal count heat map\nggplot(winter_medal_world_map, aes(fill = total_medals)) +\n  geom_sf() +\n  labs(\n    title = \"World Map By Medals Won\",\n    subtitle = \"Winter Olympic Games\"\n  ) +\n  scale_fill_viridis_c(name = \"Total Medals\", option = \"plasma\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    panel.border = element_rect(color = \"gray\", fill = NA, size = 1),\n    plot.background = element_rect(fill = \"white\", color = \"darkgrey\", size = 1),\n    panel.background = element_rect(fill = \"white\"),\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\nCode\n# clean up names for data table display\nwinter_medal_count_tabular &lt;- winter_medal_count |&gt;\n  rename(\n    Country = country,\n    `Total Medals` = total_medals,\n    `Gold Medals` = total_gold,\n    `Silver Medals` = total_silver,\n    `Bronze Medals` = total_bronze\n  )\n\ndatatable(winter_medal_count_tabular, caption = \"Winter Olympic Medal Distribution\")\n\n\n\n\n\n\nCode\n# find all time medal count by country in the summer Olympics\nsummer_medal_count &lt;- medal_tally |&gt;\n  filter(str_detect(edition, \"Summer\")) |&gt;\n  group_by(country) |&gt;\n  summarize(\n    total_medals = sum(total), # count total medals won all time\n    total_gold = sum(gold), # count total gold medals won all time\n    total_silver = sum(silver), # count total silver medals won all time\n    total_bronze = sum(bronze)\n  ) |&gt; # count total bronze medals won all time\n  select(country, total_medals, total_gold, total_silver, total_bronze) |&gt;\n  arrange(desc(total_medals))\n\n# join summer medal count to sf file\nsummer_medal_world_map &lt;- world_shapefile |&gt;\n  left_join(\n    summer_medal_count,\n    join_by(NAME_EN == country)\n  )\n\n# plot medal count heat map\nggplot(summer_medal_world_map, aes(fill = total_medals)) +\n  geom_sf() +\n  labs(\n    title = \"World Map By Medals Won\",\n    subtitle = \"Summer Olympic Games\"\n  ) +\n  scale_fill_viridis_c(name = \"Total Medals\", option = \"plasma\") +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    panel.border = element_rect(color = \"gray\", fill = NA, size = 1),\n    plot.background = element_rect(fill = \"white\", color = \"darkgrey\", size = 1),\n    panel.background = element_rect(fill = \"white\"),\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\nCode\n# clean up names for data table\nsummer_medal_count_tabular &lt;- summer_medal_count |&gt;\n  rename(\n    Country = country,\n    `Total Medals` = total_medals,\n    `Gold Medals` = total_gold,\n    `Silver Medals` = total_silver,\n    `Bronze Medals` = total_bronze\n  )\n\ndatatable(summer_medal_count_tabular, caption = \"Summer Olympic Medal Distribution\")\n\n\n\n\n\n\n\n \n\nIf we compare the top 10 performing countries in each set, we have a few overlapping nations. We see that Germany, Russia, USA, and Italy intersect in each list. At one point or another in history, these countries were considered superpowers of the world. That’s something we might want to keep in mind.\nFurthermore, let’s inspect the historical performance of the top 5 countries in each list to get a better understanding if there has been consistency throughout the years.\n\n\nCode\n# some analysis solely looking at the top 5 winning countries in summer and winter, can look at historical data performance for insights\nwinter_medal_count_top5 &lt;- winter_medal_count |&gt;\n  slice_max(total_medals, n = 5)\n\nwinter_top_5_performance &lt;- medal_tally |&gt;\n  filter(str_detect(edition, \"Winter\")) |&gt;\n  filter(country %in% winter_medal_count_top5$country)\n\n# animate the performance\nanimated_winter_top5 &lt;- ggplot(winter_top_5_performance, aes(x = year, y = total, color = country)) +\n  geom_line(size = 1.2) +\n  labs(\n    title = \"Winter Olympics Historical Performances\",\n    subtitle = \"Top 5 Countries With Most Medals\",\n    y = \"Total Medals Won\",\n    x = \"Year\"\n  ) +\n  facet_grid(country ~ .) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  transition_reveal(year)\n\n# render the animation\nanimate(animated_winter_top5, nframes = 100, fps = 10, renderer = gifski_renderer(\"winter_olympics_animation.gif\"))\n\n\n\n\n\n\n\n\n\nThe timeline shows us that Canada, Germany, Norway and the US have been performing better in the recent years as compared to the past. This might be explained by factors such as more investments into Winter Olympic athletes in the modern century. Canada and the US in particular showcased little success in the past.\nNext we can take a look at the Summer Olympics.\n\n\nCode\n# get top 5 performing countries from summer Olympics\nsummer_medal_count_top5 &lt;- summer_medal_count |&gt;\n  slice_max(total_medals, n = 5)\n\nsummer_top_5_performance &lt;- medal_tally |&gt;\n  filter(str_detect(edition, \"Summer\")) |&gt;\n  filter(country %in% summer_medal_count_top5$country)\n\n# animate the performance\nanimated_summer_top5 &lt;- ggplot(summer_top_5_performance, aes(x = year, y = total, color = country)) +\n  geom_line(size = 1.2) +\n  labs(\n    title = \"Summer Olympics Historical Performances\",\n    subtitle = \"Top 5 Countries With Most Medals\",\n    y = \"Total Medals Won\",\n    x = \"Year\"\n  ) +\n  facet_grid(country ~ .) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  transition_reveal(year) # This creates the animation effect based on the year\n\n# render the animation\nanimate(animated_summer_top5, nframes = 100, fps = 10, renderer = gifski_renderer(\"summer_olympics_animation.gif\"))\n\n\n\n\n\n\n\n\n\nWhat sticks out right away is that China did not start winning any medals until later in time as compared to the other four countries. Even with a late start in winning medals, the country broke into the top 5 historically performing countries. We might want to think about what factors may have led to their success as compared to other countries that have won medals earlier in time."
  },
  {
    "objectID": "individual_report.html#latitude-regression",
    "href": "individual_report.html#latitude-regression",
    "title": "Course Project Individual Report: Does a Country’s Geographic Characteristics Affect Performance at the Olympics?",
    "section": "Latitude Regression",
    "text": "Latitude Regression\n\nNow that we’ve gathered some basic insights into the Olympic data, we can move forward to looking at the relationship between geographical factors and success at the Olympics. The first approach is to generate a scatter plot of each countries latitude versus medals won. We’re hoping to find some relationship between the variables.\nWe’ll take a look at the Winter Olympics first.\n\n\nCode\n# assess relationship between latitude and medal count for the Winter Olympics\n\n# join coordinate system to the winter medal data frame\nlatitude_medals_winter &lt;- winter_medal_count |&gt;\n  left_join(\n    country_coordinates,\n    join_by(country == country)\n  ) |&gt;\n  select(country, total_medals, latitude, hemisphere) |&gt;\n  na.omit()\n\n# plot relationship\nggplot(latitude_medals_winter, aes(x = latitude, y = total_medals, color = hemisphere)) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Medal Count & Latitude\",\n    subtitle = \"Winter Olympics\",\n    x = \"Latitude\",\n    y = \"Total Medals\"\n  ) +\n  scale_color_manual(values = c(\"Northern\" = \"slateblue1\", \"Southern\" = \"indianred1\")) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    panel.border = element_rect(color = \"gray\", fill = NA, size = 1),\n    plot.background = element_rect(fill = \"white\", color = \"darkgrey\", size = 1),\n    panel.background = element_rect(fill = \"white\"),\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\nThere are no winning countries in the Winter Olympics located near the equator. This may support the idea that hotter climate countries see little success at the Winter Games due to lack of resources available to practice sports in winter environments. It’s clear there is no linear relationship that can be established between latitude and medals at the Winter Olympics. We can only draw the conclusion that countries that experience a colder environment will experience more likelihood of success. Note that the majority of the medals are located in the northern hemisphere. This may be attributed to the fact that more countries exist in the northern part of the world.\nRecall earlier that the Summer Olympics distributes almost four times as many medals as compared to the Winter Olympics. Next we’ll move onto analyzing the same relationship for the Summer games. With more data points available, we’ll expect a more robust analysis available to us to conduct.\n\n\nCode\n# for the summer Olympics latitude\nlatitude_medals_summer &lt;- summer_medal_count |&gt;\n  left_join(\n    country_coordinates,\n    join_by(country == country)\n  ) |&gt;\n  select(country, total_medals, latitude, hemisphere) |&gt;\n  na.omit()\n\nggplot(latitude_medals_summer |&gt; na.omit(), aes(x = latitude, y = total_medals)) +\n  geom_point() +\n  labs(\n    title = \"Medal Count & Latitude\",\n    subtitle = \"Summer Olympics\",\n    x = \"Latitude\",\n    y = \"Total Medals\"\n  ) +\n  theme_minimal() +\n  theme(\n    panel.border = element_rect(color = \"gray\", fill = NA, size = 1),\n    plot.background = element_rect(fill = \"white\", color = \"darkgrey\", size = 1),\n    panel.background = element_rect(fill = \"white\"),\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\nCode\n# visually no linear relationship\n\n\nThere is no obvious linear relationship present between the two variables. Furthermore, there is no distinct conclusion we can draw as seen in the Winter Olympic plot from before. We’ll need to investigate further on how we can derive some insights from the data.\nThe first method would be to normalize the data by some common factor. Recall that population data was imported earlier. Here we can normalize the total medals won by population. We’ll define the new Y-axis as total medals per capita, so that the analysis will be more objective.\n\n\nCode\n# normalize by population for latitude regression\nmedal_tally_normalized &lt;- medal_tally |&gt;\n  mutate(medals_per_capita = total / population)\n\nsummer_medal_count_normalized &lt;- medal_tally_normalized |&gt;\n  filter(str_detect(edition, \"Summer\")) |&gt;\n  group_by(country) |&gt;\n  summarize(total_medals_per_capita = sum(medals_per_capita)) |&gt; # count total bronze medals won all time\n  select(country, total_medals_per_capita)\n\nlatitude_medals_summer_normalized &lt;- summer_medal_count_normalized |&gt;\n  left_join(\n    country_coordinates,\n    join_by(country == country)\n  ) |&gt;\n  select(country, total_medals_per_capita, latitude) |&gt;\n  na.omit()\n\nggplot(latitude_medals_summer_normalized |&gt; na.omit(), aes(x = latitude, y = total_medals_per_capita)) +\n  geom_point() +\n  labs(\n    title = \"Medal Count per Capita & Latitude (Normalized)\",\n    subtitle = \"Summer Olympics\",\n    x = \"Latitude\",\n    y = \"Total Medals per Capita\"\n  ) +\n  theme_minimal() +\n  theme(\n    panel.border = element_rect(color = \"gray\", fill = NA, size = 1),\n    plot.background = element_rect(fill = \"white\", color = \"darkgrey\", size = 1),\n    panel.background = element_rect(fill = \"white\"),\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\nAfter normalizing the data, the scatter plot shape remains relatively the same. There’s a slight change on the higher end of the latitude but not enough to warrant a linear relationship still. Although we can’t see anything visually, we draw conclusions by comparing this normalized graph to the regular graph from above.\nThe similarity in plot shape suggests that population size does not have a significant influence on the relationship between latitude and the number of medals won. If larger populations do not yield a different pattern when you control for them, it implies that other factors may play a more substantial role in determining how many medals a country wins.\nNext, we’ll take a statistical driven method to transform our original plot again to help derive other insights. The first process is to find the residuals of the data to determine what sort of transformation should be done.\n\n\nCode\n# fit a linear model\nsummer_model &lt;- lm(total_medals ~ latitude, data = latitude_medals_summer)\n\n# pull the residuals\nlatitude_medals_summer$residuals &lt;- residuals(summer_model)\n\n# create the residual plot\nggplot(latitude_medals_summer, aes(x = fitted(summer_model), y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") + # plot zero line to interpret residuals\n  labs(\n    title = \"Residual Plot for Summer Medal Count Model\",\n    x = \"Fitted Values\",\n    y = \"Residuals\"\n  ) +\n  theme_minimal() +\n  theme(\n    panel.border = element_rect(color = \"gray\", fill = NA, size = 1),\n    plot.background = element_rect(fill = \"white\", color = \"darkgrey\", size = 1),\n    panel.background = element_rect(fill = \"white\"),\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\nThe residual plot shows a violation of constant variance in the data. We already knew that from visually looking at the original plot, but this statistical method confirms our findings. Next we’ll want to perform a transformation on the Y-value in order to generate a better graph to interpret. We can determine the procedure using the Box-Cox transformation.\n\n\nCode\nlatitude_data &lt;- latitude_medals_summer$latitude\ntotal_medals_data &lt;- latitude_medals_summer$total_medals\n\n# fit linear model between medals and latitude\nmodel &lt;- lm(total_medals_data ~ latitude_data)\n\nlibrary(MASS) # loading here as this package interferes with dplyr, only using one function momentarily\n\n# Box-Cox transformation from MASS package\nboxcox_results &lt;- boxcox(model,\n  lambda = seq(-2, 2, 1 / 10),\n  plotit = TRUE,\n  xlab = expression(lambda),\n  ylab = \"log-Likelihood\",\n  interp = TRUE,\n  eps = 1 / 50\n)\n\n# add titles\ntitle(\n  main = \"Box-Cox Transformation\",\n  sub = \"Analyzing the relationship between latitude and total medals\"\n)\n\n# outer border for aesthetic\nbox(which = \"outer\", lty = \"solid\", col = \"darkgrey\")\n\n\n\n\n\n\n\n\n\nCode\ndetach(\"package:MASS\", unload = TRUE) # unload package in case of any other dplyr interference later\n\n\nThe Box-Cox procedure identifies that 0 falls within the 95% interval of lambda; suggesting a logarithmic transformation on Y.\n\n\nCode\n# transform Y value by applying log10 function\nlatitude_medals_summer$total_medals_transformed &lt;- log10(latitude_medals_summer$total_medals)\nm.transformed_medals &lt;- lm(total_medals_transformed ~ latitude, data = latitude_medals_summer)\n\nggplot(latitude_medals_summer |&gt; na.omit(), aes(x = latitude, y = total_medals_transformed)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"lightslateblue\") +\n  labs(\n    title = \"Medal Count Transformed & Latitude\",\n    subtitle = \"Summer Olympics\",\n    x = \"Latitude\",\n    y = \"Total Medals (Log Scale)\"\n  ) +\n  theme_minimal() +\n  theme(\n    panel.border = element_rect(color = \"gray\", fill = NA, size = 1),\n    plot.background = element_rect(fill = \"white\", color = \"darkgrey\", size = 1),\n    panel.background = element_rect(fill = \"white\"),\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\nThe scatter plot now exhibits a reasonable linear relationship. We can draw some conclusions based off this transformation. We can confirm there is a non-linear relationship between latitude and medals won at the Summer Olympics. This suggests that factors other than latitude (such as socio-economic factors) may be influencing success, and that their respective relationships may not be straight forward either.\n\nPermutation Testing\n\nThe final statistical test I want to perform is a permutation test. The visual tests have provided evidence suggesting there is a relationship between latitude and medals at the Summer Olympics. I’ll be testing the significance of the correlation between the two variables.\n\nnull hypothesis: There is no relationship between latitude and the number of medals won.\nalternate hypothesis: There is a relationship between latitude and the number of medals won.\n\n\n\nCode\nset.seed(123)\n\n# define observed correlation between variables\nobserved_correlation &lt;- cor(latitude_medals_summer$latitude, latitude_medals_summer$total_medals)\n\n# permutation test\nn_permutations &lt;- 10000\npermuted_correlation &lt;- replicate(n_permutations, {\n  permuted_lat &lt;- sample(latitude_medals_summer$latitude)\n  cor(permuted_lat, latitude_medals_summer$total_medals)\n})\n\n# calculate p-value\np_value &lt;- mean(abs(permuted_correlation) &gt;= abs(observed_correlation))\n\n# visualize the results\nhist(permuted_correlation, breaks = 30, main = \"Null Distribution of Correlations\", xlab = \"Correlation\", col = \"lightblue\", border = \"white\")\nabline(v = observed_correlation, col = \"black\", lwd = 2)\ntext(observed_correlation, 200, paste(\"Observed:\", round(observed_correlation, 2)), col = \"black\", pos = 2)\n\n\n\n\n\n\n\n\n\nCode\nprint(paste0(\"The p-value from this permutation test is: \", p_value))\n\n\n[1] \"The p-value from this permutation test is: 0.0029\"\n\n\nFrom the initial permutation test, we can draw conclusions from the histogram and p-value. Since the observed correlation value lies in the far right tail & the p-value is small (assuming a confidence level of 95%, where alpha = 0.05, p-value &lt; alpha); we reject the null hypothesis and conclude the alternate. We can confidently conclude that the relationship between latitude and medals is significant.\nHowever, I’m skeptical about how the data is presented. As I mentioned earlier, there may be sampling bias due to the fact that the number of countries in the northern hemisphere outnumber those found in the southern hemisphere. I’ll take the permutation test one step further and conduct it on both hemispheres.\n\n\nCode\n# split data by hemisphere\nnorthern_summer_data &lt;- subset(latitude_medals_summer, hemisphere == \"Northern\")\nsouthern_summer_data &lt;- subset(latitude_medals_summer, hemisphere == \"Southern\")\n\n# calculate correlation in each hemisphere\nnorthern_correlation &lt;- cor(northern_summer_data$latitude, northern_summer_data$total_medals)\nsouthern_correlation &lt;- cor(southern_summer_data$latitude, southern_summer_data$total_medals)\n\n# create function for permutation test with visualization\npermutation_test_hist &lt;- function(lat, medals, n = 10000, title) {\n  observed &lt;- cor(lat, medals)\n  permuted_corrs &lt;- replicate(n, cor(sample(lat), medals))\n\n  # P-value calculation\n  p_value &lt;- mean(abs(permuted_corrs) &gt;= abs(observed))\n\n  # generate histogram data\n  hist_data &lt;- hist(permuted_corrs, breaks = 30, plot = FALSE) # don't plot, just get data\n\n  # calculate dynamic x-limit based on the data\n  xlim_range &lt;- range(c(hist_data$breaks, observed))\n\n  # add a buffer\n  xlim_range &lt;- c(xlim_range[1] - 0.05, xlim_range[2] + 0.05)\n\n  # plot the histogram with dynamic x-limit\n  plot(hist_data,\n    col = \"lightblue\", border = \"white\", main = title,\n    xlab = \"Permuted Correlations\", xlim = xlim_range\n  )\n\n  # plot the observed correlation line\n  abline(v = observed, col = \"black\", lwd = 2)\n\n  # position for text annotation\n  text_position &lt;- max(hist_data$counts) * 0.8\n\n  # paste observed value\n  text(observed, text_position,\n    paste(\"Observed:\", round(observed, 2)),\n    col = \"black\", pos = 2\n  )\n\n\n  return(list(observed = observed, p_value = p_value, permuted_corrs = permuted_corrs))\n}\n\n# set seed before calling function\nset.seed(100)\n\n# pass data into function\nnorthern_summer_test &lt;- permutation_test_hist(northern_summer_data$latitude, northern_summer_data$total_medals, \n                                              title = \"Northern Hemisphere\")\n\n\n\n\n\n\n\n\n\nCode\nsouthern_summer_test &lt;- permutation_test_hist(southern_summer_data$latitude, southern_summer_data$total_medals, \n                                              title = \"Southern Hemisphere\")\n\n\n\n\n\n\n\n\n\nCode\n# display results\ncat(\"The p-value from the permutation test conducted on the northern hemisphere is:\", northern_summer_test$p_value, \"\\n\")\n\n\nThe p-value from the permutation test conducted on the northern hemisphere is: 3e-04 \n\n\nCode\ncat(\"The p-value from the permutation test conducted on the southern hemisphere is:\", southern_summer_test$p_value, \"\\n\")\n\n\nThe p-value from the permutation test conducted on the southern hemisphere is: 0.4915 \n\n\nThis time I get two different results from each test.\n\nIn the northern hemisphere: the result is statistically significant. Countries at higher latitudes in the northern hemisphere are associated with winning more medals. This could reflect factors like greater representation, wealthier economies, or better sports infrastructure.\nIn the southern hemisphere: there is no evidence of a relationship between the two variables. The correlation is small and likely due to random chance."
  },
  {
    "objectID": "individual_report.html#climate-region-analysis",
    "href": "individual_report.html#climate-region-analysis",
    "title": "Course Project Individual Report: Does a Country’s Geographic Characteristics Affect Performance at the Olympics?",
    "section": "Climate Region Analysis",
    "text": "Climate Region Analysis\n\nThe next part of the analysis is to generalize the geographic locations based on climate. The Earth can be divided into four different climate zones based on latitude; cold, temperate, sub-tropical and tropical. We can try to derive some insights from a more generalized grouping approach than singular latitudes.\n\n\nCode\n# look at climate zones, define them as defined by Meteoblue: https://content.meteoblue.com/en/research-education/educational-resources/meteoscool/general-climate-zones\n# Tropical zone from 0°–23.5°, Subtropics from 23.5°–40°, Temperate zone from 40°–60, Cold zone from 60°–90°\nclimate_zones &lt;- country_coordinates |&gt;\n  mutate(region = case_when(\n    abs(latitude) &gt;= 0 & abs(latitude) &lt;= 23.5 ~ \"Tropical\",\n    abs(latitude) &gt; 23.5 & abs(latitude) &lt;= 40 ~ \"Subtropics\",\n    abs(latitude) &gt; 40 & abs(latitude) &lt;= 60 ~ \"Temperate\",\n    abs(latitude) &gt; 60 & abs(latitude) &lt;= 90 ~ \"Cold\"\n  )) |&gt;\n  mutate(region = ifelse(country == \"United States\", \"Temperate\", region)) |&gt; # US is huge, most of it falls in temperate zone, some in subtropical\n  select(country, region)\n\n# winter Olympic medal count by climate regions\nwinter_medals_by_climate &lt;- climate_zones |&gt;\n  left_join(winter_medal_count,\n            join_by(country == country)) |&gt;\n  select(country, region, total_medals) |&gt;\n  drop_na() |&gt;\n  group_by(region) |&gt;\n  summarize(regional_total_medals = sum(total_medals)) |&gt;\n  arrange(desc(regional_total_medals))\n\nggplot(winter_medals_by_climate, aes(x = region, y = regional_total_medals)) +\n  geom_col(fill = \"skyblue\", color = \"black\", size = 0.5) +\n  labs(\n    title = \"Total Medals Won by Climate Region\",\n    subtitle = \"Winter Olympics\",\n    x = \"Climate Region\", # Add x-axis label\n    y = \"Medals Won\"\n  ) +\n  theme_minimal() +\n  geom_text(aes(label = regional_total_medals),\n    vjust = -0.3, size = 3.5, color = \"black\"\n  ) +\n  theme(\n    panel.border = element_rect(color = \"gray\", fill = NA, size = 1),\n    plot.background = element_rect(fill = \"white\", color = \"darkgrey\", size = 1),\n    panel.background = element_rect(fill = \"white\"),\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\nCode\n# Summer Olympic medal count by climate regions\nsummer_medals_by_climate &lt;- climate_zones |&gt;\n  left_join(summer_medal_count,\n            join_by(country == country)) |&gt;\n  select(country, region, total_medals) |&gt;\n  drop_na() |&gt;\n  group_by(region) |&gt;\n  summarize(regional_total_medals = sum(total_medals)) |&gt;\n  arrange(desc(regional_total_medals))\n\n# bar plot representation\nggplot(summer_medals_by_climate, aes(x = region, y = regional_total_medals)) +\n  geom_col(fill = \"lightcoral\", color = \"black\", size = 0.5) +\n  labs(\n    title = \"Total Medals Won by Climate Region\",\n    subtitle = \"Summer Olympics\",\n    x = \"Climate Region\",\n    y = \"Medals Won\"\n  ) +\n  theme_minimal() +\n  geom_text(aes(label = regional_total_medals),\n    vjust = -0.3, size = 3.5, color = \"black\"\n  ) +\n  theme(\n    panel.border = element_rect(color = \"gray\", fill = NA, size = 1),\n    plot.background = element_rect(fill = \"white\", color = \"darkgrey\", size = 1),\n    panel.background = element_rect(fill = \"white\"),\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\nEvidently, countries in the temperate zone have the most medals. We’ll conduct an ANOVA test to confirm how significant this is.\n\n\nCode\n# perform ANOVA analysis on this since there shouldn't be a linear relationship\n\n# define climate zones in summer games\nsummer_games_climate &lt;- climate_zones |&gt;\n  left_join(summer_medal_count,\n            join_by(country == country)) |&gt;\n  select(country, region, total_medals) |&gt;\n  drop_na()\n\n# define region as a factor for analysis\nsummer_games_climate$region &lt;- as.factor(summer_games_climate$region)\n\n# use built in ANOVA function\nsummer_anova &lt;- aov(total_medals ~ region, data=summer_games_climate)\nsummary(summer_anova)\n\n\n             Df   Sum Sq Mean Sq F value   Pr(&gt;F)    \nregion        3  3355814 1118605   6.391 0.000448 ***\nResiduals   131 22930236  175040                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn the Summer Olympics, the p-value confirms that regions have a significant effect on medal counts.\n\n\nCode\n# define climate zones in winter games\nwinter_games_climate &lt;- climate_zones |&gt;\n  left_join(\n    winter_medal_count,\n    join_by(country == country)\n  ) |&gt;\n  select(country, region, total_medals) |&gt;\n  drop_na()\n\n# define region as a factor for analysis\nwinter_games_climate$region &lt;- as.factor(winter_games_climate$region)\n\n# use built in ANOVA function\nwinter_anova &lt;- aov(total_medals ~ region, data = winter_games_climate)\nsummary(winter_anova)\n\n\n            Df  Sum Sq Mean Sq F value Pr(&gt;F)  \nregion       2  412515  206258   5.243   0.01 *\nResiduals   36 1416101   39336                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn the Winter Olympics, the p-value confirms that regions have a significant effect on medal counts.\n\n\nTo view customized results, utilize this interactive model:\n\n\nShiny applications not supported in static R Markdown documents"
  }
]